{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This Jupyter Notebook contains the code for training and evaluating a Sequence to Sequence (seq2seq) Encoder - Decoder model for semantic role labeling (SRL), as project for the course LT2326, autumn 2021. Data preparation is defined and handled elsewhere; see `data_builder.ipynb`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# On torchtext module"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There seemed to be no installation of `torchtext` on MLTGPU, so I ran:\n",
    "\n",
    "```pip install torchtext==0.10.0```\n",
    "\n",
    "which shold be compatible with `torch` version 1.9.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"PyTorch Version: {torch.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random, time, operator\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchtext\n",
    "import torch.nn.functional as F\n",
    "#from torchtext.data import Field, BucketIterator, Iterator, TabularDataset\n",
    "from torchtext.legacy.data import Field, BucketIterator, Iterator, TabularDataset\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Meta variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define where to get and store data and which device to use. For test pipline with less data during development set `mini_training`to `True`; when using complete dataset, set to `False`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:1\")\n",
    "#device = torch.device('cpu')\n",
    "\n",
    "my_data_directory = \"../data/\" # my settings on MLTGPU\n",
    "\n",
    "my_models_directory = \"../models/\" # my settings on MLTGPU\n",
    "\n",
    "mini_testing = True  \n",
    "my_train_file = \"mini_train.csv\" if mini_testing == True else \"train.csv\"\n",
    "my_test_file  = \"mini_test.csv\" if mini_testing == True else \"test.csv\"\n",
    "\n",
    "dir_for_evaluations = \"../evals/\" # my settings on MLTGPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define batchsize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataloader(directory  = my_data_directory,\n",
    "               train_file = my_train_file,\n",
    "               test_file  = my_test_file,\n",
    "               batch      = batch_size):\n",
    "    \n",
    "    num_whitespacer = lambda x: [int(e) for e in x.split(\" \")]\n",
    "    \n",
    "    SENTENCE = Field(lower = True,\n",
    "                     batch_first = True, \n",
    "                     init_token = \"<sos>\", \n",
    "                     eos_token = \"<eos>\")\n",
    "    \n",
    "    PREDICATE = Field(tokenize = num_whitespacer, # Here might be some problems ...\n",
    "                      batch_first = True, \n",
    "                      pad_token = 0,\n",
    "                      use_vocab = False,\n",
    "                      init_token = 0, \n",
    "                      eos_token = 0) \n",
    "    \n",
    "    SRLABEL = Field(batch_first = True, \n",
    "                    init_token = \"<sos>\", \n",
    "                    eos_token = \"<eos>\")\n",
    "    \n",
    "    my_fields = [(\"sentence\", SENTENCE),\n",
    "                 (\"predicate\", PREDICATE),\n",
    "                 (\"srlabel\", SRLABEL)]\n",
    "    \n",
    "    train, test = TabularDataset.splits(path   = directory,\n",
    "                                        train  = train_file,\n",
    "                                        test   = test_file,\n",
    "                                        format = 'csv',\n",
    "                                        fields = my_fields,\n",
    "                                        csv_reader_params = {'delimiter':'\\t',\n",
    "                                                             'quotechar':'Â¤'}) # Seems not to be in data\n",
    "    SENTENCE.build_vocab(train)\n",
    "    SRLABEL.build_vocab(train)  \n",
    "\n",
    "    train_iter, test_iter = BucketIterator.splits((train, test),\n",
    "                                                  batch_size        = batch,\n",
    "                                                  sort_within_batch = True,\n",
    "                                                  sort_key          = lambda x: len(x.sentence),\n",
    "                                                  shuffle           = True,\n",
    "                                                  device            = device)\n",
    "\n",
    "    return train_iter, test_iter, SENTENCE.vocab, SRLABEL.vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calling dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test, vocab, labels = dataloader()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Definitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The encoder encodes sentence-predicate pairs through LSTMs. In forward pass, it returns *the final cell state* and *the final hidden state* (somtimes referred to as the *context vector*).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SRL_Encoder(nn.Module):\n",
    "    def __init__(self, voc_size, embedding_size, hidden_size, n_layers, p_dropout):  \n",
    "        super(SRL_Encoder, self).__init__()\n",
    "        \n",
    "        self.embeddings = nn.Embedding(voc_size, embedding_size)\n",
    "        self.sp_pair = embedding_size + 1 # emedded sentence + predicate vector\n",
    "        self.hidden_size = hidden_size\n",
    "        self.n_layers = n_layers\n",
    "        \n",
    "        self.rnn = nn.LSTM(self.sp_pair, \n",
    "                           self.hidden_size, \n",
    "                           num_layers = self.n_layers,\n",
    "                           dropout = p_dropout,\n",
    "                           #bidirectional=True, # !\n",
    "                           batch_first=True) # !\n",
    "        self.dropout = nn.Dropout(p_dropout)\n",
    "        \n",
    "    def forward(self, sentences, pred_vec):\n",
    "        \n",
    "        embeddings = self.embeddings(sentences)\n",
    "        pred_vec = pred_vec.unsqueeze(2)        \n",
    "        sentence_pred_pair = torch.cat((embeddings, pred_vec), dim=2)\n",
    "        contextualized_embedding, (hidden_final, cell_final) = self.rnn(sentence_pred_pair)\n",
    "        \n",
    "        return hidden_final, cell_final\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The decoder predicts the next element of a sequence based on the previous sequence and the final cell state and the final hidden state of that sequence through an LSTM. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SRL_Decoder(nn.Module):\n",
    "    def __init__(self, n_labels, embedding_size, hidden_size, n_layers, p_dropout):  \n",
    "        super(SRL_Decoder, self).__init__()\n",
    " \n",
    "        self.hidden_size = hidden_size\n",
    "        self.n_layers = n_layers\n",
    "        self.n_labels = n_labels\n",
    "        \n",
    "        self.embeddings = nn.Embedding(n_labels, embedding_size) # ?\n",
    "        self.rnn = nn.LSTM(embedding_size, \n",
    "                           self.hidden_size, \n",
    "                           num_layers = self.n_layers, \n",
    "                           batch_first=True,\n",
    "                           #bidirectional=True,\n",
    "                           dropout = p_dropout)\n",
    "        self.classifier = nn.Linear(hidden_size, self.n_labels)\n",
    "        \n",
    "    def forward(self, previous, hidden, cell):\n",
    "        \n",
    "        embedded = self.embeddings(previous)\n",
    "        output, (hidden, cell) = self.rnn(embedded, (hidden, cell))\n",
    "        prediction = self.classifier(output)\n",
    "        \n",
    "        return prediction, hidden, cell\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder - Decoder Interface"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the `SRL_Seq2SeqLabeler`, the context vector (i.e. final cell and hidden states) of the `Encoder` together with the start token `<sos>` serves as inputs to predict a sequence of semantic role labels. After the first prediction, the decoder uses its own predictions as the input sequence to predict the next token. This model uses teacher forcing, meaning that, at some proportion of the time, as defined by a teacher force ratio (TFR), the true label of the sequence is put into the sequence, instead of the prediction by the encoder. \n",
    "\n",
    "Minor note: the classification problem engaged with here is a one-to-one mapping. Translation problems more generally might involve mappings of sequences of different lengths. To handle mappings of different lengths properly would require further work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SRL_Seq2SeqLabeler(nn.Module):\n",
    "    def __init__(self, encoder, decoder):  \n",
    "        super(SRL_Seq2SeqLabeler, self).__init__()\n",
    "        \n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        \n",
    "        assert encoder.hidden_size == decoder.hidden_size, \"hidden dimension of encoder must be equal to that of decoder\"\n",
    "        assert encoder.n_layers == decoder.n_layers, \"n_layers of encoder must be equal to that of decoder\"\n",
    "        \n",
    "    def forward(self, sentence, predicate, srl_labels, tfr = None): # tfr = teacher forcing ratio\n",
    "\n",
    "        batch_size = sentence.shape[0]\n",
    "        seq_len = sentence.shape[1]\n",
    "        n_labels = self.decoder.n_labels\n",
    "\n",
    "        outputs = torch.zeros(batch_size, seq_len, n_labels).to(device) # for storage\n",
    "\n",
    "        hidden, cell = self.encoder(sentence, predicate)\n",
    "        \n",
    "        seq_element = srl_labels[:, 0].unsqueeze(1) # start of sentence token; index of <sos>\n",
    "\n",
    "        for l in range(1, seq_len): # Note: starts from 1; first column of outputs will \"remain\" 0\n",
    "            output, hidden, cell = self.decoder(seq_element, hidden, cell)\n",
    "            outputs[:, l, :] = output.squeeze()\n",
    "            best_guess = output.argmax(2)\n",
    "\n",
    "            if tfr != None:\n",
    "                teacher_force = random.random() < tfr\n",
    "                seq_element = srl_labels[:, l].unsqueeze(1) if teacher_force else best_guess\n",
    "            else:\n",
    "                seq_element = best_guess\n",
    "\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainer(model, # Must be an instance of a model!\n",
    "            name_of_model,\n",
    "            learning_rate,\n",
    "            epochs,\n",
    "            data,\n",
    "            my_tfr,\n",
    "            clip_grad = None,\n",
    "            ignore_label = \"<pad>\", # set to None for no ignore_label\n",
    "            val_data = None,\n",
    "            save_model = False,\n",
    "            directory = my_models_directory,\n",
    "            my_loss_function = nn.CrossEntropyLoss,\n",
    "            my_optimizer = optim.Adam\n",
    "           ):\n",
    "    \"\"\" Specifices a general training procedure for a model. \n",
    "        Note: trainer() requires an instantiated model as model argument.\n",
    "    \"\"\"\n",
    "    \n",
    "    optimizer = my_optimizer(model.parameters(), lr=learning_rate)    \n",
    "    \n",
    "    model.to(device)\n",
    "    model.train()\n",
    "    \n",
    "    if ignore_label != None:\n",
    "        label_idx = labels.stoi[ignore_label]\n",
    "        loss_function = my_loss_function(ignore_index=label_idx) # We ignore e.g. pad token in loss calculation\n",
    "    else:\n",
    "        loss_function = my_loss_function()\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        print(f\"Epoch: {epoch+1} (out of {epochs}).\")\n",
    "        epoch_loss = 0\n",
    "        \n",
    "        for i, batch in enumerate(data):\n",
    "            print(\"Batch: \", i, end=\"\\r\")\n",
    "            optimizer.zero_grad # reset gradients\n",
    "            \n",
    "            sentence = batch.sentence\n",
    "            predicate = batch.predicate\n",
    "            targets = batch.srlabel\n",
    "            \n",
    "            output = model(sentence, predicate, targets, tfr = my_tfr)\n",
    "            \n",
    "            # Before calculation of loss outputs and targets needs to be \"aligned\", so to speak.\n",
    "            # Outputs are of shape [batch, seq_len, dimension]. Targets of shape [batch, seq_len]\n",
    "            # The representation of the first element of the output sequence will be 0s (see \n",
    "            # above). The first element of the targets will be <sos>. We ignore these first elements\n",
    "            # in calculating the loss. \n",
    "            \n",
    "            # Moreover, our loss function (CrossEntropyLoss) expects predicitons as [n_predictions, \n",
    "            # n_classes] and targets as [n_predictions]. Here, n_predictions = batch_size * sequence_\n",
    "            # length. \n",
    "            \n",
    "            bsz = output.shape[0]\n",
    "            length = output.shape[1]\n",
    "            output_dim = output.shape[2]\n",
    "        \n",
    "            output = output[:, 1:, :].reshape(bsz*(length - 1), output_dim) # first token (\"column\") being zeroes\n",
    "            targets = targets[:, 1:].flatten() # first token being <sos>\n",
    "            \n",
    "            # Now, calculate the loss\n",
    "            loss = loss_function(output, targets)\n",
    "            \n",
    "            epoch_loss += loss.item()\n",
    "            \n",
    "            if clip_grad != None:\n",
    "                nn.utils.clip_grad_norm_(model.parameters(), clip_grad) # to handle exploding gradients\n",
    "            \n",
    "            loss.backward() # compute gradients\n",
    "            optimizer.step() # update parameters\n",
    "            #break\n",
    "            \n",
    "        print(f\"Total loss for Epoch {epoch+1}: {epoch_loss}.\")\n",
    "            \n",
    "        if val_data != None:\n",
    "            model.eval()\n",
    "            # Here we could do some evaluation of model progress, but I have ignored this for now. \n",
    "            model.train()\n",
    "            \n",
    "    if save_model == True:\n",
    "        torch.save(model, directory+name_of_model+\".pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some examples from the web of hyperparameters used for se2seq encoder-decoder models:\n",
    "\n",
    "|Author            |No. of layers|Batch Size|Embeddingsdim.|Hidden Dim.|Dropout|WWW             |\n",
    "|------------------|-------------|----------|--------------|-----------|-------|---------|\n",
    "|Ziqi Yuan         |            2|       128|           256|        512|    0.5|https://www.kaggle.com/columbine/seq2seq-pytorch|\n",
    "|Balakrishnakumar V|            2|        32|           300|       1024|    0.5|https://towardsdatascience.com/a-comprehensive-guide-to-neural-machine-translation-using-seq2sequence-modelling-using-pytorch-41c9b84ba350|\n",
    "|Matthew Inkawhich |            2|        64|           ?  |        500|    0.1|https://pytorch.org/tutorials/beginner/deploy_seq2seq_hybrid_frontend_tutorial.html|\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_epochs = 10\n",
    "learning_rate = 0.001\n",
    "dropout = 0.5\n",
    "TFR = 0.5\n",
    "# batch size defined before calling dataloader\n",
    "\n",
    "vocab_size = len(vocab)\n",
    "num_labels = len(labels)\n",
    "emb_sizeE = 256\n",
    "#emb_sizeD = num_labels\n",
    "emb_sizeD = int(num_labels/2) # embeddings for labels\n",
    "hid_size = 512\n",
    "\n",
    "num_layers = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instantiate models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "my_encoder = SRL_Encoder(vocab_size, emb_sizeE, hid_size, num_layers, p_dropout=dropout)\n",
    "my_decoder = SRL_Decoder(num_labels, emb_sizeD, hid_size, num_layers, p_dropout=dropout)\n",
    "my_SRLLabeler = SRL_Seq2SeqLabeler(my_encoder, my_decoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Know your models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "my_encoder.parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_decoder.parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Name model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "protoname = f\"srl_b{batch_size}ep{my_epochs}ly{num_layers}em{emb_sizeE}h{hid_size}tfr{str(TFR)[2:]}do{str(dropout)[2:]}lr{str(learning_rate)[2:]}\"\n",
    "model_name = f\"{protoname}_minisample\" if mini_testing else f\"{protoname}_csample\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "trainer(model = my_SRLLabeler, \n",
    "        name_of_model = model_name, \n",
    "        learning_rate = learning_rate, \n",
    "        epochs = my_epochs, \n",
    "        data = train, \n",
    "        clip_grad = 1,\n",
    "        ignore_label = \"<pad>\",\n",
    "        my_tfr = TFR,\n",
    "        save_model = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For evaluation, several features of the model output are considered: accuracy, F1, confusion matrix, correlation of performance with length of sentence, the best and worst sentences that the model annotated. See report for details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The list of labels\n",
    "lst_labels = [labels.itos[x] for x in range(len(labels))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions and a class for handling information "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOURCES:\n",
    "# https://www.baeldung.com/cs/multi-class-f1-score\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html\n",
    "\n",
    "def metrics(prediction, truth):\n",
    "    \"\"\" Calculates accuracy and F1, given two sequences (lists, arrays) of labels. Since, \n",
    "        these metrices here are used for multi-label classification, two versions \n",
    "        of F1 are calculated: \"macro\" and \"weigthed\", where the former is the mean of F1 for\n",
    "        each label, and the latter is the mean weigthed by support (the number of true \n",
    "        instances for each label), which account for label imbalance.\n",
    "    \"\"\"\n",
    "    \n",
    "    accuracy = accuracy_score(truth, prediction)\n",
    "    f1_macro = f1_score(truth, prediction, average = \"macro\") # Calculate metrics for each label, and find their unweighted mean. Does not take label imbalance into account.\n",
    "    f1_weighted = f1_score(truth, prediction, average = \"weighted\") # Calculate metrics globally by counting the total true positives, false negatives and false positives.\n",
    "    \n",
    "    return accuracy, f1_macro, f1_weighted\n",
    "\n",
    "def mean(array):\n",
    "    \"\"\" Calculates the mean and standard deviation of an aray of numbers.\n",
    "    \"\"\"\n",
    "    mean = np.mean(array)\n",
    "    std  = np.std(array)\n",
    "    return mean, std\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Evaluation:\n",
    "    \"\"\" For storing and handling information from the evaluation of model(s).\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        \n",
    "        self.pooled_acc         = \"Not yet defined\"\n",
    "        self.pooled_f1_macro    = \"Not yet defined\"\n",
    "        self.pooled_f1_weighted = \"Not yet defined\" \n",
    "        \n",
    "        self.mean_acc           = (\"Not yet defined\", \"Not yet defined\")\n",
    "        self.mean_f1_macro      = (\"Not yet defined\", \"Not yet defined\")\n",
    "        self.mean_f1_weighted   = (\"Not yet defined\", \"Not yet defined\")\n",
    "        \n",
    "        self.corr_l_acc         = \"Not yet defined\"\n",
    "        self.corr_l_f1_macro    = \"Not yet defined\"\n",
    "        self.corr_l_f1_weighted = \"Not yet defined\"\n",
    "        \n",
    "        self.confusion = {\"Not yet defined\": {\"Not yet defined\": \"Not yet defined\"}}\n",
    "        self.metrics_dict = {\"accuracy\": [\"Not yet defined\", \"Not yet defined\"], \n",
    "                             \"f1_macro\": [\"Not yet defined\", \"Not yet defined\"], \n",
    "                             \"f1_weighted\": [\"Not yet defined\", \"Not yet defined\"]}\n",
    "\n",
    "    def best_case(self, metric):\n",
    "        \"\"\" Returns the file which has the best performance score with respect \n",
    "            to a metric.\n",
    "        \"\"\"\n",
    "        m_list = self.metrics_dict[metric]\n",
    "        zic_zac = False if metric == \"mse\" else True\n",
    "        m_list.sort(key=operator.itemgetter(1), reverse=zic_zac)\n",
    "        return m_list[0][0]\n",
    "    \n",
    "    def best_cases(self, metric, n):\n",
    "        \"\"\" Returns a list of the N files which has the best performance score \n",
    "            with respect to a metric.\n",
    "        \"\"\"\n",
    "        m_list = self.metrics_dict[metric]\n",
    "        zic_zac = False if metric == \"mse\" else True\n",
    "        m_list.sort(key=operator.itemgetter(1), reverse=zic_zac)\n",
    "        files, values = zip(*m_list)\n",
    "        return list(files[:n])\n",
    "    \n",
    "    def worst_case(self, metric):\n",
    "        \"\"\" Returns the file which has the best performance score with respect \n",
    "            to a metric.\n",
    "        \"\"\"\n",
    "        m_list = self.metrics_dict[metric]\n",
    "        zic_zac = True if metric == \"mse\" else False\n",
    "        m_list.sort(key=operator.itemgetter(1), reverse=zic_zac)\n",
    "        return m_list[0][0]\n",
    "\n",
    "    def worst_cases(self, metric, n):\n",
    "        \"\"\" Returns a list of the N files which has the best performance score \n",
    "            with respect to a metric.\n",
    "        \"\"\"\n",
    "        m_list = self.metrics_dict[metric]\n",
    "        zic_zac = True if metric == \"mse\" else False\n",
    "        m_list.sort(key=operator.itemgetter(1), reverse=zic_zac)\n",
    "        files, values = zip(*m_list)\n",
    "        return list(files[:n])\n",
    " \n",
    "    def summary(self):\n",
    "        \"\"\" Summarises an evaluation. Returns string.\"\"\"\n",
    "        summary  = \"\\n\".join([f\"Model {self.name} performs as follows:\", \n",
    "                      f\"Pooled Accuracy: {self.pooled_acc}\",\n",
    "                      f\"Pooled F1_macro: {self.pooled_f1_macro}\",\n",
    "                      f\"Pooled F1_weighted: {self.pooled_f1_weighted}\",\n",
    "                              \n",
    "                      f\"Mean Accuracy: {self.mean_acc[0]} (std = {self.mean_acc[1]})\",\n",
    "                      f\"Mean F1_macro: {self.mean_f1_macro[0]} (std = {self.mean_f1_macro[1]})\",\n",
    "                      f\"Mean F1_weighted: {self.mean_f1_weighted[0]} (std = {self.mean_f1_weighted[1]})\",\n",
    "                      \n",
    "                      f\"Correlation sentence length and accuracy: {self.corr_l_acc}\",\n",
    "                      f\"Correlation sentence length and F1_macro: {self.corr_l_f1_macro}\",\n",
    "                      f\"Correlation sentence length and F1_weighted: {self.corr_l_f1_weighted}\"]) \n",
    "        return summary\n",
    "    \n",
    "    def confusion_matrix(self):\n",
    "        \"\"\" Returns and prints a confusion matrix. \n",
    "        \"\"\"\n",
    "        \n",
    "        srl_labels = list(self.confusion.keys())\n",
    "        \n",
    "        matrix = [[\"\"] + srl_labels] # headings\n",
    "        for l in srl_labels:\n",
    "            row = [l]\n",
    "            for k in srl_labels:\n",
    "                row.append(str(self.confusion[l][k]))\n",
    "            matrix.append(row)\n",
    "        \n",
    "        txt = \"\\n\".join([\"\\t\".join(row) for row in matrix])\n",
    "        \n",
    "        return txt\n",
    "    \n",
    "    def save(self, metric, directory=dir_for_evaluations):\n",
    "        \"\"\" Writes the summary of an evaluation to a text file (at some diectory).\"\"\"\n",
    "        \n",
    "        summary = self.summary()\n",
    "        confusion_matrix = self.confusion_matrix()\n",
    "        best_sentences = \"\\n\".join([f\"Best sentences ({metric}):\"] + self.best_cases(metric, 5))\n",
    "        worst_sentences = \"\\n\".join([f\"Worst sentences ({metric}):\"] + self.worst_cases(metric, 5))\n",
    "        \n",
    "        output_to_save = summary + \"\\n\" + confusion_matrix + \"\\n\" + best_sentences + \"\\n\" + worst_sentences\n",
    "        \n",
    "        with open(f\"{directory}{self.name}_{metric}.txt\", \"w\") as e:\n",
    "            e.write(output_to_save)\n",
    "    \n",
    "    def print_summary(self):\n",
    "        \"\"\" Prints out the summary of an evaluation.\n",
    "        \"\"\"\n",
    "        summary = self.summary()\n",
    "        print(summary)\n",
    "        \n",
    "    def print_confusion_matrix(self):\n",
    "        \"\"\" Prints out the confusion matrix.\n",
    "        \"\"\"\n",
    "        c_matrix = self.confusion_matrix()\n",
    "        print(c_matrix)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluator(model, name, test_data = test, srl_labels = lst_labels, detach_me=False):\n",
    "    \"\"\" \n",
    "    \"\"\"\n",
    "    t1 = time.perf_counter()\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    evaluation = Evaluation(name)\n",
    "    \n",
    "    prediction_pooled = [] # to collect all predictions\n",
    "    truth_pooled = []      # to collect all true labels\n",
    "    seq_lengths = []       # to collect the length of sentences\n",
    "    confusion = {label: {label: 0 for label in srl_labels} for label in srl_labels} # for confusion matrix\n",
    "    metrics_calc = {\"accuracy\": [], \"f1_macro\": [], \"f1_weighted\": []} # to collect accuracy and f1 for every sentence\n",
    "    \n",
    "    for batch in test_data:\n",
    "        sentence = batch.sentence\n",
    "        predicate = batch.predicate\n",
    "        truth = batch.srlabel\n",
    "            \n",
    "        if detach_me == True: # to avoid some CUDA memory shortage issues\n",
    "            prediction = model(sentence, predicate, truth).detach().to(\"cpu\")\n",
    "            truth = batch.srlabel.detach().to(\"cpu\")\n",
    "        \n",
    "        else:\n",
    "            prediction = model(sentence, predicate, truth) \n",
    "        \n",
    "        batched_pred_labels = prediction[:, 1:, :].argmax(2) # first element of the sequences will never match (0 with <sos>)\n",
    "        batched_true_labels = truth[:, 1:]\n",
    "        \n",
    "        bsz = batched_pred_labels.shape[0]\n",
    "        \n",
    "        for b in range(bsz):\n",
    "            pidx           = predicate[b].tolist().index(1)\n",
    "            lst_sent       = [vocab.itos[token] for token in sentence[b]]\n",
    "            lst_sent[pidx] = lst_sent[pidx].upper() # mark the predicate by using capitals\n",
    "            str_sent       = \" \".join(lst_sent) \n",
    "            seq_len        = len([x for x in sentence[b] if vocab.itos[x] not in [\"<pad>\", \"<sos>\", \"<eos>\"]])\n",
    "            pred_labels    = batched_pred_labels[b].tolist()\n",
    "            true_labels    = batched_true_labels[b].tolist()\n",
    "            lst_lab        = [labels.itos[token] for token in pred_labels]\n",
    "            annot_sent     = \" \".join([f\"{w}/{sr}\" for w, sr in zip(lst_sent, lst_lab)])\n",
    "            \n",
    "            accuracy, f1_macro, f1_weighted = metrics(true_labels, pred_labels)\n",
    "            \n",
    "            prediction_pooled.extend(pred_labels)\n",
    "            truth_pooled.extend(true_labels)\n",
    "            seq_lengths.append(seq_len)\n",
    "            \n",
    "            for p, t in zip(pred_labels, true_labels):\n",
    "                confusion[srl_labels[p]][srl_labels[t]] += 1\n",
    "                \n",
    "            for m, v in zip([\"accuracy\", \"f1_macro\", \"f1_weighted\"], [accuracy, f1_macro, f1_weighted]):\n",
    "                metrics_calc[m].append( (f\"{str_sent}\\n{annot_sent}\", v) )\n",
    "    \n",
    "    pooled_accuracy, pooled_f1_macro, pooled_f1_weighted = metrics(truth_pooled, prediction_pooled)\n",
    "\n",
    "    lst_accuracy    = list(zip(*metrics_calc[\"accuracy\"]))[1]\n",
    "    lst_f1_macro    = list(zip(*metrics_calc[\"f1_macro\"]))[1]\n",
    "    lst_f1_weighted = list(zip(*metrics_calc[\"f1_weighted\"]))[1]\n",
    "\n",
    "    evaluation.pooled_acc         = pooled_accuracy\n",
    "    evaluation.pooled_f1_macro    = pooled_f1_macro\n",
    "    evaluation.pooled_f1_weighted = pooled_f1_weighted \n",
    "\n",
    "    evaluation.mean_acc         = mean(lst_accuracy)\n",
    "    evaluation.mean_f1_macro    = mean(lst_f1_macro)\n",
    "    evaluation.mean_f1_weighted = mean(lst_f1_weighted)\n",
    "    \n",
    "    evaluation.corr_l_acc         = np.corrcoef(lst_accuracy, seq_lengths)[0][1] \n",
    "    evaluation.corr_l_f1_macro    = np.corrcoef(lst_f1_macro, seq_lengths)[0][1]\n",
    "    evaluation.corr_l_f1_weighted = np.corrcoef(lst_f1_weighted, seq_lengths)[0][1]\n",
    "\n",
    "    evaluation.confusion    = confusion\n",
    "    evaluation.metrics_dict = metrics_calc\n",
    "    \n",
    "    t2 = time.perf_counter()\n",
    "    passed_time = t2 - t1\n",
    "    print(\"Done! ({} m., {} s.)\".format(int(passed_time/60), int(passed_time%60)))\n",
    "    \n",
    "    return evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "srl_evaluation = evaluator(my_SRLLabeler, model_name, detach_me = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "srl_evaluation.print_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "srl_evaluation.print_confusion_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "srl_evaluation.best_case(\"f1_macro\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "srl_evaluation.best_case(\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "srl_evaluation.best_cases(\"accuracy\", 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "srl_evaluation.worst_case(\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "srl_evaluation.worst_cases(\"f1_macro\", 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "srl_evaluation.save(\"accuracy\")\n",
    "srl_evaluation.save(\"f1_macro\")\n",
    "srl_evaluation.save(\"f1_weighted\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finding specific sentences ... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "finding_sentence = srl_evaluation.metrics_dict[\"f1_macro\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent, val = zip(*finding_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for s in sent:\n",
    "    f = s.split(\"\\n\")\n",
    "    if f[0] == \"<sos> next stop WAS crosshouse hospital in kilmarnock , followed by ayr hospital . <eos>\":\n",
    "        print(f[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
