{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This Jupyter Notebook contains the code for training and evaluating a Sequence to Sequence (seq2seq) Encoder - Decoder model for semantic role labeling (SRL), as project for the course LT2326, autumn 2021. Data preparation is defined and handled elsewhere; see `data_builder.ipynb`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random, time, operator\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchtext\n",
    "import torch.nn.functional as F\n",
    "#from torchtext.data import Field, BucketIterator, Iterator, TabularDataset\n",
    "from torchtext.legacy.data import Field, BucketIterator, Iterator, TabularDataset # Needed for running this on my laptop\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Meta variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define where to get and store data and which device to use. For test pipline with less data during development set `mini_training`to `True`; when using complete dataset, set to `False`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#device = torch.device('cuda:0')\n",
    "device = torch.device('cpu')\n",
    "\n",
    "my_data_directory = \"../../data/\" # where to store files\n",
    "my_models_directory = \"../../models/\"\n",
    "\n",
    "mini_testing = True\n",
    "my_train_file = \"mini_train.csv\" if mini_testing == True else \"train.csv\"\n",
    "my_test_file  = \"mini_test.csv\" if mini_testing == True else \"test.csv\"\n",
    "\n",
    "dir_for_evaluations = \"../../evals/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define batchsize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 3 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataloader(directory  = my_data_directory,\n",
    "               train_file = my_train_file,\n",
    "               test_file  = my_test_file,\n",
    "               batch      = batch_size):\n",
    "    \n",
    "    num_whitespacer = lambda x: [int(e) for e in x.split(\" \")]\n",
    "    \n",
    "    SENTENCE = Field(lower = True, \n",
    "                     batch_first = True, \n",
    "                     init_token = \"<sos>\", \n",
    "                     eos_token = \"<eos>\")\n",
    "    \n",
    "    PREDICATE = Field(tokenize = num_whitespacer, # Here might be some problems ...\n",
    "                      batch_first = True, \n",
    "                      pad_token = 0,\n",
    "                      use_vocab = False,\n",
    "                      init_token = 0, \n",
    "                      eos_token = 0) \n",
    "    \n",
    "    SRLABEL = Field(batch_first = True, \n",
    "                    init_token = \"<sos>\", \n",
    "                    eos_token = \"<eos>\")\n",
    "    \n",
    "    my_fields = [(\"sentence\", SENTENCE),\n",
    "                 (\"predicate\", PREDICATE),\n",
    "                 (\"srlabel\", SRLABEL)]\n",
    "    \n",
    "    train, test = TabularDataset.splits(path   = directory,\n",
    "                                        train  = train_file,\n",
    "                                        test   = test_file,\n",
    "                                        format = 'csv',\n",
    "                                        fields = my_fields,\n",
    "                                        csv_reader_params = {'delimiter':'\\t',\n",
    "                                                             'quotechar':'Â¤'}) # Seems not to be in data\n",
    "    SENTENCE.build_vocab(train)\n",
    "    SRLABEL.build_vocab(train)  \n",
    "\n",
    "    train_iter, test_iter = BucketIterator.splits((train, test),\n",
    "                                                  batch_size        = batch,\n",
    "                                                  sort_within_batch = True,\n",
    "                                                  sort_key          = lambda x: len(x.sentence),\n",
    "                                                  shuffle           = True,\n",
    "                                                  device            = device)\n",
    "\n",
    "    return train_iter, test_iter, SENTENCE.vocab, SRLABEL.vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calling dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test, vocab, labels = dataloader()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Definitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The encoder encodes sentence-predicate pairs through LSTMs. In forward pass, it returns *the final cell state* and *the final hidden state* (somtimes referred to as the *context vector*).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SRL_Encoder(nn.Module):\n",
    "    def __init__(self, voc_size, embedding_size, hidden_size, n_layers, p_dropout):  \n",
    "        super(SRL_Encoder, self).__init__()\n",
    "        \n",
    "        self.embeddings = nn.Embedding(voc_size, embedding_size)\n",
    "        self.sp_pair = embedding_size + 1 # emedded sentence + predicate vector\n",
    "        self.hidden_size = hidden_size\n",
    "        self.n_layers = n_layers\n",
    "        \n",
    "        self.rnn = nn.LSTM(self.sp_pair, \n",
    "                           self.hidden_size, \n",
    "                           num_layers = self.n_layers,\n",
    "                           dropout = p_dropout,\n",
    "                           #bidirectional=True, # !\n",
    "                           batch_first=True) # !\n",
    "        self.dropout = nn.Dropout(p_dropout)\n",
    "        \n",
    "    def forward(self, sentences, pred_vec):\n",
    "        \n",
    "        embeddings = self.embeddings(sentences)\n",
    "        pred_vec = pred_vec.unsqueeze(2)        \n",
    "        sentence_pred_pair = torch.cat((embeddings, pred_vec), dim=2)\n",
    "        contextualized_embedding, (hidden_final, cell_final) = self.rnn(sentence_pred_pair)\n",
    "        \n",
    "        #print(\"ENC, hidden:\", hidden_final.shape)\n",
    "        #print(\"ENC, cell:\", cell_final.shape)\n",
    "        \n",
    "        return hidden_final, cell_final\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The decoder predicts the next element of a sequence based on the previous sequence and the final cell state and the final hidden state of that sequence through an LSTM. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SRL_Decoder(nn.Module):\n",
    "    def __init__(self, n_labels, embedding_size, hidden_size, n_layers, p_dropout):  \n",
    "        super(SRL_Decoder, self).__init__()\n",
    " \n",
    "        self.hidden_size = hidden_size\n",
    "        self.n_layers = n_layers\n",
    "        self.n_labels = n_labels\n",
    "        \n",
    "        self.embeddings = nn.Embedding(n_labels, embedding_size) # ?\n",
    "        self.rnn = nn.LSTM(embedding_size, \n",
    "                           self.hidden_size, \n",
    "                           num_layers = self.n_layers, \n",
    "                           batch_first=True,\n",
    "                           #bidirectional=True,\n",
    "                           dropout = p_dropout)\n",
    "        self.classifier = nn.Linear(hidden_size, self.n_labels)\n",
    "        \n",
    "    def forward(self, previous, hidden, cell):\n",
    "        \n",
    "        #previous = previous.unsqueeze(1)\n",
    "        \n",
    "        embedded = self.embeddings(previous)\n",
    "        #print(\"DEC, emb_previous:\", embedded.shape)\n",
    "        output, (hidden, cell) = self.rnn(embedded, (hidden, cell))\n",
    "        prediction = self.classifier(output)\n",
    "        \n",
    "        return prediction, hidden, cell\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder - Decoder Interface"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the `SRL_Seq2SeqLabeler`, the context vector (i.e. final cell and hidden states) of the `Encoder` together with the start token `<sos>` serves as inputs to predict a sequence of semantic role labels. After the first prediction, the decoder uses its own predictions as the input sequence to predict the next token. This model uses teacher forcing, meaning that, at some proportion of the time, as defined by a teacher force ratio (TFR), the true label of the sequence is put into the sequence, instead of the prediction by the encoder. \n",
    "\n",
    "Minor note: the classification problem engaged with here is a one-to-one mapping. Translation problems more generally might involve mappings of sequences of different lengths. To handle mappings of different lengths properly would require further work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SRL_Seq2SeqLabeler(nn.Module):\n",
    "    def __init__(self, encoder, decoder):  \n",
    "        super(SRL_Seq2SeqLabeler, self).__init__()\n",
    "        \n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        \n",
    "        assert encoder.hidden_size == decoder.hidden_size, \"hidden dimension of encoder must be equal to that of decoder\"\n",
    "        assert encoder.n_layers == decoder.n_layers, \"n_layers of encoder must be equal to that of decoder\"\n",
    "        \n",
    "    def forward(self, sentence, predicate, srl_labels, tfr = None): # tfr = teacher forcing ratio\n",
    "\n",
    "        batch_size = sentence.shape[0]\n",
    "        seq_len = sentence.shape[1]\n",
    "        n_labels = self.decoder.n_labels\n",
    "\n",
    "        outputs = torch.zeros(batch_size, seq_len, n_labels).to(device) # for storage\n",
    "\n",
    "        hidden, cell = self.encoder(sentence, predicate)\n",
    "\n",
    "        seq_element = srl_labels[:, 0].unsqueeze(1) # start of sentence token; index of <sos>\n",
    "        #print(\"S2S, Seq_elem, prior:\", seq_element)\n",
    "\n",
    "        for l in range(1, seq_len): # Note: starts from 1; first column of outputs will \"remain\" 0\n",
    "            \n",
    "            #print(\"S2S, Seq_elem:\", seq_element.shape)\n",
    "\n",
    "            output, hidden, cell = self.decoder(seq_element, hidden, cell)\n",
    "            #print(\"S2S, Outputs:\", outputs.shape)\n",
    "            #print(\"S2S, Output:\", output.shape)\n",
    "            outputs[:, l, :] = output.squeeze()\n",
    "            best_guess = output.argmax(2)\n",
    "            #print(\"S2S, Best guess:\", best_guess)\n",
    "\n",
    "            if tfr != None:\n",
    "                teacher_force = random.random() < tfr\n",
    "#                 if teacher_force:\n",
    "#                     print(\"TF\")\n",
    "#                 else:\n",
    "#                     print(\"No TF\")\n",
    "                seq_element = srl_labels[:, l].unsqueeze(1) if teacher_force else best_guess\n",
    "            else:\n",
    "                seq_element = best_guess\n",
    "\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General training function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainer(model, # Must be an instance of a model!\n",
    "            name_of_model,\n",
    "            learning_rate,\n",
    "            epochs,\n",
    "            data,\n",
    "            my_tfr = 0.5,\n",
    "            val_data = None,\n",
    "            save_model = False,\n",
    "            directory = my_models_directory,\n",
    "            my_loss_function = nn.CrossEntropyLoss,\n",
    "            my_optimizer = optim.Adam\n",
    "           ):\n",
    "    \"\"\" Specifices a general training procedure for a model. \n",
    "        Note: trainer() requires an instantiated model as model argument. \n",
    "    \"\"\"\n",
    "    \n",
    "    optimizer = my_optimizer(model.parameters(), lr=learning_rate)    \n",
    "    \n",
    "    model.to(device)\n",
    "    model.train()\n",
    "    \n",
    "    pad_idx = labels.stoi[\"<pad>\"]\n",
    "    \n",
    "    loss_function = my_loss_function(ignore_index=pad_idx) # We ignorew pad token in loss calculation\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        epoch_loss = 0\n",
    "        \n",
    "        for i, batch in enumerate(data):\n",
    "            optimizer.zero_grad # reset gradients\n",
    "            \n",
    "            sentence = batch.sentence\n",
    "            predicate = batch.predicate\n",
    "            targets = batch.srlabel\n",
    "            #print(\"TRAIN, Targets:\", targets.shape)\n",
    "            \n",
    "            output = model(sentence, predicate, targets, tfr = my_tfr)\n",
    "            #print(\"TRAIN, Output:\", output.shape)\n",
    "            \n",
    "            # Before calculation of loss outputs and targets needs to be \"aligned\", so to speak.\n",
    "            # Outputs are of shape [batch, seq_len, dimension]. Targets of shape [batch, seq_len]\n",
    "            # The representation of the first element of the output sequence will be 0s (see \n",
    "            # above). The first element of the targets will be <sos>. We ignore these first elements\n",
    "            # in calculating the loss. \n",
    "            \n",
    "            # Moreover, our loss function (CrossEntropyLoss) expects predicitons as [n_predictions, \n",
    "            # n_classes] and targets as [n_predictions]. Here, n_predictions = batch_size * sequence_\n",
    "            # length. \n",
    "            \n",
    "            bsz = output.shape[0]\n",
    "            length = output.shape[1]\n",
    "            output_dim = output.shape[2]\n",
    "        \n",
    "            output = output[:, 1:, :].reshape(bsz*(length - 1), output_dim) # first token (\"column\") being zeroes\n",
    "            targets = targets[:, 1:].flatten() # first token being <sos>\n",
    "            \n",
    "            # Now, calculate the loss\n",
    "            loss = loss_function(output, targets)\n",
    "            \n",
    "            epoch_loss += loss.item()\n",
    "            loss.backward() # compute gradients\n",
    "            optimizer.step() # update parameters\n",
    "            #break\n",
    "            \n",
    "        print(f\"Epoch: {epoch+1} (out of {epochs}); total loss: {epoch_loss}.\")\n",
    "            \n",
    "        if val_data != None:\n",
    "            model.eval()\n",
    "            # Here we could do some evaluation of model progress, but I have ignored this for now. \n",
    "            model.train()\n",
    "            \n",
    "    if save_model == True:\n",
    "        torch.save(model, directory+name_of_model+\".pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## * Hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some examples from the web:\n",
    "\n",
    "|Author            |No. of layers|Batch Size|Embeddingsdim.|Hidden Dim.|Dropout|WWW             |\n",
    "|------------------|-------------|----------|--------------|-----------|-------|---------|\n",
    "|Ziqi Yuan         |            2|       128|           256|        512|    0.5|https://www.kaggle.com/columbine/seq2seq-pytorch|\n",
    "|Balakrishnakumar V|            2|        32|           300|       1024|    0.5|https://towardsdatascience.com/a-comprehensive-guide-to-neural-machine-translation-using-seq2sequence-modelling-using-pytorch-41c9b84ba350|\n",
    "|Matthew Inkawhich |            2|        64|           ?  |        500|    0.1|https://pytorch.org/tutorials/beginner/deploy_seq2seq_hybrid_frontend_tutorial.html|\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_epochs = 3\n",
    "learning_rate = 0.01\n",
    "# batch size defined before calling dataloader\n",
    "\n",
    "vocab_size = len(vocab)\n",
    "num_labels = len(labels)\n",
    "emb_sizeE = 50 \n",
    "emb_sizeD = int(num_labels/2) # embeddings for labels\n",
    "hid_size_encoder = 100\n",
    "hid_size_decoder = hid_size_encoder # ?\n",
    "\n",
    "num_layers = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instantiate models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_encoder = SRL_Encoder(vocab_size, emb_sizeE, hid_size_encoder, num_layers, p_dropout=0.2)\n",
    "my_decoder = SRL_Decoder(num_labels, emb_sizeD, hid_size_decoder, num_layers, p_dropout=0.2)\n",
    "my_SRLLabeler = SRL_Seq2SeqLabeler(my_encoder, my_decoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Know your models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Module.parameters of SRL_Encoder(\n",
       "  (embeddings): Embedding(1054, 50)\n",
       "  (rnn): LSTM(51, 100, num_layers=2, batch_first=True, dropout=0.2)\n",
       "  (dropout): Dropout(p=0.2, inplace=False)\n",
       ")>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_encoder.parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Module.parameters of SRL_Decoder(\n",
       "  (embeddings): Embedding(34, 17)\n",
       "  (rnn): LSTM(17, 100, num_layers=2, batch_first=True, dropout=0.2)\n",
       "  (classifier): Linear(in_features=100, out_features=34, bias=True)\n",
       ")>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_decoder.parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Name model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model_name = f\"srl_b{batch_size}_e{my_epochs}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 (out of 3); total loss: 54.00996106863022.\n",
      "Epoch: 2 (out of 3); total loss: 70.22633963823318.\n",
      "Epoch: 3 (out of 3); total loss: 52.72664314508438.\n"
     ]
    }
   ],
   "source": [
    "trainer(model = my_SRLLabeler, \n",
    "        name_of_model = model_name, \n",
    "        learning_rate = learning_rate, \n",
    "        epochs = my_epochs, \n",
    "        data = train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For evaluation, several functions are defined. Also, a class for managing information from evalution is defined. Overall, the model is evaluated by\n",
    "*    its accuracy (number of correct predictions / total)\n",
    "*    F1 averaged globally\n",
    "*    F1 averaged by mean\n",
    "*    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The list of labels\n",
    "lst_labels = [labels.itos[x] for x in range(len(labels))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<unk>',\n",
       " '<pad>',\n",
       " '<sos>',\n",
       " '<eos>',\n",
       " '0',\n",
       " 'I-ARG1',\n",
       " 'I-ARG2',\n",
       " 'B-PRD',\n",
       " 'B-ARG1',\n",
       " 'I-ARGM-MNR',\n",
       " 'I-ARG0',\n",
       " 'B-ARG0',\n",
       " 'I-ARGM-ADV',\n",
       " 'B-ARG2',\n",
       " 'I-ARGM-PRP',\n",
       " 'I-ARGM-TMP',\n",
       " 'B-ARGM-MNR',\n",
       " 'B-ARGM-TMP',\n",
       " 'I-ARGM-LOC',\n",
       " 'B-ARGM-MOD',\n",
       " 'I-ARGM-DIR',\n",
       " 'B-ARGM-ADV',\n",
       " 'B-ARGM-LOC',\n",
       " 'B-ARGM-NEG',\n",
       " 'B-ARGM-PRP',\n",
       " 'B-R-ARG1',\n",
       " 'I-ARGM-PNC',\n",
       " 'B-ARG4',\n",
       " 'B-ARGM-CAU',\n",
       " 'B-ARGM-DIR',\n",
       " 'B-ARGM-DIS',\n",
       " 'B-ARGM-PNC',\n",
       " 'B-R-ARG0',\n",
       " 'I-ARG4']"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lst_labels #remove stuff?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# baseline?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions and a class for handling information "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOURCES:\n",
    "# https://www.baeldung.com/cs/multi-class-f1-score\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html\n",
    "\n",
    "def metrics(prediction, truth, labels = lst_labels):\n",
    "    \"\"\" Calculates accuracy and F1, given two sequences (lists, arrays) of labels. Since, \n",
    "        these metrices here are calculated for multi-label classification, two versions \n",
    "        of F1 are calculated: \"macro\" and \"micro\", where the former is the mean of F1 for\n",
    "        each label, and the latter is calculated globally by counting the total true \n",
    "        positives, etc.\n",
    "    \"\"\"\n",
    "    \n",
    "    f1_dict = {}\n",
    "    \n",
    "    accuracy = accuracy_score(truth, prediction)\n",
    "    f1_lsted = f1_score(truth, prediction, labels = labels, average = None)\n",
    "    f1_macro = f1_score(truth, prediction, labels = labels, average = \"macro\") # Calculate metrics for each label, and find their unweighted mean. Does not take label imbalance into account.\n",
    "    f1_micro = f1_score(truth, prediction, labels = labels, average = \"micro\") # Calculate metrics globally by counting the total true positives, false negatives and false positives.\n",
    "    \n",
    "    for label in labels:\n",
    "        f1_dict[label] = f1_lsted[labels.index(label)] # overkill?\n",
    "    \n",
    "    return accuracy, f1_macro, f1_micro, f1_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean(array):\n",
    "    \"\"\" Calculates the mean and standard deviation of an aray of numbers.\n",
    "    \"\"\"\n",
    "    mean = np.mean(array)\n",
    "    std  = np.std(array)\n",
    "    return mean, std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Evaluation:\n",
    "    \"\"\" For storing and handling information from the evaluation of model(s).\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        \n",
    "        self.pooled_acc      = \"Not yet defined\"\n",
    "        self.pooled_f1_macro = \"Not yet defined\"\n",
    "        self.pooled_f1_micro = \"Not yet defined\" \n",
    "        \n",
    "        self.mean_acc        = (\"Not yet defined\", \"Not yet defined\")\n",
    "        self.mean_f1_macro   = (\"Not yet defined\", \"Not yet defined\")\n",
    "        self.mean_f1_micro   = (\"Not yet defined\", \"Not yet defined\")\n",
    "        \n",
    "        self.corr_l_acc      = \"Not yet defined\"\n",
    "        self.corr_l_f1_macro = \"Not yet defined\"\n",
    "        self.corr_l_f1_micro = \"Not yet defined\"\n",
    "        \n",
    "        self.confusion = {\"Not yet defined\": {\"Not yet defined\": \"Not yet defined\"}}\n",
    "        self.metrics_dict = {\"accuracy\": [\"Not yet defined\", \"Not yet defined\"], \n",
    "                             \"f1_macro\": [\"Not yet defined\", \"Not yet defined\"], \n",
    "                             \"f1_micro\": [\"Not yet defined\", \"Not yet defined\"]}\n",
    "\n",
    "    def best_case(self, metric):\n",
    "        \"\"\" Returns the file which has the best performance score with respect \n",
    "            to a metric.\n",
    "        \"\"\"\n",
    "        m_list = self.metrics_dict[metric]\n",
    "        zic_zac = False if metric == \"mse\" else True\n",
    "        m_list.sort(key=operator.itemgetter(1), reverse=zic_zac)\n",
    "        return m_list[0][0]\n",
    "    \n",
    "    def best_cases(self, metric, n):\n",
    "        \"\"\" Returns a list of the N files which has the best performance score \n",
    "            with respect to a metric.\n",
    "        \"\"\"\n",
    "        m_list = self.metrics_dict[metric]\n",
    "        zic_zac = False if metric == \"mse\" else True\n",
    "        m_list.sort(key=operator.itemgetter(1), reverse=zic_zac)\n",
    "        files, values = zip(*m_list)\n",
    "        return list(files[:n])\n",
    "    \n",
    "    def worst_case(self, metric):\n",
    "        \"\"\" Returns the file which has the best performance score with respect \n",
    "            to a metric.\n",
    "        \"\"\"\n",
    "        m_list = self.metrics_dict[metric]\n",
    "        zic_zac = True if metric == \"mse\" else False\n",
    "        m_list.sort(key=operator.itemgetter(1), reverse=zic_zac)\n",
    "        return m_list[0][0]\n",
    "\n",
    "    def worst_cases(self, metric, n):\n",
    "        \"\"\" Returns a list of the N files which has the best performance score \n",
    "            with respect to a metric.\n",
    "        \"\"\"\n",
    "        m_list = self.metrics_dict[metric]\n",
    "        zic_zac = True if metric == \"mse\" else False\n",
    "        m_list.sort(key=operator.itemgetter(1), reverse=zic_zac)\n",
    "        files, values = zip(*m_list)\n",
    "        return list(files[:n])\n",
    " \n",
    "    def summary(self):\n",
    "        \"\"\" Summarises an evaluation. Returns string.\"\"\"\n",
    "        summary  = \"\\n\".join([f\"Model {self.name} performs as follows:\", \n",
    "                      f\"Pooled Accuracy: {self.pooled_acc}\",\n",
    "                      f\"Pooled F1_macro: {self.pooled_f1_macro}\",\n",
    "                      f\"Pooled F1_micro: {self.pooled_f1_micro}\",\n",
    "                              \n",
    "                      f\"Mean Accuracy: {self.mean_acc[0]} (std = {self.mean_acc[1]})\",\n",
    "                      f\"Mean F1_macro: {self.mean_f1_macro[0]} (std = {self.mean_f1_macro[1]})\",\n",
    "                      f\"Mean F1_micro: {self.mean_f1_micro[0]} (std = {self.mean_f1_micro[1]})\",\n",
    "                      \n",
    "                      f\"Correlation sentence length and accuracy: {self.corr_l_acc}\",\n",
    "                      f\"Correlation sentence length and F1_macro: {self.corr_l_f1_macro}\",\n",
    "                      f\"Correlation sentence length and F1_micro: {self.corr_l_f1_micro}\"]) \n",
    "        return summary\n",
    "    \n",
    "    def confusion_matrix(self):\n",
    "        \"\"\" Returns and prints a confusion matrix. \n",
    "        \"\"\"\n",
    "        \n",
    "        labels = list(self.confusion.keys())\n",
    "        \n",
    "        matrix = [[\"\"] + labels] # headings\n",
    "        for l in labels:\n",
    "            row = [l]\n",
    "            for k in labels:\n",
    "                row.append(str(self.confusion[l][k]))\n",
    "            matrix.append(row)\n",
    "            \n",
    "        #matrix_txt = [[str(cell) for cell in row] for row in matrix]\n",
    "        \n",
    "        txt = \"\\n\".join([\"\\t\".join(row) for row in matrix])\n",
    "        \n",
    "        #print(txt)\n",
    "        return txt\n",
    "    \n",
    "    def save(self, metric, directory=dir_for_evaluations):\n",
    "        \"\"\" Writes the summary of an evaluation to a text file (at some diectory).\"\"\"\n",
    "        \n",
    "        summary = self.summary()\n",
    "        confusion_matrix = self.confusion_matrix()\n",
    "        best_sentences = \"\\n\".join([f\"Best sentences ({metric}):\"] + self.best_cases(metric, 5))\n",
    "        worst_sentences = \"\\n\".join([f\"Worst sentences ({metric}):\"] + self.worst_cases(metric, 5))\n",
    "        \n",
    "        output_to_save = summary + \"\\n\" + confusion_matrix + \"\\n\" + best_sentences + \"\\n\" + worst_sentences\n",
    "        \n",
    "        with open(f\"{directory}{self.name}_{metric}.txt\", \"w\") as e:\n",
    "            e.write(output_to_save)\n",
    "    \n",
    "    def print_summary(self):\n",
    "        \"\"\" Prints out the summary of an evaluation.\n",
    "        \"\"\"\n",
    "        summary = self.summary()\n",
    "        print(summary)\n",
    "        \n",
    "    def print_confusion_matrix(self):\n",
    "        \"\"\" Prints out the confusion matrix.\n",
    "        \"\"\"\n",
    "        c_matrix = self.confusion_matrix()\n",
    "        print(c_matrix)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluator(model, name, test_data = test, labels = lst_labels, detach_me=False):\n",
    "    \"\"\" \n",
    "    \"\"\"\n",
    "    t1 = time.perf_counter()\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    evaluation = Evaluation(name)\n",
    "    \n",
    "    prediction_pooled = [] # to collect all predictions\n",
    "    truth_pooled = []      # to collect all true labels\n",
    "    seq_lengths = []       # to collect the length of sentences\n",
    "    confusion = {label: {label: 0 for label in labels} for label in labels} # for confusion matrix\n",
    "    metrics_calc = {\"accuracy\": [], \"f1_macro\": [], \"f1_micro\": []} # to collect accuracy and f1 for every sentence\n",
    "    \n",
    "    #i=1 # in order to print out progress\n",
    "    for batch in test_data:\n",
    "\n",
    "        sentence = batch.sentence\n",
    "        predicate = batch.predicate\n",
    "            \n",
    "        if detach_me == True: # to avoid some CUDA memory shortage issues\n",
    "            truth = batch.srlabel.detach().to(\"cpu\")\n",
    "            prediction = model(sentence, predicate, truth).detach().to(\"cpu\") \n",
    "            \n",
    "        \n",
    "        else:\n",
    "            truth = batch.srlabel\n",
    "            prediction = model(sentence, predicate, truth) \n",
    "        \n",
    "        batched_pred_labels = prediction[:, 1:, :].argmax(2) # ... should not be batch-wise, but sentence-wise\n",
    "        batched_true_labels = truth[:, 1:]\n",
    "        \n",
    "        bsz = batched_pred_labels.shape[0]\n",
    "        \n",
    "        for b in range(bsz):\n",
    "            str_sent    = \" \".join([vocab.itos[token] for token in sentence[b]]) # to list?\n",
    "            seq_len     = len([x for x in sentence[b] if vocab.itos[x] not in [\"<pad>\", \"<sos>\", \"<eos>\"]])\n",
    "            pred_labels = batched_pred_labels[b].tolist()\n",
    "            true_labels = batched_true_labels[b].tolist()\n",
    "            \n",
    "            accuracy, f1_macro, f1_micro, X = metrics(true_labels, pred_labels)\n",
    "            \n",
    "            prediction_pooled.extend(pred_labels)\n",
    "            truth_pooled.extend(true_labels)\n",
    "            seq_lengths.append(seq_len)\n",
    "            \n",
    "            for p, t in zip(pred_labels, true_labels):\n",
    "                confusion[lst_labels[p]][lst_labels[t]] += 1\n",
    "                \n",
    "            for m, v in zip([\"accuracy\", \"f1_macro\", \"f1_micro\"], [accuracy, f1_macro, f1_micro]):\n",
    "                metrics_calc[m].append( (str_sent, v) )\n",
    "    \n",
    "    #print(prediction_pooled)\n",
    "    \n",
    "    pooled_accuracy, pooled_f1_macro, pooled_f1_micro, X = metrics(truth_pooled, prediction_pooled)\n",
    "\n",
    "    lst_accuracy = list(zip(*metrics_calc[\"accuracy\"]))[1]\n",
    "    lst_f1_macro = list(zip(*metrics_calc[\"f1_macro\"]))[1]\n",
    "    lst_f1_micro = list(zip(*metrics_calc[\"f1_micro\"]))[1]\n",
    "\n",
    "    evaluation.pooled_acc      = pooled_accuracy\n",
    "    evaluation.pooled_f1_macro = pooled_f1_macro\n",
    "    evaluation.pooled_f1_micro = pooled_f1_micro \n",
    "\n",
    "    evaluation.mean_acc        = mean(lst_accuracy)\n",
    "    evaluation.mean_f1_macro   = mean(lst_f1_macro)\n",
    "    evaluation.mean_f1_micro   = mean(lst_f1_micro)\n",
    "\n",
    "    evaluation.corr_l_acc      = np.corrcoef(lst_accuracy, seq_lengths)[0][0] # doubble zero indices due to output of numpy.corrcoef\n",
    "    evaluation.corr_l_f1_macro = np.corrcoef(lst_f1_macro, seq_lengths)[0][0]\n",
    "    evaluation.corr_l_f1_micro = np.corrcoef(lst_f1_micro, seq_lengths)[0][0]\n",
    "\n",
    "    evaluation.confusion       = confusion\n",
    "    evaluation.metrics_dict    = metrics_calc\n",
    "    \n",
    "    t2 = time.perf_counter()\n",
    "    passed_time = t2 - t1\n",
    "    print(\"Done! ({} m., {} s.)\".format(int(passed_time/60), int(passed_time%60)))\n",
    "    \n",
    "    return evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done! (0 m., 0 s.)\n"
     ]
    }
   ],
   "source": [
    "srl_evaluation = evaluator(my_SRLLabeler, model_name, detach_me = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model srl_b3_e3 performs as follows:\n",
      "Pooled Accuracy: 0.6996197718631179\n",
      "Pooled F1_macro: 0.0\n",
      "Pooled F1_micro: 0.0\n",
      "Mean Accuracy: 0.5969852154845512 (std = 0.2673525026264313)\n",
      "Mean F1_macro: 0.0 (std = 0.0)\n",
      "Mean F1_micro: 0.0 (std = 0.0)\n",
      "Correlation sentence length and accuracy: 0.9999999999999998\n",
      "Correlation sentence length and F1_macro: nan\n",
      "Correlation sentence length and F1_micro: nan\n"
     ]
    }
   ],
   "source": [
    "srl_evaluation.print_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t<unk>\t<pad>\t<sos>\t<eos>\t0\tI-ARG1\tI-ARG2\tB-PRD\tB-ARG1\tI-ARGM-MNR\tI-ARG0\tB-ARG0\tI-ARGM-ADV\tB-ARG2\tI-ARGM-PRP\tI-ARGM-TMP\tB-ARGM-MNR\tB-ARGM-TMP\tI-ARGM-LOC\tB-ARGM-MOD\tI-ARGM-DIR\tB-ARGM-ADV\tB-ARGM-LOC\tB-ARGM-NEG\tB-ARGM-PRP\tB-R-ARG1\tI-ARGM-PNC\tB-ARG4\tB-ARGM-CAU\tB-ARGM-DIR\tB-ARGM-DIS\tB-ARGM-PNC\tB-R-ARG0\tI-ARG4\n",
      "<unk>\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\n",
      "<pad>\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\n",
      "<sos>\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\n",
      "<eos>\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\n",
      "0\t23\t128\t0\t30\t1104\t78\t49\t30\t27\t15\t14\t17\t7\t12\t7\t6\t5\t3\t7\t2\t3\t2\t3\t0\t1\t0\t0\t0\t0\t2\t2\t0\t1\t0\n",
      "I-ARG1\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\n",
      "I-ARG2\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\n",
      "B-PRD\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\n",
      "B-ARG1\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\n",
      "I-ARGM-MNR\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\n",
      "I-ARG0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\n",
      "B-ARG0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\n",
      "I-ARGM-ADV\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\n",
      "B-ARG2\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\n",
      "I-ARGM-PRP\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\n",
      "I-ARGM-TMP\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\n",
      "B-ARGM-MNR\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\n",
      "B-ARGM-TMP\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\n",
      "I-ARGM-LOC\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\n",
      "B-ARGM-MOD\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\n",
      "I-ARGM-DIR\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\n",
      "B-ARGM-ADV\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\n",
      "B-ARGM-LOC\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\n",
      "B-ARGM-NEG\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\n",
      "B-ARGM-PRP\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\n",
      "B-R-ARG1\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\n",
      "I-ARGM-PNC\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\n",
      "B-ARG4\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\n",
      "B-ARGM-CAU\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\n",
      "B-ARGM-DIR\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\n",
      "B-ARGM-DIS\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\n",
      "B-ARGM-PNC\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\n",
      "B-R-ARG0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\n",
      "I-ARG4\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\n"
     ]
    }
   ],
   "source": [
    "srl_evaluation.print_confusion_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<sos> the <unk> to statutory <unk> <unk> <unk> <unk> <unk> and <unk> for all part - time workers free health <unk> for all night - <unk> workers legal protection for workers taking <unk> <unk> <unk> helped by <unk> the <unk> period for <unk> <unk> from 2 years to <unk> year <unk> <unk> <unk> <unk> to 18 <unk> <unk> <unk> <unk> from <unk> <unk> to 40 <unk> the above is just a small section of the new <unk> , and <unk> of our <unk> may well of <unk> these for a number of years . <eos>'"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "srl_evaluation.best_case(\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<sos> the <unk> to statutory <unk> <unk> <unk> <unk> <unk> and <unk> for all part - time workers free health <unk> for all night - <unk> workers legal protection for workers taking <unk> <unk> <unk> helped by <unk> the <unk> period for <unk> <unk> from 2 years to <unk> year <unk> <unk> <unk> <unk> to 18 <unk> <unk> <unk> <unk> from <unk> <unk> to 40 <unk> the above is just a small section of the new <unk> , and <unk> of our <unk> may well of <unk> these for a number of years . <eos>',\n",
       " '<sos> on <unk> morning we <unk> the other <unk> which <unk> in <unk> work from two <unk> and had <unk> of their <unk> over a long period of time , so that it <unk> <unk> <unk> source of <unk> and <unk> on <unk> aspects and <unk> about <unk> development and <unk> <unk> . <eos>',\n",
       " '<sos> he <unk> at various <unk> <unk> <unk> ) during his law <unk> at university , <unk> in a series of \" <unk> <unk> \" which were <unk> <unk> of <unk> in public with other <unk> as the <unk> and which were his idea and <unk> by him to make his <unk> more <unk> ( he <unk> said that this had helped him <unk> his <unk> <unk> ) ; 2 ) <unk> the attempts of his <unk> <unk> to <unk> <unk> <unk> and <unk> on his <unk> ; <unk> ) his <unk> <unk> <unk> and <unk> ; <unk> ) his <unk> not to <unk> <unk> at a <unk> level , even though it was <unk> he found most <unk> , because of the <unk> <unk> of the <unk> of <unk> <unk> . <eos>']"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "srl_evaluation.best_cases(\"accuracy\", 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"<sos> she <unk> to <unk> as one of <unk> <unk> 's <unk> <unk> <unk> first <unk> in <unk> more 's history of richard <unk> ( <unk> ) . <eos>\""
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "srl_evaluation.worst_case(\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"<sos> she <unk> to <unk> as one of <unk> <unk> 's <unk> <unk> <unk> first <unk> in <unk> more 's history of richard <unk> ( <unk> ) . <eos>\",\n",
       " '<sos> our <unk> <unk> that there are two groups of <unk> <unk> <unk> . <eos> <pad> <pad> <pad>',\n",
       " '<sos> all they <unk> <unk> i <unk> is for those <unk> each to <unk> up a <unk> <unk> <unk> th oct people are e - <unk> <unk> when <unk> <unk> will be open <unk> at <unk> . <eos>']"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "srl_evaluation.worst_cases(\"accuracy\", 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "srl_evaluation.save(\"accuracy\")\n",
    "srl_evaluation.save(\"f1_macro\")\n",
    "srl_evaluation.save(\"f1_micro\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
