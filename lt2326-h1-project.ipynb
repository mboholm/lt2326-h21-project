{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchtext\n",
    "import torch.nn.functional as F\n",
    "#from torchtext.data import Field, BucketIterator, Iterator, TabularDataset\n",
    "from torchtext.legacy.data import Field, BucketIterator, Iterator, TabularDataset # Needed for running this on my laptop\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Meta variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#device = torch.device('cuda:0')\n",
    "device = torch.device('cpu')\n",
    "\n",
    "my_data_directory = \"../../data/\" # where to store files\n",
    "my_models_directory = \"../../models/\"\n",
    "\n",
    "mini_testing = True\n",
    "my_train_file = \"mini_train.csv\" if mini_testing == True else \"train.csv\"\n",
    "my_test_file  = \"mini_test.csv\" if mini_testing == True else \"test.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 3 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataloader(directory  = my_data_directory,\n",
    "               train_file = my_train_file,\n",
    "               test_file  = my_test_file,\n",
    "               batch      = batch_size):\n",
    "    \n",
    "    num_whitespacer = lambda x: [int(e) for e in x.split(\" \")]\n",
    "    \n",
    "    SENTENCE = Field(lower = True, \n",
    "                     batch_first = True, \n",
    "                     init_token = \"<sos>\", \n",
    "                     eos_token = \"<eos>\")\n",
    "    \n",
    "    PREDICATE = Field(tokenize = num_whitespacer, # Here might be some problems ...\n",
    "                      batch_first = True, \n",
    "                      pad_token = 0,\n",
    "                      use_vocab = False,\n",
    "                      init_token = 0, \n",
    "                      eos_token = 0) \n",
    "    \n",
    "    SRLABEL = Field(batch_first = True, \n",
    "                    init_token = \"<sos>\", \n",
    "                    eos_token = \"<eos>\")\n",
    "    \n",
    "    my_fields = [(\"sentence\", SENTENCE),\n",
    "                 (\"predicate\", PREDICATE),\n",
    "                 (\"srlabel\", SRLABEL)]\n",
    "    \n",
    "    train, test = TabularDataset.splits(path   = directory,\n",
    "                                        train  = train_file,\n",
    "                                        test   = test_file,\n",
    "                                        format = 'csv',\n",
    "                                        fields = my_fields,\n",
    "                                        csv_reader_params = {'delimiter':'\\t',\n",
    "                                                             'quotechar':'Â¤'}) # Seems not to be in data\n",
    "    SENTENCE.build_vocab(train)\n",
    "    SRLABEL.build_vocab(train)  \n",
    "\n",
    "    train_iter, test_iter = BucketIterator.splits((train, test),\n",
    "                                                  batch_size        = batch,\n",
    "                                                  sort_within_batch = True,\n",
    "                                                  sort_key          = lambda x: len(x.sentence),\n",
    "                                                  shuffle           = True,\n",
    "                                                  device            = device)\n",
    "\n",
    "    return train_iter, test_iter, SENTENCE.vocab, SRLABEL.vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SRL_Encoder(nn.Module):\n",
    "    def __init__(self, voc_size, embedding_size, hidden_size, n_layers, p_dropout):  \n",
    "        super(SRL_Encoder, self).__init__()\n",
    "        \n",
    "        self.embeddings = nn.Embedding(voc_size, embedding_size)\n",
    "        self.sp_pair = embedding_size + 1 # emedded sentence + predicate vector\n",
    "        self.hidden_size = hidden_size\n",
    "        self.n_layers = n_layers\n",
    "        \n",
    "        self.rnn = nn.LSTM(self.sp_pair, \n",
    "                           self.hidden_size, \n",
    "                           num_layers = self.n_layers,\n",
    "                           dropout = p_dropout,\n",
    "                           #bidirectional=True, # !\n",
    "                           batch_first=True) # !\n",
    "        self.dropout = nn.Dropout(p_dropout)\n",
    "        \n",
    "    def forward(self, sentences, pred_vec):\n",
    "        \n",
    "        embeddings = self.embeddings(sentences)\n",
    "        pred_vec = pred_vec.unsqueeze(2)        \n",
    "        sentence_pred_pair = torch.cat((embeddings, pred_vec), dim=2)\n",
    "        contextualized_embedding, (hidden_final, cell_final) = self.rnn(sentence_pred_pair)\n",
    "        \n",
    "        #print(\"ENC, hidden:\", hidden_final.shape)\n",
    "        #print(\"ENC, cell:\", cell_final.shape)\n",
    "        \n",
    "        return hidden_final, cell_final\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SRL_Decoder(nn.Module):\n",
    "    def __init__(self, n_labels, embedding_size, hidden_size, n_layers, p_dropout):  \n",
    "        super(SRL_Decoder, self).__init__()\n",
    " \n",
    "        self.hidden_size = hidden_size\n",
    "        self.n_layers = n_layers\n",
    "        self.n_labels = n_labels\n",
    "        \n",
    "        self.embeddings = nn.Embedding(n_labels, embedding_size) # ?\n",
    "        self.rnn = nn.LSTM(embedding_size, \n",
    "                           self.hidden_size, \n",
    "                           num_layers = self.n_layers, \n",
    "                           batch_first=True,\n",
    "                           #bidirectional=True,\n",
    "                           dropout = p_dropout)\n",
    "        self.classifier = nn.Linear(hidden_size, self.n_labels)\n",
    "        \n",
    "    def forward(self, previous, hidden, cell):\n",
    "        \n",
    "        #previous = previous.unsqueeze(1)\n",
    "        \n",
    "        embedded = self.embeddings(previous)\n",
    "        #print(\"DEC, emb_previous:\", embedded.shape)\n",
    "        output, (hidden, cell) = self.rnn(embedded, (hidden, cell))\n",
    "        prediction = self.classifier(output)\n",
    "        \n",
    "        return prediction, hidden, cell\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SRL_Seq2SeqLabeler(nn.Module):\n",
    "    def __init__(self, encoder, decoder):  \n",
    "        super(SRL_Seq2SeqLabeler, self).__init__()\n",
    "        \n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        \n",
    "        assert encoder.hidden_size == decoder.hidden_size, \"hidden dimension of encoder must be equal to that of decoder\"\n",
    "        assert encoder.n_layers == decoder.n_layers, \"n_layers of encoder must be equal to that of decoder\"\n",
    "        \n",
    "    def forward(self, sentence, predicate, srl_labels, tfr = None): # teacher forcing ratio (tfr)\n",
    "\n",
    "        batch_size = sentence.shape[0]\n",
    "        seq_len = sentence.shape[1]\n",
    "        n_labels = self.decoder.n_labels\n",
    "\n",
    "        outputs = torch.zeros(batch_size, seq_len, n_labels).to(device) # for storage\n",
    "\n",
    "        hidden, cell = self.encoder(sentence, predicate)\n",
    "\n",
    "        seq_element = srl_labels[:, 0].unsqueeze(1) # start of sentence token; index of <sos>\n",
    "        #print(\"S2S, Seq_elem, prior:\", seq_element)\n",
    "\n",
    "        for l in range(1, seq_len): #start from 1; first column of outputs will \"remain\" 0\n",
    "            \n",
    "            #print(\"S2S, Seq_elem:\", seq_element.shape)\n",
    "\n",
    "            output, hidden, cell = self.decoder(seq_element, hidden, cell)\n",
    "            #print(\"S2S, Outputs:\", outputs.shape)\n",
    "            #print(\"S2S, Output:\", output.shape)\n",
    "            outputs[:, l, :] = output.squeeze()\n",
    "            best_guess = output.argmax(2)\n",
    "            #print(\"S2S, Best guess:\", best_guess)\n",
    "\n",
    "            if tfr != None:\n",
    "                teacher_force = random.random() < tfr\n",
    "#                 if teacher_force:\n",
    "#                     print(\"TF\")\n",
    "#                 else:\n",
    "#                     print(\"No TF\")\n",
    "                seq_element = srl_labels[:, l].unsqueeze(1) if teacher_force else best_guess\n",
    "            else:\n",
    "                seq_element = best_guess\n",
    "\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.random()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainer(model, # Must be an instance of a model!\n",
    "            name_of_model,\n",
    "            learning_rate,\n",
    "            epochs,\n",
    "            data,\n",
    "            my_tfr = 0.5,\n",
    "            val_data = None,\n",
    "            save_model = False,\n",
    "            directory = my_models_directory,\n",
    "            my_loss_function = nn.CrossEntropyLoss,\n",
    "            my_optimizer = optim.Adam\n",
    "           ):\n",
    "    \"\"\" Specifices a general training procedure for a model. \n",
    "        Note: trainer() requires an instantiated model as model argument. \n",
    "    \"\"\"\n",
    "    \n",
    "    optimizer = my_optimizer(model.parameters(), lr=learning_rate)    \n",
    "    \n",
    "    model.to(device)\n",
    "    model.train()\n",
    "    \n",
    "    pad_idx = labels.stoi[\"<pad>\"]\n",
    "    \n",
    "    loss_function = my_loss_function(ignore_index=pad_idx)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        epoch_loss = 0\n",
    "        \n",
    "        for i, batch in enumerate(data):\n",
    "            optimizer.zero_grad # reset gradients\n",
    "            \n",
    "            sentence = batch.sentence\n",
    "            predicate = batch.predicate\n",
    "            targets = batch.srlabel\n",
    "            #print(\"TRAIN, Targets:\", targets.shape)\n",
    "            \n",
    "            output = model(sentence, predicate, targets, tfr = my_tfr)\n",
    "            #print(\"TRAIN, Output:\", output.shape)\n",
    "            \n",
    "            ### STUFF TO BE DONE WITH OUTPUT AND TARGETS HERE ###\n",
    "            \n",
    "            bsz = output.shape[0]\n",
    "            length = output.shape[1]\n",
    "            output_dim = output.shape[2]\n",
    "        \n",
    "            output = output[:, 1:, :].reshape(bsz*(length - 1), output_dim) # first token (\"column\") being zeroes\n",
    "            targets = targets[:, 1:].flatten() # first token being <sos>\n",
    "\n",
    "            loss = loss_function(output, targets)\n",
    "            \n",
    "            epoch_loss += loss.item()\n",
    "            loss.backward() # compute gradients\n",
    "            optimizer.step() # update parameters\n",
    "            #break\n",
    "            \n",
    "        print(f\"Epoch: {epoch+1} (out of {epochs}); total loss: {epoch_loss}.\")\n",
    "            \n",
    "        if val_data != None:\n",
    "            model.eval()\n",
    "            # Here we could do some evaluation of model progress, but I have ignored this for now. \n",
    "            model.train()\n",
    "            \n",
    "    if save_model == True:\n",
    "        torch.save(model, directory+name_of_model+\".pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "# https://www.baeldung.com/cs/multi-class-f1-score\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html\n",
    "\n",
    "\n",
    "def metrics(prediction, truth, labels):\n",
    "    \"\"\" Calculates accuracy and F1.  \n",
    "    \"\"\"\n",
    "    \n",
    "    score_keeping = {}\n",
    "    \n",
    "    accuracy = accuracy_score(truth, prediction)\n",
    "    f1_lsted = f1_score(truth, prediction, labels = labels, average = None)\n",
    "    f1_macro = f1_score(truth, prediction, labels = labels, average = \"macro\") # Calculate metrics for each label, and find their unweighted mean. Does not take label imbalance into account.\n",
    "    f1_micro = f1_score(truth, prediction, labels = labels, average = \"micro\") # Calculate metrics globally by counting the total true positives, false negatives and false positives.\n",
    "    \n",
    "    score_keeping[\"accuracy\"] = accuracy\n",
    "    score_keeping[\"f1_macro\"] = f1_macro\n",
    "    score_keeping[\"f1_micro\"] = f1_micro\n",
    "    for label in labels:\n",
    "        score_keeping[label] = f1_lsted[labels.index(label)] # overkill?\n",
    "    \n",
    "    return score_keeping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean(array):\n",
    "    \"\"\" Calculates the mean and standard deviation of an aray of numbers.\n",
    "    \"\"\"\n",
    "#     print(array)\n",
    "    mean = np.mean(array)\n",
    "    std  = np.std(array)\n",
    "    return mean, std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Evaluation:\n",
    "    \"\"\" For storing and handling information from the evaluation of model(s).\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.pooled_acc      = \"Not yet defined\"\n",
    "        self.pooled_f1_macro = \"Not yet defined\"\n",
    "        self.pooled_f1_micro = \"Not yet defined\"        \n",
    "        self.mean_acc        = (\"Not yet defined\", \"Not yet defined\")\n",
    "        self.mean_f1_macro   = (\"Not yet defined\", \"Not yet defined\")\n",
    "        self.mean_f1_micro   = (\"Not yet defined\", \"Not yet defined\")\n",
    "        self.corr_l_acc      = \"Not yet defined\"\n",
    "        self.corr_l_f1_macro = \"Not yet defined\"\n",
    "        self.corr_l_f1_micro = \"Not yet defined\"\n",
    "        \n",
    "        self.metrics_dict = {\"accuracy\": [\"Not yet defined\", \"Not yet defined\"], \n",
    "                             \"f1_macro\": [\"Not yet defined\", \"Not yet defined\"], \n",
    "                             \"f1_micro\": [\"Not yet defined\", \"Not yet defined\"]}\n",
    "\n",
    "    def best_case(self, metric):\n",
    "        \"\"\" Returns the file which has the best performance score with respect \n",
    "            to a metric.\n",
    "        \"\"\"\n",
    "        m_list = self.metrics_dict[metric]\n",
    "        zic_zac = False if metric == \"mse\" else True\n",
    "        m_list.sort(key=operator.itemgetter(1), reverse=zic_zac)\n",
    "        return m_list[0][0]\n",
    "    \n",
    "    def best_cases(self, metric, n):\n",
    "        \"\"\" Returns a list of the N files which has the best performance score \n",
    "            with respect to a metric.\n",
    "        \"\"\"\n",
    "        m_list = self.metrics_dict[metric]\n",
    "        zic_zac = False if metric == \"mse\" else True\n",
    "        m_list.sort(key=operator.itemgetter(1), reverse=zic_zac)\n",
    "        files, values = zip(*m_list)\n",
    "        return list(files[:n])\n",
    "    \n",
    "    def worst_case(self, metric):\n",
    "        \"\"\" Returns the file which has the best performance score with respect \n",
    "            to a metric.\n",
    "        \"\"\"\n",
    "        m_list = self.metrics_dict[metric]\n",
    "        zic_zac = True if metric == \"mse\" else False\n",
    "        m_list.sort(key=operator.itemgetter(1), reverse=zic_zac)\n",
    "        return m_list[0][0]\n",
    "\n",
    "    def worst_cases(self, metric, n):\n",
    "        \"\"\" Returns a list of the N files which has the best performance score \n",
    "            with respect to a metric.\n",
    "        \"\"\"\n",
    "        m_list = self.metrics_dict[metric]\n",
    "        zic_zac = True if metric == \"mse\" else False\n",
    "        m_list.sort(key=operator.itemgetter(1), reverse=zic_zac)\n",
    "        files, values = zip(*m_list)\n",
    "        return list(files[:n])\n",
    " \n",
    "    def summary(self):\n",
    "        \"\"\" Summarises an evaluation. Returns string.\"\"\"\n",
    "        summary  = \"\\n\".join([f\"Model {self.name} performs as follows:\", \n",
    "                      f\"Pooled Accuracy: {self.pooled_acc}\",\n",
    "                      f\"Pooled F1_macro: {self.pooled_f1_macro}\",\n",
    "                      f\"Pooled F1_micro: {self.pooled_f1_micro}\",\n",
    "                              \n",
    "                      f\"Mean Accuracy: {self.mean_acc[0]} (std = {self.mean_acc[1]})\",\n",
    "                      f\"Mean F1_macro: {self.mean_f1_macro[0]} (std = {self.mean_f1_macro[1]})\",\n",
    "                      f\"Mean F1_micro: {self.mean_f1_micro[0]} (std = {self.mean_f1_micro[1]})\",\n",
    "                      \n",
    "                      f\"Correlation sentence length and accuracy: {self.corr_l_acc}\",\n",
    "                      f\"Correlation sentence length and F1_macro: {self.corr_l_f1_macro}\",\n",
    "                      f\"Correlation sentence length and F1_micro: {self.corr_l_f1_micro}\"]) \n",
    "        return summary\n",
    "    \n",
    "    def save(self, directory=path_to_save_evaluations):\n",
    "        \"\"\" Writes the summary of an evaluation to a text file (at some diectory).\"\"\"\n",
    "        \n",
    "        summary = self.summary()\n",
    "        with open(directory+self.name+\".txt\", \"w\") as e:\n",
    "            e.write(summary)\n",
    "    \n",
    "    def print_summary(self):\n",
    "        \"\"\" Prints out the summary of an evaluation.\n",
    "        \"\"\"\n",
    "        summary = self.summary()\n",
    "        print(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# which sentence-predicate pairs are best, worst\n",
    "# length? correlation of length and [F1, accuracy]\n",
    "# confusion\n",
    "\n",
    "def evaluator(model, name, test_data = test, labels = lst_labels, detach_me=False):\n",
    "    \"\"\" \n",
    "    \"\"\"\n",
    "    t1 = time.perf_counter()\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    evaluation = Evaluation(name)\n",
    "    \n",
    "    prediction_pooled = []\n",
    "    truth_pooled = []\n",
    "    seq_lengths = []\n",
    "    confusion = {label: {label: 0 for label in labels} for label in labels}\n",
    "    metrics_calc = {\"accuracy\": [], \"f1_macro\": [], \"f1_micro\": []}\n",
    "    \n",
    "    #i=1 # in order to print out progress\n",
    "    for batch in test_data:\n",
    "\n",
    "        sentence = batch.sentence\n",
    "        predicate = batch.predicate\n",
    "            \n",
    "        if detach_me == True:\n",
    "            prediction = model(sentence, predicate, trf = None).detach().to(\"cpu\") \n",
    "            truth = batch.srlabel.detach().to(\"cpu\")\n",
    "        \n",
    "        else:\n",
    "            prediction = model(sentence, predicate, trf = None) \n",
    "            truth = batch.srlabel\n",
    "        \n",
    "        batched_pred_labels = prediction[:, 1:, :].argmax(2) # ... should not be batch-wise, but sentence-wise\n",
    "        batched_true_labels = truth[:, 1:]\n",
    "        \n",
    "        bsz = batched_pred_labels.shape[0]\n",
    "        \n",
    "        for b in range(bsz):\n",
    "            str_sent    = \" \".join([vocab.itos[token] for token in sentence[b]]) # to list?\n",
    "            seq_len     = len([x for x in sentence[b] if vocab.itos[x] not in [\"<pad>\", \"<sos>\", \"<eos>\"]])\n",
    "            pred_labels = batched_pred_labels[b]\n",
    "            true_labels = batched_true_labels[b]\n",
    "            \n",
    "            accuracy, f1_macro, f1_micro, X = metrics(true_labels, pred_labels)\n",
    "            \n",
    "            prediction_pooled.append(pred_labels)\n",
    "            truth_pooled.append(true_labels)\n",
    "            seq_lengths.append(seq_len)\n",
    "            \n",
    "            for p, t in zip(pred_labels, true_labels):\n",
    "                confusion[p][t] += 1\n",
    "                \n",
    "            for m, v in zip([\"accuracy\", \"f1_macro\", \"f1_micro\"], [accuracy, f1_macro, f1_micro]):\n",
    "                metrics_calc[m].append( (str_sent, v) )\n",
    "                \n",
    "#################################                \n",
    "            \n",
    "        pooled_accuracy, pooled_f1_macro, pooled_f1_micro, X = metrics(truth_pooled, prediction_pooled)\n",
    "        \n",
    "        mean_accuracy = mean(list(zip(*metrics_calc[\"accuracy\"]))[1])\n",
    "        mean_f1_macro = mean(list(zip(*metrics_calc[\"f1_macro\"]))[1])\n",
    "        mean_f1_micro = mean(list(zip(*metrics_calc[\"f1_micro\"]))[1])\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        mean(list(zip(*metrics_calc[\"accuracy\"]))[1])\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        bsz = prediction.shape[0]\n",
    "        length = prediction.shape[1]\n",
    "        output_dim = prediction.shape[2]\n",
    "        \n",
    "        \n",
    "\n",
    "        prediction = prediction[:, 1:, :].reshape(bsz*(length - 1), output_dim) # first token (\"column\") being zeroes\n",
    "        turth = targets[:, 1:].flatten() # first token being <sos>        \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "#     thld_frequencies_pooled = {\"tp\": 0, \"fp\": 0, \"tn\": 0, \"fn\": 0}\n",
    "#     thld_metrics_calc = {\"mse\": [], \"accuracy\": [], \"recall\": [], \"precision\": [], \"f1\": []}\n",
    "\n",
    "    i=1 # in order to print out progress\n",
    "    for instance in test_data:\n",
    "        \n",
    "        if detach_me == True:\n",
    "            prediction = torch.flatten(model(instance[\"img_vector\"])).detach().to(\"cpu\") # due to design of model\n",
    "            truth = torch.flatten(instance[\"label\"]).int().detach().to(\"cpu\")\n",
    "            file = instance[\"file\"]\n",
    "        \n",
    "        else:\n",
    "            prediction = torch.flatten(model(instance[\"img_vector\"])) # due to design of model\n",
    "            truth = torch.flatten(instance[\"label\"]).int()\n",
    "            file = instance[\"file\"]\n",
    "            \n",
    "        prediction_pooled.append( (file, prediction) )\n",
    "        truth_pooled.append(truth)        \n",
    "        \n",
    "        mse = F.mse_loss(prediction, truth)\n",
    "        thld_metrics_calc[\"mse\"].append( (file, mse.item()) )\n",
    "        \n",
    "        #######\n",
    "        #roundof = (prediction >= 0.1).int()\n",
    "        #print(\"sum of roundof, file:\", file, torch.sum(roundof))\n",
    "        #break\n",
    "        #######\n",
    "        \n",
    "        # IF-block for calculating accuracy, recall, precision, f1, which is very\n",
    "        # time consuming, due to identifications of TPs, FPs, TNs, and FNs for \n",
    "        # large matrices.\n",
    "        if threshold != None:\n",
    "            roundof = (prediction >= threshold).int()\n",
    "            #print(\"sum of roundof\", torch.sum(roundof))\n",
    "            tp = sum(roundof * truth)\n",
    "            fp = sum(roundof * (~truth.bool()))\n",
    "            tn = sum((~roundof.bool()) * (~truth.bool()))\n",
    "            fn = sum((~roundof.bool()) * truth)\n",
    "\n",
    "            accuracy, recall, precision, f1 = thld_metrics(tp, fp, tn, fn)\n",
    "\n",
    "            for key, value in zip([\"tp\", \"fp\", \"tn\", \"fn\"], [tp, fp, tn, fn]):\n",
    "                thld_frequencies_pooled[key]+=value\n",
    "\n",
    "            for key, value in zip([\"accuracy\", \"recall\", \"precision\", \"f1\"], \n",
    "                                  [accuracy.item(), recall.item(), precision.item(), f1.item()]):\n",
    "                thld_metrics_calc[key].append( (file, value) )\n",
    "        \n",
    "        print(\"({}%)\".format(round((i/len(test_data)*100), 1)), end=\"\\r\")\n",
    "        i+=1\n",
    "    \n",
    "    file, predictions = zip(*prediction_pooled)\n",
    "    evaluation.pooled_mse = F.mse_loss(torch.flatten(torch.stack(list(predictions))), \n",
    "                                       torch.flatten(torch.stack(truth_pooled))).item()\n",
    "    \n",
    "    if threshold != None:\n",
    "        pooled_accuracy, pooled_recall, pooled_precision, pooled_f1 = thld_metrics(\n",
    "            thld_frequencies_pooled[\"tp\"], \n",
    "            thld_frequencies_pooled[\"fp\"], \n",
    "            thld_frequencies_pooled[\"tn\"], \n",
    "            thld_frequencies_pooled[\"fn\"])\n",
    "        \n",
    "        evaluation.pooled_acc = pooled_accuracy\n",
    "        evaluation.pooled_rec = pooled_recall\n",
    "        evaluation.pooled_prc = pooled_precision\n",
    "        evaluation.pooled_f1  = pooled_f1\n",
    "    \n",
    "    # The code below is a bit nested. What it does in plain English is:\n",
    "    # go to the dictionary where we keep all the performance scores with respect to \n",
    "    # each file. Every key (i.e. metric) of that dict maps to a list of tupples of \n",
    "    # file and value of the metric. Here, we \"unzip\" that list of tupples and calculate the \n",
    "    # mean (and standard deviation) for the values and use that mean (and std) to define \n",
    "    # the respective attributes of the Evaluation class instance.\n",
    "    \n",
    "    evaluation.mean_mse = mean(list(zip(*thld_metrics_calc[\"mse\"]))[1])\n",
    "    \n",
    "    if threshold != None:\n",
    "        evaluation.mean_acc = mean(list(zip(*thld_metrics_calc[\"accuracy\"]))[1]) \n",
    "        evaluation.mean_rec = mean(list(zip(*thld_metrics_calc[\"recall\"]))[1]) \n",
    "        evaluation.mean_prc = mean(list(zip(*thld_metrics_calc[\"precision\"]))[1]) \n",
    "        evaluation.mean_f1  = mean(list(zip(*thld_metrics_calc[\"f1\"]))[1]) \n",
    "    \n",
    "    evaluation.metrics_dict = thld_metrics_calc\n",
    "    \n",
    "    t2 = time.perf_counter()\n",
    "    passed_time = t2 - t1\n",
    "    print(\"Done! ({} m., {} s.)\".format(int(passed_time/60), int(passed_time%60)))\n",
    "    \n",
    "    return evaluation, dict(prediction_pooled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Runnig it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test, vocab, labels = dataloader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "lst_labels = [labels.itos[x] for x in range(len(labels))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<unk>',\n",
       " '<pad>',\n",
       " '<sos>',\n",
       " '<eos>',\n",
       " '0',\n",
       " 'I-ARG1',\n",
       " 'I-ARG2',\n",
       " 'B-PRD',\n",
       " 'B-ARG1',\n",
       " 'I-ARGM-MNR',\n",
       " 'I-ARG0',\n",
       " 'B-ARG0',\n",
       " 'I-ARGM-ADV',\n",
       " 'B-ARG2',\n",
       " 'I-ARGM-PRP',\n",
       " 'I-ARGM-TMP',\n",
       " 'B-ARGM-MNR',\n",
       " 'B-ARGM-TMP',\n",
       " 'I-ARGM-LOC',\n",
       " 'B-ARGM-MOD',\n",
       " 'I-ARGM-DIR',\n",
       " 'B-ARGM-ADV',\n",
       " 'B-ARGM-LOC',\n",
       " 'B-ARGM-NEG',\n",
       " 'B-ARGM-PRP',\n",
       " 'B-R-ARG1',\n",
       " 'I-ARGM-PNC',\n",
       " 'B-ARG4',\n",
       " 'B-ARGM-CAU',\n",
       " 'B-ARGM-DIR',\n",
       " 'B-ARGM-DIS',\n",
       " 'B-ARGM-PNC',\n",
       " 'B-R-ARG0',\n",
       " 'I-ARG4']"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lst_labels #remove stuff?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab.stoi[\"<pad>\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels.stoi[\"<pad>\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_epochs = 3\n",
    "\n",
    "vocab_size = len(vocab)\n",
    "num_labels = len(labels)\n",
    "emb_sizeE = 50\n",
    "emb_sizeD = int(num_labels/2) # embeddings for labels\n",
    "hid_size_encoder = 100\n",
    "hid_size_decoder = hid_size_encoder # ?\n",
    "\n",
    "num_layers = 2\n",
    "\n",
    "my_encoder = SRL_Encoder(vocab_size, emb_sizeE, hid_size_encoder, num_layers, p_dropout=0.2)\n",
    "my_decoder = SRL_Decoder(num_labels, emb_sizeD, hid_size_decoder, num_layers, p_dropout=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_encoder.parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_decoder.parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_SRLLabeler = SRL_Seq2SeqLabeler(my_encoder, my_decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 (out of 3); total loss: 65.64285123348236.\n",
      "Epoch: 2 (out of 3); total loss: 96.61577893793583.\n",
      "Epoch: 3 (out of 3); total loss: 79.67333137989044.\n"
     ]
    }
   ],
   "source": [
    "model_name = f\"srl_b{batch_size}_e{my_epochs}\"\n",
    "trainer(model = my_SRLLabeler, \n",
    "        name_of_model = model_name, \n",
    "        learning_rate = 0.01, \n",
    "        epochs = my_epochs, \n",
    "        data = train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SRLabeler1(nn.Module):\n",
    "    def __init__(self, voc_size, embedding_size, n_labels):  \n",
    "        super(SRLabeler1, self).__init__()\n",
    "        \n",
    "        self.embeddings = nn.Embedding(voc_size, embedding_size)\n",
    "        self.sp_pair = embedding_size + 1 # emedded sentence + predicate vector\n",
    "        self.rnn = nn.LSTM(self.sp_pair, n_labels, bidirectional=True, batch_first=True)\n",
    "        \n",
    "    def forward(self, sentences, pred_vec, softmax=False):\n",
    "        \n",
    "        embeddings = self.embeddings(sentences)\n",
    "        pred_vec = pred_vec.unsqueeze(2)        \n",
    "        sentence_pred_pair = torch.cat((embeddings, pred_vec), dim=2)\n",
    "        contextualized_embedding, *_ = self.rnn(sentence_pred_pair)\n",
    "        \n",
    "        if softmax == True:\n",
    "            return F.softmax(contextualized_embedding, dim=2)\n",
    "        else:\n",
    "            return contextualized_embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainer1(model, # Must be an instance of a model!\n",
    "            name_of_model,\n",
    "            learning_rate,\n",
    "            epochs,\n",
    "            data,\n",
    "            val_data = None,\n",
    "            save_model = False,\n",
    "            directory = my_models_directory,\n",
    "            my_loss_function = nn.CrossEntropyLoss,\n",
    "            my_optimizer = optim.Adam\n",
    "           ):\n",
    "    \"\"\" Specifices a general training procedure for a model. \n",
    "        Note: trainer() requires an instantiated model as model argument. \n",
    "    \"\"\"\n",
    "    \n",
    "    optimizer = my_optimizer(model.parameters(), lr=learning_rate)    \n",
    "    \n",
    "    model.to(device)\n",
    "    model.train()\n",
    "    \n",
    "    loss_function = my_loss_function()\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        epoch_loss = 0\n",
    "        \n",
    "        for i, batch in enumerate(data):\n",
    "            optimizer.zero_grad # reset gradients\n",
    "            \n",
    "            sentence = batch.sentence\n",
    "            predicate = batch.predicate\n",
    "            targets = batch.srlabel\n",
    "            \n",
    "            b=sentence.shape[0] # !\n",
    "            sequence_length = sentence.shape[1] # !\n",
    "            l = targets.shape[1] # !\n",
    "                        \n",
    "            output = model(sentence, predicate)\n",
    "            d = output.shape[2] # !\n",
    "            \n",
    "            #print(\"Output:\", output.shape)\n",
    "            #print(\"Target:\", targets.shape)\n",
    "            \n",
    "            loss = loss_function(output.reshape(b*sequence_length, d), # !\n",
    "                                 targets.reshape(b*sequence_length))\n",
    "            \n",
    "            epoch_loss += loss.item()\n",
    "            loss.backward() # compute gradients\n",
    "            optimizer.step() # update parameters\n",
    "            #break\n",
    "            \n",
    "        print(f\"Epoch: {epoch+1} (out of {epochs}); total loss: {epoch_loss}.\")\n",
    "            \n",
    "        if val_data != None:\n",
    "            model.eval()\n",
    "            # Here we could do some evaluation of model progress, but I have ignored this for now. \n",
    "            model.train()\n",
    "            \n",
    "    if save_model == True:\n",
    "        torch.save(model, directory+name_of_model+\".pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "vocab_size = len(vocab)\n",
    "number_of_labels = len(labels)\n",
    "print(\"Size of vocabulary:\", vocab_size)\n",
    "print(\"Number of labels:\", number_of_labels)\n",
    "epochs = 100\n",
    "my_learning_rate = 0.01\n",
    "my_emedding_size = 50\n",
    "simple_srl_model = SRLabeler1(vocab_size, my_emedding_size, number_of_labels)\n",
    "trainer(simple_srl_model, \"simple_srl\", my_learning_rate, epochs, train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Know your enemies; keep until ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for x in train:\n",
    "    output = my_model(x.sentence, x.predicate)\n",
    "    print(\"op\", output.shape)\n",
    "    soft = F.softmax(output, dim=2)\n",
    "    print(torch.argmax(soft, dim=2))\n",
    "    #print(\"sm\", soft.shape)\n",
    "    #print(torch.sum(soft, dim=2).shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
