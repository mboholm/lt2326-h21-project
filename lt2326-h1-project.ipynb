{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This Jupyter Notebook contains the code for training and evaluating a Sequence to Sequence (seq2seq) Encoder - Decoder model for semantic role labeling (SRL), as project for the course LT2326, autumn 2021. Data preparation is defined and handled elsewhere; see `data_builder.ipynb`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## On torchtext module"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There seemed to be no installation of `torchtext`, so I ran:\n",
    "\n",
    "```pip install torchtext==0.10.0```\n",
    "\n",
    "which shold be compatible with `torch` version 1.9.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"PyTorch Version: {torch.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random, time, operator\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchtext\n",
    "import torch.nn.functional as F\n",
    "#from torchtext.data import Field, BucketIterator, Iterator, TabularDataset\n",
    "from torchtext.legacy.data import Field, BucketIterator, Iterator, TabularDataset # Needed for running this on my laptop\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Meta variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define where to get and store data and which device to use. For test pipline with less data during development set `mini_training`to `True`; when using complete dataset, set to `False`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\")\n",
    "#device = torch.device('cpu')\n",
    "\n",
    "my_data_directory = \"../data/\" # my settings on MLTGPU\n",
    "\n",
    "my_models_directory = \"../models/\" # my settings on MLTGPU\n",
    "\n",
    "mini_testing = False\n",
    "my_train_file = \"mini_train.csv\" if mini_testing == True else \"train.csv\"\n",
    "my_test_file  = \"mini_test.csv\" if mini_testing == True else \"test.csv\"\n",
    "\n",
    "dir_for_evaluations = \"../evals/\" # my settings on MLTGPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define batchsize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataloader(directory  = my_data_directory,\n",
    "               train_file = my_train_file,\n",
    "               test_file  = my_test_file,\n",
    "               batch      = batch_size):\n",
    "    \n",
    "    num_whitespacer = lambda x: [int(e) for e in x.split(\" \")]\n",
    "    \n",
    "    SENTENCE = Field(lower = True,\n",
    "                     batch_first = True, \n",
    "                     init_token = \"<sos>\", \n",
    "                     eos_token = \"<eos>\")\n",
    "    \n",
    "    PREDICATE = Field(tokenize = num_whitespacer, # Here might be some problems ...\n",
    "                      batch_first = True, \n",
    "                      pad_token = 0,\n",
    "                      use_vocab = False,\n",
    "                      init_token = 0, \n",
    "                      eos_token = 0) \n",
    "    \n",
    "    SRLABEL = Field(batch_first = True, \n",
    "                    init_token = \"<sos>\", \n",
    "                    eos_token = \"<eos>\")\n",
    "    \n",
    "    my_fields = [(\"sentence\", SENTENCE),\n",
    "                 (\"predicate\", PREDICATE),\n",
    "                 (\"srlabel\", SRLABEL)]\n",
    "    \n",
    "    train, test = TabularDataset.splits(path   = directory,\n",
    "                                        train  = train_file,\n",
    "                                        test   = test_file,\n",
    "                                        format = 'csv',\n",
    "                                        fields = my_fields,\n",
    "                                        csv_reader_params = {'delimiter':'\\t',\n",
    "                                                             'quotechar':'Â¤'}) # Seems not to be in data\n",
    "    SENTENCE.build_vocab(train)\n",
    "    SRLABEL.build_vocab(train)  \n",
    "\n",
    "    train_iter, test_iter = BucketIterator.splits((train, test),\n",
    "                                                  batch_size        = batch,\n",
    "                                                  sort_within_batch = True,\n",
    "                                                  sort_key          = lambda x: len(x.sentence),\n",
    "                                                  shuffle           = True,\n",
    "                                                  device            = device)\n",
    "\n",
    "    return train_iter, test_iter, SENTENCE.vocab, SRLABEL.vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calling dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test, vocab, labels = dataloader()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Definitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The encoder encodes sentence-predicate pairs through LSTMs. In forward pass, it returns *the final cell state* and *the final hidden state* (somtimes referred to as the *context vector*).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SRL_Encoder(nn.Module):\n",
    "    def __init__(self, voc_size, embedding_size, hidden_size, n_layers, p_dropout):  \n",
    "        super(SRL_Encoder, self).__init__()\n",
    "        \n",
    "        self.embeddings = nn.Embedding(voc_size, embedding_size)\n",
    "        self.sp_pair = embedding_size + 1 # emedded sentence + predicate vector\n",
    "        self.hidden_size = hidden_size\n",
    "        self.n_layers = n_layers\n",
    "        \n",
    "        self.rnn = nn.LSTM(self.sp_pair, \n",
    "                           self.hidden_size, \n",
    "                           num_layers = self.n_layers,\n",
    "                           dropout = p_dropout,\n",
    "                           #bidirectional=True, # !\n",
    "                           batch_first=True) # !\n",
    "        self.dropout = nn.Dropout(p_dropout)\n",
    "        \n",
    "    def forward(self, sentences, pred_vec):\n",
    "        \n",
    "        embeddings = self.embeddings(sentences)\n",
    "        pred_vec = pred_vec.unsqueeze(2)        \n",
    "        sentence_pred_pair = torch.cat((embeddings, pred_vec), dim=2)\n",
    "        #print(sentence_pred_pair)\n",
    "        contextualized_embedding, (hidden_final, cell_final) = self.rnn(sentence_pred_pair)\n",
    "        \n",
    "        #print(\"ENC, hidden:\", hidden_final.shape)\n",
    "        #print(\"ENC, cell:\", cell_final.shape)\n",
    "        \n",
    "        return hidden_final, cell_final\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The decoder predicts the next element of a sequence based on the previous sequence and the final cell state and the final hidden state of that sequence through an LSTM. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SRL_Decoder(nn.Module):\n",
    "    def __init__(self, n_labels, embedding_size, hidden_size, n_layers, p_dropout):  \n",
    "        super(SRL_Decoder, self).__init__()\n",
    " \n",
    "        self.hidden_size = hidden_size\n",
    "        self.n_layers = n_layers\n",
    "        self.n_labels = n_labels\n",
    "        \n",
    "        self.embeddings = nn.Embedding(n_labels, embedding_size) # ?\n",
    "        self.rnn = nn.LSTM(embedding_size, \n",
    "                           self.hidden_size, \n",
    "                           num_layers = self.n_layers, \n",
    "                           batch_first=True,\n",
    "                           #bidirectional=True,\n",
    "                           dropout = p_dropout)\n",
    "        self.classifier = nn.Linear(hidden_size, self.n_labels)\n",
    "        \n",
    "    def forward(self, previous, hidden, cell):\n",
    "        \n",
    "        #previous = previous.unsqueeze(1)\n",
    "        \n",
    "        embedded = self.embeddings(previous)\n",
    "        #print(\"DEC, emb_previous:\", embedded.shape)\n",
    "        output, (hidden, cell) = self.rnn(embedded, (hidden, cell))\n",
    "        prediction = self.classifier(output)\n",
    "        \n",
    "        return prediction, hidden, cell\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder - Decoder Interface"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the `SRL_Seq2SeqLabeler`, the context vector (i.e. final cell and hidden states) of the `Encoder` together with the start token `<sos>` serves as inputs to predict a sequence of semantic role labels. After the first prediction, the decoder uses its own predictions as the input sequence to predict the next token. This model uses teacher forcing, meaning that, at some proportion of the time, as defined by a teacher force ratio (TFR), the true label of the sequence is put into the sequence, instead of the prediction by the encoder. \n",
    "\n",
    "Minor note: the classification problem engaged with here is a one-to-one mapping. Translation problems more generally might involve mappings of sequences of different lengths. To handle mappings of different lengths properly would require further work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SRL_Seq2SeqLabeler(nn.Module):\n",
    "    def __init__(self, encoder, decoder):  \n",
    "        super(SRL_Seq2SeqLabeler, self).__init__()\n",
    "        \n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        \n",
    "        assert encoder.hidden_size == decoder.hidden_size, \"hidden dimension of encoder must be equal to that of decoder\"\n",
    "        assert encoder.n_layers == decoder.n_layers, \"n_layers of encoder must be equal to that of decoder\"\n",
    "        \n",
    "    def forward(self, sentence, predicate, srl_labels, tfr = None): # tfr = teacher forcing ratio\n",
    "\n",
    "        batch_size = sentence.shape[0]\n",
    "        seq_len = sentence.shape[1]\n",
    "        n_labels = self.decoder.n_labels\n",
    "\n",
    "        outputs = torch.zeros(batch_size, seq_len, n_labels).to(device) # for storage\n",
    "\n",
    "        hidden, cell = self.encoder(sentence, predicate)\n",
    "\n",
    "        seq_element = srl_labels[:, 0].unsqueeze(1) # start of sentence token; index of <sos>\n",
    "        #print(\"S2S, Seq_elem, prior:\", seq_element)\n",
    "\n",
    "        for l in range(1, seq_len): # Note: starts from 1; first column of outputs will \"remain\" 0\n",
    "            \n",
    "            #print(\"S2S, Seq_elem:\", seq_element.shape)\n",
    "\n",
    "            output, hidden, cell = self.decoder(seq_element, hidden, cell)\n",
    "            #print(\"S2S, Outputs:\", outputs.shape)\n",
    "            #print(\"S2S, Output:\", output.shape)\n",
    "            outputs[:, l, :] = output.squeeze()\n",
    "            best_guess = output.argmax(2)\n",
    "            #print(\"S2S, Best guess:\", best_guess)\n",
    "\n",
    "            if tfr != None:\n",
    "                teacher_force = random.random() < tfr\n",
    "#                 if teacher_force:\n",
    "#                     print(\"TF\")\n",
    "#                 else:\n",
    "#                     print(\"No TF\")\n",
    "                seq_element = srl_labels[:, l].unsqueeze(1) if teacher_force else best_guess\n",
    "            else:\n",
    "                seq_element = best_guess\n",
    "\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General training function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainer(model, # Must be an instance of a model!\n",
    "            name_of_model,\n",
    "            learning_rate,\n",
    "            epochs,\n",
    "            data,\n",
    "            my_tfr,\n",
    "            clip_grad = None,\n",
    "            val_data = None,\n",
    "            save_model = False,\n",
    "            directory = my_models_directory,\n",
    "            my_loss_function = nn.CrossEntropyLoss,\n",
    "            my_optimizer = optim.Adam\n",
    "           ):\n",
    "    \"\"\" Specifices a general training procedure for a model. \n",
    "        Note: trainer() requires an instantiated model as model argument.\n",
    "    \"\"\"\n",
    "    \n",
    "    optimizer = my_optimizer(model.parameters(), lr=learning_rate)    \n",
    "    \n",
    "    model.to(device)\n",
    "    model.train()\n",
    "    \n",
    "    pad_idx = labels.stoi[\"<pad>\"]\n",
    "    \n",
    "    loss_function = my_loss_function(ignore_index=pad_idx) # We ignore pad token in loss calculation\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        print(f\"Epoch: {epoch+1} (out of {epochs}).\")\n",
    "        epoch_loss = 0\n",
    "        \n",
    "        for i, batch in enumerate(data):\n",
    "            print(\"Batch: \", i, end=\"\\r\")\n",
    "            optimizer.zero_grad # reset gradients\n",
    "            \n",
    "            sentence = batch.sentence\n",
    "            predicate = batch.predicate\n",
    "            targets = batch.srlabel\n",
    "            #print(\"TRAIN, Targets:\", targets.shape)\n",
    "            \n",
    "            output = model(sentence, predicate, targets, tfr = my_tfr)\n",
    "            #print(\"TRAIN, Output:\", output.shape)\n",
    "            \n",
    "            # Before calculation of loss outputs and targets needs to be \"aligned\", so to speak.\n",
    "            # Outputs are of shape [batch, seq_len, dimension]. Targets of shape [batch, seq_len]\n",
    "            # The representation of the first element of the output sequence will be 0s (see \n",
    "            # above). The first element of the targets will be <sos>. We ignore these first elements\n",
    "            # in calculating the loss. \n",
    "            \n",
    "            # Moreover, our loss function (CrossEntropyLoss) expects predicitons as [n_predictions, \n",
    "            # n_classes] and targets as [n_predictions]. Here, n_predictions = batch_size * sequence_\n",
    "            # length. \n",
    "            \n",
    "            bsz = output.shape[0]\n",
    "            length = output.shape[1]\n",
    "            output_dim = output.shape[2]\n",
    "        \n",
    "            output = output[:, 1:, :].reshape(bsz*(length - 1), output_dim) # first token (\"column\") being zeroes\n",
    "            targets = targets[:, 1:].flatten() # first token being <sos>\n",
    "            \n",
    "            # Now, calculate the loss\n",
    "            loss = loss_function(output, targets)\n",
    "            \n",
    "            epoch_loss += loss.item()\n",
    "            \n",
    "            if clip_grad != None:\n",
    "                nn.utils.clip_grad_norm_(model.parameters(), clip_grad) # to handle exploding gradients\n",
    "            \n",
    "            loss.backward() # compute gradients\n",
    "            optimizer.step() # update parameters\n",
    "            #break\n",
    "            \n",
    "        print(f\"Total loss for Epoch {epoch+1}: {epoch_loss}.\")\n",
    "            \n",
    "        if val_data != None:\n",
    "            model.eval()\n",
    "            # Here we could do some evaluation of model progress, but I have ignored this for now. \n",
    "            model.train()\n",
    "            \n",
    "    if save_model == True:\n",
    "        torch.save(model, directory+name_of_model+\".pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some examples from the web:\n",
    "\n",
    "|Author            |No. of layers|Batch Size|Embeddingsdim.|Hidden Dim.|Dropout|WWW             |\n",
    "|------------------|-------------|----------|--------------|-----------|-------|---------|\n",
    "|Ziqi Yuan         |            2|       128|           256|        512|    0.5|https://www.kaggle.com/columbine/seq2seq-pytorch|\n",
    "|Balakrishnakumar V|            2|        32|           300|       1024|    0.5|https://towardsdatascience.com/a-comprehensive-guide-to-neural-machine-translation-using-seq2sequence-modelling-using-pytorch-41c9b84ba350|\n",
    "|Matthew Inkawhich |            2|        64|           ?  |        500|    0.1|https://pytorch.org/tutorials/beginner/deploy_seq2seq_hybrid_frontend_tutorial.html|\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_epochs = 20\n",
    "learning_rate = 0.01\n",
    "dropout = 0.5\n",
    "TFR = 0.5\n",
    "# batch size defined before calling dataloader\n",
    "\n",
    "vocab_size = len(vocab)\n",
    "num_labels = len(labels)\n",
    "emb_sizeE = 512\n",
    "#emb_sizeD = num_labels\n",
    "emb_sizeD = int(num_labels/2) # embeddings for labels\n",
    "hid_size = 1024\n",
    "\n",
    "num_layers = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instantiate models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_encoder = SRL_Encoder(vocab_size, emb_sizeE, hid_size, num_layers, p_dropout=dropout)\n",
    "my_decoder = SRL_Decoder(num_labels, emb_sizeD, hid_size, num_layers, p_dropout=dropout)\n",
    "my_SRLLabeler = SRL_Seq2SeqLabeler(my_encoder, my_decoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Know your models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Module.parameters of SRL_Encoder(\n",
       "  (embeddings): Embedding(34068, 512)\n",
       "  (rnn): LSTM(513, 1024, num_layers=2, batch_first=True, dropout=0.5)\n",
       "  (dropout): Dropout(p=0.5, inplace=False)\n",
       ")>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_encoder.parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Module.parameters of SRL_Decoder(\n",
       "  (embeddings): Embedding(87, 43)\n",
       "  (rnn): LSTM(43, 1024, num_layers=2, batch_first=True, dropout=0.5)\n",
       "  (classifier): Linear(in_features=1024, out_features=87, bias=True)\n",
       ")>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_decoder.parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Name model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "protoname = f\"srl_b{batch_size}_e{my_epochs}_h{hid_size}_tfr{str(TFR)[2:]}_do{dropout}_lr{learning_rate}\"\n",
    "model_name = f\"{protoname}_minisample\" if mini_testing else f\"{protoname}_csample\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 (out of 20).\n",
      "Batch:  46\r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_2513771/3901702893.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m trainer(model = my_SRLLabeler, \n\u001b[0m\u001b[1;32m      2\u001b[0m         \u001b[0mname_of_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m         \u001b[0mlearning_rate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmy_epochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_2513771/2070603990.py\u001b[0m in \u001b[0;36mtrainer\u001b[0;34m(model, name_of_model, learning_rate, epochs, data, my_tfr, clip_grad, val_data, save_model, directory, my_loss_function, my_optimizer)\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0mepoch_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Batch: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"\\r\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m \u001b[0;31m# reset gradients\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/torchtext/legacy/data/iterator.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    158\u001b[0m                     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m                         \u001b[0mminibatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort_key\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreverse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 160\u001b[0;31m                 \u001b[0;32myield\u001b[0m \u001b[0mBatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mminibatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    161\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrepeat\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m                 \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/torchtext/legacy/data/batch.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, dataset, device)\u001b[0m\n\u001b[1;32m     32\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mfield\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m                     \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m                     \u001b[0msetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfield\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/torchtext/legacy/data/field.py\u001b[0m in \u001b[0;36mprocess\u001b[0;34m(self, batch, device)\u001b[0m\n\u001b[1;32m    229\u001b[0m         \"\"\"\n\u001b[1;32m    230\u001b[0m         \u001b[0mpadded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 231\u001b[0;31m         \u001b[0mtensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumericalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpadded\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    232\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/torchtext/legacy/data/field.py\u001b[0m in \u001b[0;36mnumericalize\u001b[0;34m(self, arr, device)\u001b[0m\n\u001b[1;32m    351\u001b[0m                 \u001b[0marr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpostprocessing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    352\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 353\u001b[0;31m         \u001b[0mvar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    354\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    355\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msequential\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_first\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trainer(model = my_SRLLabeler, \n",
    "        name_of_model = model_name, \n",
    "        learning_rate = learning_rate, \n",
    "        epochs = my_epochs, \n",
    "        data = train, \n",
    "        clip_grad = 1,\n",
    "        my_tfr = TFR,\n",
    "        save_model = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For evaluation, several functions are defined. Also, a class for managing information from evalution is defined. Overall, the model is evaluated by\n",
    "*    its accuracy (number of correct predictions / total)\n",
    "*    mean F1 \n",
    "*    wiegthed mean F1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The list of labels\n",
    "lst_labels = [labels.itos[x] for x in range(len(labels))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<unk>',\n",
       " '<pad>',\n",
       " '<sos>',\n",
       " '<eos>',\n",
       " 'O',\n",
       " 'I-ARG1',\n",
       " 'I-ARG2',\n",
       " 'B-PRD',\n",
       " 'B-ARG1',\n",
       " 'I-ARG0',\n",
       " 'I-ARGM-ADV',\n",
       " 'B-ARG0',\n",
       " 'I-ARGM-TMP',\n",
       " 'I-ARGM-LOC',\n",
       " 'B-ARG2',\n",
       " 'I-ARGM-MNR',\n",
       " 'I-ARGM-PRP',\n",
       " 'B-ARGM-TMP',\n",
       " 'B-ARGM-MOD',\n",
       " 'B-ARGM-ADV',\n",
       " 'I-ARGM-CAU',\n",
       " 'B-ARGM-MNR',\n",
       " 'B-ARGM-LOC',\n",
       " 'I-ARG3',\n",
       " 'I-C-ARG1',\n",
       " 'I-ARGM-ADJ',\n",
       " 'B-ARGM-DIS',\n",
       " 'B-ARGM-NEG',\n",
       " 'I-R-ARG0',\n",
       " 'I-ARG4',\n",
       " 'I-ARGM-DIR',\n",
       " 'B-ARGM-PRP',\n",
       " 'B-R-ARG0',\n",
       " 'B-R-ARG1',\n",
       " 'I-ARGM-PRD',\n",
       " 'B-ARGM-ADJ',\n",
       " 'B-ARGM-DIR',\n",
       " 'I-ARGM-DIS',\n",
       " 'B-ARG3',\n",
       " 'B-ARGM-CAU',\n",
       " 'I-ARGM-GOL',\n",
       " 'I-ARGM-PNC',\n",
       " 'B-ARGM-EXT',\n",
       " 'B-ARG4',\n",
       " 'I-R-ARG1',\n",
       " 'B-R-ARGM-LOC',\n",
       " 'I-ARGM-COM',\n",
       " 'B-ARGM-PRD',\n",
       " 'B-ARGM-LVB',\n",
       " 'B-C-ARG1',\n",
       " 'I-ARGM-EXT',\n",
       " 'I-C-ARG2',\n",
       " 'B-ARGM-GOL',\n",
       " 'B-R-ARGM-TMP',\n",
       " 'B-R-ARG2',\n",
       " 'B-ARGM-COM',\n",
       " 'I-R-ARGM-LOC',\n",
       " 'B-ARGM-PNC',\n",
       " 'B-R-ARGM-MNR',\n",
       " 'I-R-ARGM-MNR',\n",
       " 'B-ARGM-REC',\n",
       " 'I-R-ARG2',\n",
       " 'I-C-ARGM-MNR',\n",
       " 'B-C-ARG2',\n",
       " 'I-ARGM-NEG',\n",
       " 'I-C-ARGM-LOC',\n",
       " 'I-R-ARGM-TMP',\n",
       " 'B-R-ARGM-CAU',\n",
       " 'B-R-ARG3',\n",
       " 'I-R-ARG3',\n",
       " 'B-R-ARGM-GOL',\n",
       " 'I-R-ARGM-GOL',\n",
       " 'B-R-ARGM-DIR',\n",
       " 'I-C-ARGM-ADV',\n",
       " 'I-C-ARGM-EXT',\n",
       " 'I-R-ARGM-DIR',\n",
       " 'B-C-ARGM-LOC',\n",
       " 'B-C-ARGM-MNR',\n",
       " 'I-C-ARG0',\n",
       " 'I-R-ARGM-CAU',\n",
       " 'B-C-ARG0',\n",
       " 'B-C-ARGM-ADV',\n",
       " 'B-C-ARGM-EXT',\n",
       " 'B-R-ARGM-ADV',\n",
       " 'B-R-ARGM-COM',\n",
       " 'I-R-ARGM-ADV',\n",
       " 'I-R-ARGM-COM']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lst_labels #remove stuff?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# baseline?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions and a class for handling information "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOURCES:\n",
    "# https://www.baeldung.com/cs/multi-class-f1-score\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html\n",
    "\n",
    "def metrics(prediction, \n",
    "            truth \n",
    "            #labels = lst_labels\n",
    "           ):\n",
    "    \"\"\" Calculates accuracy and F1, given two sequences (lists, arrays) of labels. Since, \n",
    "        these metrices here are used for multi-label classification, two versions \n",
    "        of F1 are calculated: \"macro\" and \"weigthed\", where the former is the mean of F1 for\n",
    "        each label, and the latter is the mean weigthed by support (the number of true \n",
    "        instances for each label), which account for label imbalance.\n",
    "    \"\"\"\n",
    "    \n",
    "    accuracy = accuracy_score(truth, prediction)\n",
    "    f1_macro = f1_score(truth, prediction, average = \"macro\") # Calculate metrics for each label, and find their unweighted mean. Does not take label imbalance into account.\n",
    "    f1_weighted = f1_score(truth, prediction, average = \"weighted\") # Calculate metrics globally by counting the total true positives, false negatives and false positives.\n",
    "    \n",
    "    return accuracy, f1_macro, f1_weighted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean(array):\n",
    "    \"\"\" Calculates the mean and standard deviation of an aray of numbers.\n",
    "    \"\"\"\n",
    "    mean = np.mean(array)\n",
    "    std  = np.std(array)\n",
    "    return mean, std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Evaluation:\n",
    "    \"\"\" For storing and handling information from the evaluation of model(s).\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        \n",
    "        self.pooled_acc         = \"Not yet defined\"\n",
    "        self.pooled_f1_macro    = \"Not yet defined\"\n",
    "        self.pooled_f1_weighted = \"Not yet defined\" \n",
    "        \n",
    "        self.mean_acc           = (\"Not yet defined\", \"Not yet defined\")\n",
    "        self.mean_f1_macro      = (\"Not yet defined\", \"Not yet defined\")\n",
    "        self.mean_f1_weighted   = (\"Not yet defined\", \"Not yet defined\")\n",
    "        \n",
    "        self.corr_l_acc         = \"Not yet defined\"\n",
    "        self.corr_l_f1_macro    = \"Not yet defined\"\n",
    "        self.corr_l_f1_weighted = \"Not yet defined\"\n",
    "        \n",
    "        self.confusion = {\"Not yet defined\": {\"Not yet defined\": \"Not yet defined\"}}\n",
    "        self.metrics_dict = {\"accuracy\": [\"Not yet defined\", \"Not yet defined\"], \n",
    "                             \"f1_macro\": [\"Not yet defined\", \"Not yet defined\"], \n",
    "                             \"f1_weighted\": [\"Not yet defined\", \"Not yet defined\"]}\n",
    "\n",
    "    def best_case(self, metric):\n",
    "        \"\"\" Returns the file which has the best performance score with respect \n",
    "            to a metric.\n",
    "        \"\"\"\n",
    "        m_list = self.metrics_dict[metric]\n",
    "        zic_zac = False if metric == \"mse\" else True\n",
    "        m_list.sort(key=operator.itemgetter(1), reverse=zic_zac)\n",
    "        return m_list[0][0]\n",
    "    \n",
    "    def best_cases(self, metric, n):\n",
    "        \"\"\" Returns a list of the N files which has the best performance score \n",
    "            with respect to a metric.\n",
    "        \"\"\"\n",
    "        m_list = self.metrics_dict[metric]\n",
    "        zic_zac = False if metric == \"mse\" else True\n",
    "        m_list.sort(key=operator.itemgetter(1), reverse=zic_zac)\n",
    "        files, values = zip(*m_list)\n",
    "        return list(files[:n])\n",
    "    \n",
    "    def worst_case(self, metric):\n",
    "        \"\"\" Returns the file which has the best performance score with respect \n",
    "            to a metric.\n",
    "        \"\"\"\n",
    "        m_list = self.metrics_dict[metric]\n",
    "        zic_zac = True if metric == \"mse\" else False\n",
    "        m_list.sort(key=operator.itemgetter(1), reverse=zic_zac)\n",
    "        return m_list[0][0]\n",
    "\n",
    "    def worst_cases(self, metric, n):\n",
    "        \"\"\" Returns a list of the N files which has the best performance score \n",
    "            with respect to a metric.\n",
    "        \"\"\"\n",
    "        m_list = self.metrics_dict[metric]\n",
    "        zic_zac = True if metric == \"mse\" else False\n",
    "        m_list.sort(key=operator.itemgetter(1), reverse=zic_zac)\n",
    "        files, values = zip(*m_list)\n",
    "        return list(files[:n])\n",
    " \n",
    "    def summary(self):\n",
    "        \"\"\" Summarises an evaluation. Returns string.\"\"\"\n",
    "        summary  = \"\\n\".join([f\"Model {self.name} performs as follows:\", \n",
    "                      f\"Pooled Accuracy: {self.pooled_acc}\",\n",
    "                      f\"Pooled F1_macro: {self.pooled_f1_macro}\",\n",
    "                      f\"Pooled F1_weighted: {self.pooled_f1_weighted}\",\n",
    "                              \n",
    "                      f\"Mean Accuracy: {self.mean_acc[0]} (std = {self.mean_acc[1]})\",\n",
    "                      f\"Mean F1_macro: {self.mean_f1_macro[0]} (std = {self.mean_f1_macro[1]})\",\n",
    "                      f\"Mean F1_weighted: {self.mean_f1_weighted[0]} (std = {self.mean_f1_weighted[1]})\",\n",
    "                      \n",
    "                      f\"Correlation sentence length and accuracy: {self.corr_l_acc}\",\n",
    "                      f\"Correlation sentence length and F1_macro: {self.corr_l_f1_macro}\",\n",
    "                      f\"Correlation sentence length and F1_weighted: {self.corr_l_f1_weighted}\"]) \n",
    "        return summary\n",
    "    \n",
    "    def confusion_matrix(self):\n",
    "        \"\"\" Returns and prints a confusion matrix. \n",
    "        \"\"\"\n",
    "        \n",
    "        srl_labels = list(self.confusion.keys())\n",
    "        \n",
    "        matrix = [[\"\"] + srl_labels] # headings\n",
    "        for l in srl_labels:\n",
    "            row = [l]\n",
    "            for k in srl_labels:\n",
    "                row.append(str(self.confusion[l][k]))\n",
    "            matrix.append(row)\n",
    "            \n",
    "        #matrix_txt = [[str(cell) for cell in row] for row in matrix]\n",
    "        \n",
    "        txt = \"\\n\".join([\"\\t\".join(row) for row in matrix])\n",
    "        \n",
    "        #print(txt)\n",
    "        return txt\n",
    "    \n",
    "    def save(self, metric, directory=dir_for_evaluations):\n",
    "        \"\"\" Writes the summary of an evaluation to a text file (at some diectory).\"\"\"\n",
    "        \n",
    "        summary = self.summary()\n",
    "        confusion_matrix = self.confusion_matrix()\n",
    "        best_sentences = \"\\n\".join([f\"Best sentences ({metric}):\"] + self.best_cases(metric, 5))\n",
    "        worst_sentences = \"\\n\".join([f\"Worst sentences ({metric}):\"] + self.worst_cases(metric, 5))\n",
    "        \n",
    "        output_to_save = summary + \"\\n\" + confusion_matrix + \"\\n\" + best_sentences + \"\\n\" + worst_sentences\n",
    "        \n",
    "        with open(f\"{directory}{self.name}_{metric}.txt\", \"w\") as e:\n",
    "            e.write(output_to_save)\n",
    "    \n",
    "    def print_summary(self):\n",
    "        \"\"\" Prints out the summary of an evaluation.\n",
    "        \"\"\"\n",
    "        summary = self.summary()\n",
    "        print(summary)\n",
    "        \n",
    "    def print_confusion_matrix(self):\n",
    "        \"\"\" Prints out the confusion matrix.\n",
    "        \"\"\"\n",
    "        c_matrix = self.confusion_matrix()\n",
    "        print(c_matrix)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluator(model, name, test_data = test, srl_labels = lst_labels, detach_me=False):\n",
    "    \"\"\" \n",
    "    \"\"\"\n",
    "    t1 = time.perf_counter()\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    evaluation = Evaluation(name)\n",
    "    \n",
    "    prediction_pooled = [] # to collect all predictions\n",
    "    truth_pooled = []      # to collect all true labels\n",
    "    seq_lengths = []       # to collect the length of sentences\n",
    "    confusion = {label: {label: 0 for label in srl_labels} for label in srl_labels} # for confusion matrix\n",
    "    metrics_calc = {\"accuracy\": [], \"f1_macro\": [], \"f1_weighted\": []} # to collect accuracy and f1 for every sentence\n",
    "    \n",
    "    #i=1 # in order to print out progress\n",
    "    for batch in test_data:\n",
    "        sentence = batch.sentence\n",
    "        predicate = batch.predicate\n",
    "        truth = batch.srlabel\n",
    "            \n",
    "        if detach_me == True: # to avoid some CUDA memory shortage issues\n",
    "            prediction = model(sentence, predicate, truth).detach().to(\"cpu\")\n",
    "            truth = batch.srlabel.detach().to(\"cpu\")\n",
    "        \n",
    "        else:\n",
    "            prediction = model(sentence, predicate, truth) \n",
    "        \n",
    "        batched_pred_labels = prediction[:, 1:, :].argmax(2) # ... should not be batch-wise, but sentence-wise\n",
    "        batched_true_labels = truth[:, 1:]\n",
    "        \n",
    "        bsz = batched_pred_labels.shape[0]\n",
    "        \n",
    "        for b in range(bsz):\n",
    "            lst_sent    = [vocab.itos[token] for token in sentence[b]]\n",
    "            str_sent    = \" \".join(lst_sent) \n",
    "            seq_len     = len([x for x in sentence[b] if vocab.itos[x] not in [\"<pad>\", \"<sos>\", \"<eos>\"]])\n",
    "            pred_labels = batched_pred_labels[b].tolist()\n",
    "            true_labels = batched_true_labels[b].tolist()\n",
    "            lst_lab     = [labels.itos[token] for token in pred_labels]\n",
    "            annot_sent  = \" \".join([f\"{w}/{sr}\" for w, sr in zip(lst_sent, lst_lab)])\n",
    "            \n",
    "            #accuracy, f1_macro, f1_micro, X = metrics(true_labels, pred_labels)\n",
    "            accuracy, f1_macro, f1_weighted = metrics(true_labels, pred_labels)\n",
    "            \n",
    "            prediction_pooled.extend(pred_labels)\n",
    "            truth_pooled.extend(true_labels)\n",
    "            seq_lengths.append(seq_len)\n",
    "            \n",
    "            for p, t in zip(pred_labels, true_labels):\n",
    "                confusion[srl_labels[p]][srl_labels[t]] += 1\n",
    "                \n",
    "            for m, v in zip([\"accuracy\", \"f1_macro\", \"f1_weighted\"], [accuracy, f1_macro, f1_weighted]):\n",
    "                metrics_calc[m].append( (f\"{str_sent}\\n{annot_sent}\", v) )\n",
    "    \n",
    "    #print(prediction_pooled)\n",
    "    \n",
    "    #pooled_accuracy, pooled_f1_macro, pooled_f1_micro, X = metrics(truth_pooled, prediction_pooled)\n",
    "    pooled_accuracy, pooled_f1_macro, pooled_f1_weighted = metrics(truth_pooled, prediction_pooled)\n",
    "\n",
    "    lst_accuracy    = list(zip(*metrics_calc[\"accuracy\"]))[1]\n",
    "    lst_f1_macro    = list(zip(*metrics_calc[\"f1_macro\"]))[1]\n",
    "    lst_f1_weighted = list(zip(*metrics_calc[\"f1_weighted\"]))[1]\n",
    "\n",
    "    evaluation.pooled_acc         = pooled_accuracy\n",
    "    evaluation.pooled_f1_macro    = pooled_f1_macro\n",
    "    evaluation.pooled_f1_weighted = pooled_f1_weighted \n",
    "\n",
    "    evaluation.mean_acc         = mean(lst_accuracy)\n",
    "    evaluation.mean_f1_macro    = mean(lst_f1_macro)\n",
    "    evaluation.mean_f1_weighted = mean(lst_f1_weighted)\n",
    "    \n",
    "    evaluation.corr_l_acc         = np.corrcoef(lst_accuracy, seq_lengths)[0][1] # double zero indices due to output of numpy.corrcoef\n",
    "    evaluation.corr_l_f1_macro    = np.corrcoef(lst_f1_macro, seq_lengths)[0][1]\n",
    "    evaluation.corr_l_f1_weighted = np.corrcoef(lst_f1_weighted, seq_lengths)[0][1]\n",
    "\n",
    "    evaluation.confusion    = confusion\n",
    "    evaluation.metrics_dict = metrics_calc\n",
    "    \n",
    "    t2 = time.perf_counter()\n",
    "    passed_time = t2 - t1\n",
    "    print(\"Done! ({} m., {} s.)\".format(int(passed_time/60), int(passed_time%60)))\n",
    "    \n",
    "    return evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l1 = [4, 4, 4, 4, 4, 8, 7, 13, 6, 6, 6, 4, 4, 4, 4, 4, 3]\n",
    "l2 = [10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 3, 3, 3]\n",
    "print(f1_score(l1, l2, labels = [4, 8, 7], average = None))\n",
    "print(f1_score(l1, l2, average = None))\n",
    "print(f1_score(l1, l2, average = \"macro\"))\n",
    "print(f1_score(l1, l2, average = 'weighted'))\n",
    "print(accuracy_score(l1, l2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done! (1 m., 15 s.)\n"
     ]
    }
   ],
   "source": [
    "srl_evaluation = evaluator(my_SRLLabeler, model_name, detach_me = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model srl_b32_e20_csample performs as follows:\n",
      "Pooled Accuracy: 0.7463785110657692\n",
      "Pooled F1_macro: 0.02614989755525346\n",
      "Pooled F1_weighted: 0.8483006258811614\n",
      "Mean Accuracy: 0.6483582553058549 (std = 0.2762150390430822)\n",
      "Mean F1_macro: 0.2709454822124985 (std = 0.10186018694414628)\n",
      "Mean F1_weighted: 0.7337662992384725 (std = 0.2641714424669466)\n",
      "Correlation sentence length and accuracy: 0.44448027222812175\n",
      "Correlation sentence length and F1_macro: -0.031709881796994334\n",
      "Correlation sentence length and F1_weighted: 0.4041647225347743\n"
     ]
    }
   ],
   "source": [
    "srl_evaluation.print_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t<unk>\t<pad>\t<sos>\t<eos>\tO\tI-ARG1\tI-ARG2\tB-PRD\tB-ARG1\tI-ARG0\tI-ARGM-ADV\tB-ARG0\tI-ARGM-TMP\tI-ARGM-LOC\tB-ARG2\tI-ARGM-MNR\tI-ARGM-PRP\tB-ARGM-TMP\tB-ARGM-MOD\tB-ARGM-ADV\tI-ARGM-CAU\tB-ARGM-MNR\tB-ARGM-LOC\tI-ARG3\tI-C-ARG1\tI-ARGM-ADJ\tB-ARGM-DIS\tB-ARGM-NEG\tI-R-ARG0\tI-ARG4\tI-ARGM-DIR\tB-ARGM-PRP\tB-R-ARG0\tB-R-ARG1\tI-ARGM-PRD\tB-ARGM-ADJ\tB-ARGM-DIR\tI-ARGM-DIS\tB-ARG3\tB-ARGM-CAU\tI-ARGM-GOL\tI-ARGM-PNC\tB-ARGM-EXT\tB-ARG4\tI-R-ARG1\tB-R-ARGM-LOC\tI-ARGM-COM\tB-ARGM-PRD\tB-ARGM-LVB\tB-C-ARG1\tI-ARGM-EXT\tI-C-ARG2\tB-ARGM-GOL\tB-R-ARGM-TMP\tB-R-ARG2\tB-ARGM-COM\tI-R-ARGM-LOC\tB-ARGM-PNC\tB-R-ARGM-MNR\tI-R-ARGM-MNR\tB-ARGM-REC\tI-R-ARG2\tI-C-ARGM-MNR\tB-C-ARG2\tI-ARGM-NEG\tI-C-ARGM-LOC\tI-R-ARGM-TMP\tB-R-ARGM-CAU\tB-R-ARG3\tI-R-ARG3\tB-R-ARGM-GOL\tI-R-ARGM-GOL\tB-R-ARGM-DIR\tI-C-ARGM-ADV\tI-C-ARGM-EXT\tI-R-ARGM-DIR\tB-C-ARGM-LOC\tB-C-ARGM-MNR\tI-C-ARG0\tI-R-ARGM-CAU\tB-C-ARG0\tB-C-ARGM-ADV\tB-C-ARGM-EXT\tB-R-ARGM-ADV\tB-R-ARGM-COM\tI-R-ARGM-ADV\tI-R-ARGM-COM\n",
      "<unk>\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\n",
      "<pad>\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\n",
      "<sos>\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\n",
      "<eos>\t0\t3027\t0\t22532\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\n",
      "O\t16\t1031\t0\t496\t739400\t80270\t31263\t22317\t18882\t16682\t10235\t9255\t8852\t7187\t6236\t5176\t4943\t3329\t2395\t1955\t1965\t1958\t1812\t1037\t1095\t860\t848\t775\t685\t751\t593\t609\t623\t544\t479\t318\t299\t321\t286\t233\t175\t281\t182\t203\t133\t158\t125\t120\t88\t111\t77\t12\t45\t42\t42\t39\t46\t35\t37\t32\t14\t15\t0\t2\t4\t0\t9\t7\t8\t8\t3\t3\t0\t4\t0\t0\t0\t0\t9\t0\t2\t1\t0\t0\t0\t0\t0\n",
      "I-ARG1\t0\t0\t0\t0\t2379\t1199\t422\t572\t395\t219\t75\t140\t120\t154\t135\t87\t60\t69\t89\t32\t0\t52\t59\t35\t4\t8\t10\t21\t0\t16\t24\t9\t4\t7\t7\t6\t12\t10\t11\t0\t3\t0\t4\t9\t1\t0\t1\t5\t0\t1\t0\t0\t1\t0\t0\t1\t0\t0\t0\t0\t1\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\n",
      "I-ARG2\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\n",
      "B-PRD\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\n",
      "B-ARG1\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\n",
      "I-ARG0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\n",
      "I-ARGM-ADV\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\n",
      "B-ARG0\t0\t0\t0\t0\t1613\t0\t0\t139\t545\t0\t0\t665\t0\t0\t30\t0\t0\t72\t9\t43\t0\t14\t10\t0\t0\t0\t57\t1\t0\t0\t0\t5\t9\t2\t0\t2\t0\t0\t0\t7\t0\t0\t0\t0\t0\t0\t0\t2\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\n",
      "I-ARGM-TMP\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\n",
      "I-ARGM-LOC\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\n",
      "B-ARG2\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\n",
      "I-ARGM-MNR\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\n",
      "I-ARGM-PRP\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\n",
      "B-ARGM-TMP\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\n",
      "B-ARGM-MOD\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\n",
      "B-ARGM-ADV\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\n",
      "I-ARGM-CAU\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\n",
      "B-ARGM-MNR\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\n",
      "B-ARGM-LOC\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\n",
      "I-ARG3\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\n",
      "I-C-ARG1\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\n",
      "I-ARGM-ADJ\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\n",
      "B-ARGM-DIS\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\n",
      "B-ARGM-NEG\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\n",
      "I-R-ARG0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\n",
      "I-ARG4\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\n",
      "I-ARGM-DIR\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\n",
      "B-ARGM-PRP\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\n",
      "B-R-ARG0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\n",
      "B-R-ARG1\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\n",
      "I-ARGM-PRD\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\n",
      "B-ARGM-ADJ\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\n",
      "B-ARGM-DIR\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\n",
      "I-ARGM-DIS\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\n",
      "B-ARG3\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\n",
      "B-ARGM-CAU\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\n",
      "I-ARGM-GOL\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\n",
      "I-ARGM-PNC\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\n",
      "B-ARGM-EXT\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\n",
      "B-ARG4\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\n",
      "I-R-ARG1\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\n",
      "B-R-ARGM-LOC\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\n",
      "I-ARGM-COM\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\n",
      "B-ARGM-PRD\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\n",
      "B-ARGM-LVB\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\n",
      "B-C-ARG1\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\n",
      "I-ARGM-EXT\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\n",
      "I-C-ARG2\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\n",
      "B-ARGM-GOL\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\n",
      "B-R-ARGM-TMP\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\n",
      "B-R-ARG2\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\n",
      "B-ARGM-COM\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\n",
      "I-R-ARGM-LOC\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\n",
      "B-ARGM-PNC\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\n",
      "B-R-ARGM-MNR\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\n",
      "I-R-ARGM-MNR\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\n",
      "B-ARGM-REC\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\n",
      "I-R-ARG2\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\n",
      "I-C-ARGM-MNR\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\n",
      "B-C-ARG2\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\n",
      "I-ARGM-NEG\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\n",
      "I-C-ARGM-LOC\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\n",
      "I-R-ARGM-TMP\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\n",
      "B-R-ARGM-CAU\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\n",
      "B-R-ARG3\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\n",
      "I-R-ARG3\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\n",
      "B-R-ARGM-GOL\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\n",
      "I-R-ARGM-GOL\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\n",
      "B-R-ARGM-DIR\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\n",
      "I-C-ARGM-ADV\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\n",
      "I-C-ARGM-EXT\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\n",
      "I-R-ARGM-DIR\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\n",
      "B-C-ARGM-LOC\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\n",
      "B-C-ARGM-MNR\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\n",
      "I-C-ARG0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\n",
      "I-R-ARGM-CAU\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\n",
      "B-C-ARG0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\n",
      "B-C-ARGM-ADV\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\n",
      "B-C-ARGM-EXT\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\n",
      "B-R-ARGM-ADV\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\n",
      "B-R-ARGM-COM\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\n",
      "I-R-ARGM-ADV\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\n",
      "I-R-ARGM-COM\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\n"
     ]
    }
   ],
   "source": [
    "srl_evaluation.print_confusion_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<sos> their experience can assist in developing and implementing the organising plan and assist newly elected branch officials in their positions . <eos>\\n<sos>/B-ARG0 their/O experience/O can/O assist/O in/O developing/O and/O implementing/O the/O organising/O plan/O and/O assist/O newly/O elected/O branch/O officials/O in/O their/O positions/O ./<eos>'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "srl_evaluation.best_case(\"f1_macro\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<sos> ists | nonesuch film music series ( music from the films of francois truffaut ) - georges delerue - noordt : organ works , vol 1 | noordt : organ works , vol 2 - nordic ethno grooves vol.3 - various artists no no nanette - original broadway cast no no nanette - original broadway cast no no no - dawn penn no no no - illustrious gy no no no - various artists no no no cha cha cha - brave combo no noise ( the very best of eyeless in gaza ) - eyeless in gaza no nose job - digital underground no nuclear war [ remastered ] - peter tosh no offence - stormwindows no offence meant plenty taken - headcleaner no offence meant plenty taken - headcleaner no one [ pa ] - no one ( rock ) no one 4 property no one but you - chris jones & the night drivers no one can hear you dvd no one cares - frank sinatra no one could protect her dvd no one else - kurt carr singers ( the ) no one here gets out alive ( an audio documentary ) - doors ( the ) no one here gets out alive - jerry hopkins and danny sugerman no one is alone <eos>\\n<sos>/O ists/O |/O nonesuch/O film/O music/O series/O (/O music/O from/O the/O films/O of/O francois/O truffaut/O )/O -/O georges/O delerue/O -/O noordt/O :/O organ/O works/O ,/O vol/O 1/O |/O noordt/O :/O organ/O works/O ,/O vol/O 2/O -/O nordic/O ethno/O grooves/O vol.3/O -/O various/O artists/O no/O no/O nanette/O -/O original/O broadway/O cast/O no/O no/O nanette/O -/O original/O broadway/O cast/O no/O no/O no/O -/O dawn/O penn/O no/O no/O no/O -/O illustrious/O gy/O no/O no/O no/O -/O various/O artists/O no/O no/O no/O cha/O cha/O cha/O -/O brave/O combo/O no/O noise/O (/O the/O very/O best/O of/O eyeless/O in/O gaza/O )/O -/O eyeless/O in/O gaza/O no/O nose/O job/O -/O digital/O underground/O no/O nuclear/O war/O [/O remastered/O ]/O -/O peter/O tosh/O no/O offence/O -/O stormwindows/O no/O offence/O meant/O plenty/O taken/O -/O headcleaner/O no/O offence/O meant/O plenty/O taken/O -/O headcleaner/O no/O one/O [/O pa/O ]/O -/O no/O one/O (/O rock/O )/O no/O one/O 4/O property/O no/O one/O but/O you/O -/O chris/O jones/O &/O the/O night/O drivers/O no/O one/O can/O hear/O you/O dvd/O no/O one/O cares/O -/O frank/O sinatra/O no/O one/O could/O protect/O her/O dvd/O no/O one/O else/O -/O kurt/O carr/O singers/O (/O the/O )/O no/O one/O here/O gets/O out/O alive/O (/O an/O audio/O documentary/O )/O -/O doors/O (/O the/O )/O no/O one/O here/O gets/O out/O alive/O -/O jerry/O hopkins/O and/O danny/O sugerman/O no/O one/O is/O alone/<eos>',\n",
       " '<sos> 63 r ] collectyons of john corbyns speche towching ireland & the erlle of desmond . / firste that he sawe the erlle in the feld on ester even & on the tewsday in the ester weke , accompanied with the grettest personaigs of the contrey as mach morys , sir john of desmond , james fitzmoris , macharte rew & others , havyng with hym above 3000 . men well appointed , & had taken pledges of macharte moore , that he shold be with hym uppon a short warning with 1500 . men furnished . / he had allso taken pledges of the maylles , of the morach conderoryes , of the donelles , [ marginalia ( by herle ) : of macharte rew . ] & of a nomber more to attend uppon hym with their forces when he shold appoint them aswell by sea as by land , having appointed the donelles to prepare all the galleyes they had , so as his preparatyon wold rise to xxm . men & to a xxxviij galleys , the sayd corbin being offred the charge of a m . men & of viij galleys . / yt was concluded by the sayd <eos>\\n<sos>/O 63/O r/O ]/O collectyons/O of/O john/O corbyns/O speche/O towching/O ireland/O &/O the/O erlle/O of/O desmond/O ./O //O firste/O that/O he/O sawe/O the/O erlle/O in/O the/O feld/O on/O ester/O even/O &/O on/O the/O tewsday/O in/O the/O ester/O weke/O ,/O accompanied/O with/O the/O grettest/O personaigs/O of/O the/O contrey/O as/O mach/O morys/O ,/O sir/O john/O of/O desmond/O ,/O james/O fitzmoris/O ,/O macharte/O rew/O &/O others/O ,/O havyng/O with/O hym/O above/O 3000/O ./O men/O well/O appointed/O ,/O &/O had/O taken/O pledges/O of/O macharte/O moore/O ,/O that/O he/O shold/O be/O with/O hym/O uppon/O a/O short/O warning/O with/O 1500/O ./O men/O furnished/O ./O //O he/O had/O allso/O taken/O pledges/O of/O the/O maylles/O ,/O of/O the/O morach/O conderoryes/O ,/O of/O the/O donelles/O ,/O [/O marginalia/O (/O by/O herle/O )/O :/O of/O macharte/O rew/O ./O ]/O &/O of/O a/O nomber/O more/O to/O attend/O uppon/O hym/O with/O their/O forces/O when/O he/O shold/O appoint/O them/O aswell/O by/O sea/O as/O by/O land/O ,/O having/O appointed/O the/O donelles/O to/O prepare/O all/O the/O galleyes/O they/O had/O ,/O so/O as/O his/O preparatyon/O wold/O rise/O to/O xxm/O ./O men/O &/O to/O a/O xxxviij/O galleys/O ,/O the/O sayd/O corbin/O being/O offred/O the/O charge/O of/O a/O m/O ./O men/O &/O of/O viij/O galleys/O ./O //O yt/O was/O concluded/O by/O the/O sayd/<eos>',\n",
       " '<sos> if you do however find out that it is diet then you will know that despite you are belief that being married has improved you are diet , you are just one of the majority for whom marriage has changed it for the worse . __________________ i want it all , i want it now and i want it covered in chocolate ... linny view public profile find more posts by linny 07 - 07 - 2006 , 03 : 10 pm # 8 odmaster registered user join date : aug 2004 location : exiled scot living in north wales posts : 92 would not have thought it was diet as i used to \" forget \" to eat or could not be bothered so surely my energy levels were lower than they are now when i eat a proper evening meal ... and i am happier now than i was a couple of years ago ( which is when i was making some good improvements ... ) guess i shall never know ... and anyway , after a minor op yesterday , i now have to stay out of the pool for 4 weeks ! <eos>\\n<sos>/O if/O you/O do/O however/O find/O out/O that/O it/O is/O diet/O then/O you/O will/O know/O that/O despite/O you/O are/O belief/O that/O being/O married/O has/O improved/O you/O are/O diet/O ,/O you/O are/O just/O one/O of/O the/O majority/O for/O whom/O marriage/O has/O changed/O it/O for/O the/O worse/O ./O __________________/O i/O want/O it/O all/O ,/O i/O want/O it/O now/O and/O i/O want/O it/O covered/O in/O chocolate/O .../O linny/O view/O public/O profile/O find/O more/O posts/O by/O linny/O 07/O -/O 07/O -/O 2006/O ,/O 03/O :/O 10/O pm/O #/O 8/O odmaster/O registered/O user/O join/O date/O :/O aug/O 2004/O location/O :/O exiled/O scot/O living/O in/O north/O wales/O posts/O :/O 92/O would/O not/O have/O thought/O it/O was/O diet/O as/O i/O used/O to/O \"/O forget/O \"/O to/O eat/O or/O could/O not/O be/O bothered/O so/O surely/O my/O energy/O levels/O were/O lower/O than/O they/O are/O now/O when/O i/O eat/O a/O proper/O evening/O meal/O .../O and/O i/O am/O happier/O now/O than/O i/O was/O a/O couple/O of/O years/O ago/O (/O which/O is/O when/O i/O was/O making/O some/O good/O improvements/O .../O )/O guess/O i/O shall/O never/O know/O .../O and/O anyway/O ,/O after/O a/O minor/O op/O yesterday/O ,/O i/O now/O have/O to/O stay/O out/O of/O the/O pool/O for/O 4/O weeks/O !/<eos>']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "srl_evaluation.best_cases(\"accuracy\", 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<sos> writing from snowdonia in wales , judy harris brings some connections alive for us in from shoreline to shoreline - aspects of copper , venus and the birch tree . <eos> <pad>\\n<sos>/O writing/O from/O snowdonia/O in/O wales/O ,/O judy/O harris/O brings/O some/O connections/O alive/O for/O us/O in/O from/O shoreline/O to/O shoreline/O -/O aspects/O of/O copper/O ,/O venus/O and/O the/O birch/O tree/O ./<eos> <eos>/<eos>'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "srl_evaluation.worst_case(\"f1_macro\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<sos> interview workshop - book with you are partnership university careers service . sym 2006 the bournemouth & poole college | terms & conditions | privacy statement | accessibility | email this page | log in <eos>\\n<sos>/O interview/O workshop/O -/O book/O with/O you/O are/O partnership/O university/O careers/O service/O ./O sym/O 2006/O the/O bournemouth/O &/O poole/O college/O |/O terms/O &/O conditions/O |/O privacy/O statement/O |/O accessibility/O |/O email/O this/O page/O |/O log/O in/<eos>',\n",
       " '<sos> more access information venues theatre resource partners in the project neighbourhood fund action in mental health people who attend event may also be interested in digital art drop - in sessions at theatre resource <eos>\\n<sos>/O more/O access/O information/O venues/O theatre/O resource/O partners/O in/O the/O project/O neighbourhood/O fund/O action/O in/O mental/O health/O people/O who/O attend/O event/O may/O also/O be/O interested/O in/O digital/O art/O drop/O -/O in/O sessions/O at/O theatre/O resource/<eos>',\n",
       " \"<sos> posted by tmatt at 10 : 09 pm | comments ( 13 ) saturday , october 29 , 2005 more on rice 's return to rome i can not tell you how many times i have had readers ask me why so many religion - news stories seem to turn on the issue of homosexuality . <eos>\\n<sos>/O posted/O by/O tmatt/O at/O 10/O :/O 09/O pm/O |/O comments/O (/O 13/O )/O saturday/O ,/O october/O 29/O ,/O 2005/O more/O on/O rice/O 's/O return/O to/O rome/O i/O can/O not/O tell/O you/O how/O many/O times/O i/O have/O had/O readers/O ask/O me/O why/O so/O many/O religion/O -/O news/O stories/O seem/O to/O turn/O on/O the/O issue/O of/O homosexuality/O ./<eos>\"]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "srl_evaluation.worst_cases(\"accuracy\", 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "srl_evaluation.save(\"accuracy\")\n",
    "srl_evaluation.save(\"f1_macro\")\n",
    "srl_evaluation.save(\"f1_weighted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
