{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random, time, operator\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchtext\n",
    "import torch.nn.functional as F\n",
    "#from torchtext.data import Field, BucketIterator, Iterator, TabularDataset\n",
    "from torchtext.legacy.data import Field, BucketIterator, Iterator, TabularDataset # Needed for running this on my laptop\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Meta variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:1')\n",
    "#device = torch.device('cpu')\n",
    "\n",
    "my_data_directory = \"../data/\" # where to store files on mltgpu\n",
    "my_models_directory = \"../models/\"\n",
    "\n",
    "mini_testing = False\n",
    "my_train_file = \"mini_train.csv\" if mini_testing == True else \"train.csv\"\n",
    "my_test_file  = \"mini_test.csv\" if mini_testing == True else \"test.csv\"\n",
    "\n",
    "dir_for_evaluations = \"../evals/\" # my settings on MLTGPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataloader(directory  = my_data_directory,\n",
    "               train_file = my_train_file,\n",
    "               test_file  = my_test_file,\n",
    "               batch      = batch_size):\n",
    "    \n",
    "    num_whitespacer = lambda x: [int(e) for e in x.split(\" \")]\n",
    "    \n",
    "    SENTENCE = Field(lower = True, \n",
    "                     batch_first = True, \n",
    "                     init_token = \"<start>\", \n",
    "                     eos_token = \"<end>\")\n",
    "    \n",
    "    PREDICATE = Field(tokenize = num_whitespacer, # Here might be some problems ...\n",
    "                      batch_first = True, \n",
    "                      pad_token = 0,\n",
    "                      use_vocab = False,\n",
    "                      init_token = 0, \n",
    "                      eos_token = 0) \n",
    "    \n",
    "    SRLABEL = Field(batch_first = True, \n",
    "                    init_token = \"<start>\", \n",
    "                    eos_token = \"<end>\")\n",
    "    \n",
    "    my_fields = [(\"sentence\", SENTENCE),\n",
    "                 (\"predicate\", PREDICATE),\n",
    "                 (\"srlabel\", SRLABEL)]\n",
    "    \n",
    "    train, test = TabularDataset.splits(path   = directory,\n",
    "                                        train  = train_file,\n",
    "                                        test   = test_file,\n",
    "                                        format = 'csv',\n",
    "                                        fields = my_fields,\n",
    "                                        csv_reader_params = {'delimiter':'\\t',\n",
    "                                                             'quotechar':'Â¤'}) # Seems not to be in data\n",
    "    SENTENCE.build_vocab(train)\n",
    "    SRLABEL.build_vocab(train)  \n",
    "\n",
    "    train_iter, test_iter = BucketIterator.splits((train, test),\n",
    "                                                  batch_size        = batch,\n",
    "                                                  sort_within_batch = True,\n",
    "                                                  sort_key          = lambda x: len(x.sentence),\n",
    "                                                  shuffle           = True,\n",
    "                                                  device            = device)\n",
    "\n",
    "    return train_iter, test_iter, SENTENCE.vocab, SRLABEL.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test, vocab, labels = dataloader()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Bidirectional LSTM SR Labeler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleBiLSTM(nn.Module):\n",
    "    def __init__(self, voc_size, embedding_size, n_labels, n_layers, dropout):  \n",
    "        super(SimpleBiLSTM, self).__init__()\n",
    "        \n",
    "        self.embeddings = nn.Embedding(voc_size, embedding_size)\n",
    "        self.sp_pair = embedding_size + 1 # emedded sentence + predicate vector\n",
    "        \n",
    "        self.rnn = nn.LSTM(self.sp_pair, n_labels, # input DIM --> output DIM\n",
    "                           num_layers = n_layers, \n",
    "                           bidirectional=True, \n",
    "                           batch_first=True, \n",
    "                           dropout = dropout)\n",
    "        \n",
    "    def forward(self, sentences, pred_vec, softmax = False):\n",
    "        \n",
    "        embeddings = self.embeddings(sentences)\n",
    "        pred_vec = pred_vec.unsqueeze(2)        \n",
    "        sentence_pred_pair = torch.cat((embeddings, pred_vec), dim=2)\n",
    "        contextualized_embedding, *_ = self.rnn(sentence_pred_pair)\n",
    "        \n",
    "        if softmax == True:\n",
    "            return F.softmax(contextualized_embedding, dim=2)\n",
    "        else:\n",
    "            return contextualized_embedding"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "class SRLabeler1(nn.Module):\n",
    "    def __init__(self, voc_size, embedding_size, n_labels, n_layers):  \n",
    "        super(SRLabeler1, self).__init__()\n",
    "        \n",
    "        self.embeddings = nn.Embedding(voc_size, embedding_size)\n",
    "        self.sp_pair = embedding_size + 1 # emedded sentence + predicate vector\n",
    "        self.rnn = nn.LSTM(self.sp_pair, n_labels, num_layers = n_layers, bidirectional=True, batch_first=True)\n",
    "        \n",
    "    def forward(self, sentences, pred_vec, softmax=False):\n",
    "        \n",
    "        embeddings = self.embeddings(sentences)\n",
    "        pred_vec = pred_vec.unsqueeze(2)        \n",
    "        sentence_pred_pair = torch.cat((embeddings, pred_vec), dim=2)\n",
    "        contextualized_embedding, *_ = self.rnn(sentence_pred_pair)\n",
    "        \n",
    "        if softmax == True:\n",
    "            return F.softmax(contextualized_embedding, dim=2)\n",
    "        else:\n",
    "            return contextualized_embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainer(model, # Must be an instance of a model!\n",
    "            name_of_model,\n",
    "            learning_rate,\n",
    "            epochs,\n",
    "            data,\n",
    "            clip_grad = None,\n",
    "            ignore_pad = False,\n",
    "            val_data = None,\n",
    "            save_model = False,\n",
    "            directory = my_models_directory,\n",
    "            my_loss_function = nn.CrossEntropyLoss,\n",
    "            my_optimizer = optim.Adam\n",
    "           ):\n",
    "    \"\"\" Specifices a general training procedure for a model. \n",
    "        Note: trainer() requires an instantiated model as model argument. \n",
    "    \"\"\"\n",
    "    \n",
    "    optimizer = my_optimizer(model.parameters(), lr=learning_rate)    \n",
    "    \n",
    "    model.to(device)\n",
    "    model.train()\n",
    "    \n",
    "    if ignore_pad:\n",
    "        pad_idx = labels.stoi[\"<pad>\"]\n",
    "        loss_function = my_loss_function(ignore_index=pad_idx) # We ignore pad token in loss calculation\n",
    "    else:\n",
    "        loss_function = my_loss_function()\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        print(f\"Epoch: {epoch+1} (out of {epochs}).\")\n",
    "        epoch_loss = 0\n",
    "        \n",
    "        for i, batch in enumerate(data):\n",
    "            print(\"Batch: \", i, end=\"\\r\")\n",
    "            optimizer.zero_grad # reset gradients\n",
    "            \n",
    "            sentence = batch.sentence\n",
    "            predicate = batch.predicate\n",
    "            targets = batch.srlabel\n",
    "            \n",
    "            b=sentence.shape[0] # !\n",
    "            sequence_length = sentence.shape[1] # !\n",
    "            l = targets.shape[1] # !\n",
    "                        \n",
    "            output = model(sentence, predicate)\n",
    "            d = output.shape[2] # !\n",
    "            \n",
    "            #print(\"Output:\", output.shape)\n",
    "            #print(\"Target:\", targets.shape)\n",
    "            \n",
    "            loss = loss_function(output.reshape(b*sequence_length, d), # !\n",
    "                                 targets.reshape(b*sequence_length))\n",
    "            \n",
    "            epoch_loss += loss.item()\n",
    "            \n",
    "            if clip_grad != None:\n",
    "                nn.utils.clip_grad_norm_(model.parameters(), clip_grad) # to handle exploding gradients\n",
    "            \n",
    "            loss.backward() # compute gradients\n",
    "            optimizer.step() # update parameters\n",
    "            #break\n",
    "            \n",
    "        print(f\"Total loss for Epoch {epoch+1}: {epoch_loss}.\")\n",
    "            \n",
    "        if val_data != None:\n",
    "            model.eval()\n",
    "            # Here we could do some evaluation of model progress, but I have ignored this for now. \n",
    "            model.train()\n",
    "            \n",
    "    if save_model == True:\n",
    "        torch.save(model, directory+name_of_model+\".pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of vocabulary: 34068\n",
      "Number of labels: 87\n"
     ]
    }
   ],
   "source": [
    "vocab_size = len(vocab)\n",
    "number_of_labels = len(labels)\n",
    "print(\"Size of vocabulary:\", vocab_size)\n",
    "print(\"Number of labels:\", number_of_labels)\n",
    "epochs = 10\n",
    "my_learning_rate = 0.01\n",
    "my_emedding_size = 512\n",
    "\n",
    "my_dropout = 0.2\n",
    "number_of_layers = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calling and training model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "SimpleModel = SimpleBiLSTM(vocab_size, my_emedding_size, number_of_labels, number_of_layers, my_dropout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Module.parameters of SimpleBiLSTM(\n",
       "  (embeddings): Embedding(34068, 512)\n",
       "  (rnn): LSTM(513, 87, num_layers=4, batch_first=True, dropout=0.2, bidirectional=True)\n",
       ")>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SimpleModel.parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 (out of 10).\n",
      "Total loss for Epoch 1: 3193.007613182068.\n",
      "Epoch: 2 (out of 10).\n",
      "Total loss for Epoch 2: 3189.3753547668457.\n",
      "Epoch: 3 (out of 10).\n",
      "Total loss for Epoch 3: 3189.3469820022583.\n",
      "Epoch: 4 (out of 10).\n",
      "Total loss for Epoch 4: 3189.3746695518494.\n",
      "Epoch: 5 (out of 10).\n",
      "Total loss for Epoch 5: 3189.405362844467.\n",
      "Epoch: 6 (out of 10).\n",
      "Total loss for Epoch 6: 3189.301868915558.\n",
      "Epoch: 7 (out of 10).\n",
      "Total loss for Epoch 7: 3189.28564953804.\n",
      "Epoch: 8 (out of 10).\n",
      "Total loss for Epoch 8: 3189.4384977817535.\n",
      "Epoch: 9 (out of 10).\n",
      "Total loss for Epoch 9: 3189.3170397281647.\n",
      "Epoch: 10 (out of 10).\n",
      "Total loss for Epoch 10: 3189.4088864326477.\n"
     ]
    }
   ],
   "source": [
    "protoname = f\"simple_b{batch_size}ep{epochs}ly{number_of_layers}em{my_emedding_size}do{str(my_dropout)[2:]}lr{str(my_learning_rate)[2:]}\"\n",
    "model_name = f\"{protoname}_minisample\" if mini_testing else f\"{protoname}_csample\"\n",
    "\n",
    "trainer(SimpleModel, model_name, my_learning_rate, epochs, train, save_model=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "lst_labels = [labels.itos[x] for x in range(len(labels))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def metrics(prediction, \n",
    "            truth \n",
    "            #labels = lst_labels\n",
    "           ):\n",
    "    \"\"\" Calculates accuracy and F1, given two sequences (lists, arrays) of labels. Since, \n",
    "        these metrices here are used for multi-label classification, two versions \n",
    "        of F1 are calculated: \"macro\" and \"weigthed\", where the former is the mean of F1 for\n",
    "        each label, and the latter is the mean weigthed by support (the number of true \n",
    "        instances for each label), which account for label imbalance.\n",
    "    \"\"\"\n",
    "    \n",
    "    accuracy = accuracy_score(truth, prediction)\n",
    "    f1_macro = f1_score(truth, prediction, average = \"macro\") # Calculate metrics for each label, and find their unweighted mean. Does not take label imbalance into account.\n",
    "    f1_weighted = f1_score(truth, prediction, average = \"weighted\") # Calculate metrics globally by counting the total true positives, false negatives and false positives.\n",
    "    \n",
    "    return accuracy, f1_macro, f1_weighted\n",
    "\n",
    "def mean(array):\n",
    "    \"\"\" Calculates the mean and standard deviation of an aray of numbers.\n",
    "    \"\"\"\n",
    "    mean = np.mean(array)\n",
    "    std  = np.std(array)\n",
    "    return mean, std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Evaluation:\n",
    "    \"\"\" For storing and handling information from the evaluation of model(s).\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        \n",
    "        self.pooled_acc         = \"Not yet defined\"\n",
    "        self.pooled_f1_macro    = \"Not yet defined\"\n",
    "        self.pooled_f1_weighted = \"Not yet defined\" \n",
    "        \n",
    "        self.mean_acc           = (\"Not yet defined\", \"Not yet defined\")\n",
    "        self.mean_f1_macro      = (\"Not yet defined\", \"Not yet defined\")\n",
    "        self.mean_f1_weighted   = (\"Not yet defined\", \"Not yet defined\")\n",
    "        \n",
    "        self.corr_l_acc         = \"Not yet defined\"\n",
    "        self.corr_l_f1_macro    = \"Not yet defined\"\n",
    "        self.corr_l_f1_weighted = \"Not yet defined\"\n",
    "        \n",
    "        self.confusion = {\"Not yet defined\": {\"Not yet defined\": \"Not yet defined\"}}\n",
    "        self.metrics_dict = {\"accuracy\": [\"Not yet defined\", \"Not yet defined\"], \n",
    "                             \"f1_macro\": [\"Not yet defined\", \"Not yet defined\"], \n",
    "                             \"f1_weighted\": [\"Not yet defined\", \"Not yet defined\"]}\n",
    "\n",
    "    def best_case(self, metric):\n",
    "        \"\"\" Returns the file which has the best performance score with respect \n",
    "            to a metric.\n",
    "        \"\"\"\n",
    "        m_list = self.metrics_dict[metric]\n",
    "        zic_zac = False if metric == \"mse\" else True\n",
    "        m_list.sort(key=operator.itemgetter(1), reverse=zic_zac)\n",
    "        return m_list[0][0]\n",
    "    \n",
    "    def best_cases(self, metric, n):\n",
    "        \"\"\" Returns a list of the N files which has the best performance score \n",
    "            with respect to a metric.\n",
    "        \"\"\"\n",
    "        m_list = self.metrics_dict[metric]\n",
    "        zic_zac = False if metric == \"mse\" else True\n",
    "        m_list.sort(key=operator.itemgetter(1), reverse=zic_zac)\n",
    "        files, values = zip(*m_list)\n",
    "        return list(files[:n])\n",
    "    \n",
    "    def worst_case(self, metric):\n",
    "        \"\"\" Returns the file which has the best performance score with respect \n",
    "            to a metric.\n",
    "        \"\"\"\n",
    "        m_list = self.metrics_dict[metric]\n",
    "        zic_zac = True if metric == \"mse\" else False\n",
    "        m_list.sort(key=operator.itemgetter(1), reverse=zic_zac)\n",
    "        return m_list[0][0]\n",
    "\n",
    "    def worst_cases(self, metric, n):\n",
    "        \"\"\" Returns a list of the N files which has the best performance score \n",
    "            with respect to a metric.\n",
    "        \"\"\"\n",
    "        m_list = self.metrics_dict[metric]\n",
    "        zic_zac = True if metric == \"mse\" else False\n",
    "        m_list.sort(key=operator.itemgetter(1), reverse=zic_zac)\n",
    "        files, values = zip(*m_list)\n",
    "        return list(files[:n])\n",
    " \n",
    "    def summary(self):\n",
    "        \"\"\" Summarises an evaluation. Returns string.\"\"\"\n",
    "        summary  = \"\\n\".join([f\"Model {self.name} performs as follows:\", \n",
    "                      f\"Pooled Accuracy: {self.pooled_acc}\",\n",
    "                      f\"Pooled F1_macro: {self.pooled_f1_macro}\",\n",
    "                      f\"Pooled F1_weighted: {self.pooled_f1_weighted}\",\n",
    "                              \n",
    "                      f\"Mean Accuracy: {self.mean_acc[0]} (std = {self.mean_acc[1]})\",\n",
    "                      f\"Mean F1_macro: {self.mean_f1_macro[0]} (std = {self.mean_f1_macro[1]})\",\n",
    "                      f\"Mean F1_weighted: {self.mean_f1_weighted[0]} (std = {self.mean_f1_weighted[1]})\",\n",
    "                      \n",
    "                      f\"Correlation sentence length and accuracy: {self.corr_l_acc}\",\n",
    "                      f\"Correlation sentence length and F1_macro: {self.corr_l_f1_macro}\",\n",
    "                      f\"Correlation sentence length and F1_weighted: {self.corr_l_f1_weighted}\"]) \n",
    "        return summary\n",
    "    \n",
    "    def confusion_matrix(self):\n",
    "        \"\"\" Returns and prints a confusion matrix. \n",
    "        \"\"\"\n",
    "        \n",
    "        srl_labels = list(self.confusion.keys())\n",
    "        \n",
    "        matrix = [[\"\"] + srl_labels] # headings\n",
    "        for l in srl_labels:\n",
    "            row = [l]\n",
    "            for k in srl_labels:\n",
    "                row.append(str(self.confusion[l][k]))\n",
    "            matrix.append(row)\n",
    "            \n",
    "        #matrix_txt = [[str(cell) for cell in row] for row in matrix]\n",
    "        \n",
    "        txt = \"\\n\".join([\"\\t\".join(row) for row in matrix])\n",
    "        \n",
    "        #print(txt)\n",
    "        return txt\n",
    "    \n",
    "    def save(self, metric, directory=dir_for_evaluations):\n",
    "        \"\"\" Writes the summary of an evaluation to a text file (at some diectory).\"\"\"\n",
    "        \n",
    "        summary = self.summary()\n",
    "        confusion_matrix = self.confusion_matrix()\n",
    "        best_sentences = \"\\n\".join([f\"Best sentences ({metric}):\"] + self.best_cases(metric, 5))\n",
    "        worst_sentences = \"\\n\".join([f\"Worst sentences ({metric}):\"] + self.worst_cases(metric, 5))\n",
    "        \n",
    "        output_to_save = summary + \"\\n\" + confusion_matrix + \"\\n\" + best_sentences + \"\\n\" + worst_sentences\n",
    "        \n",
    "        with open(f\"{directory}{self.name}_{metric}.txt\", \"w\") as e:\n",
    "            e.write(output_to_save)\n",
    "    \n",
    "    def print_summary(self):\n",
    "        \"\"\" Prints out the summary of an evaluation.\n",
    "        \"\"\"\n",
    "        summary = self.summary()\n",
    "        print(summary)\n",
    "        \n",
    "    def print_confusion_matrix(self):\n",
    "        \"\"\" Prints out the confusion matrix.\n",
    "        \"\"\"\n",
    "        c_matrix = self.confusion_matrix()\n",
    "        print(c_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluator(model, name, test_data = test, srl_labels = lst_labels, detach_me=False):\n",
    "    \"\"\" \n",
    "    \"\"\"\n",
    "    t1 = time.perf_counter()\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    evaluation = Evaluation(name)\n",
    "    \n",
    "    prediction_pooled = [] # to collect all predictions\n",
    "    truth_pooled = []      # to collect all true labels\n",
    "    seq_lengths = []       # to collect the length of sentences\n",
    "    confusion = {label: {label: 0 for label in srl_labels} for label in srl_labels} # for confusion matrix\n",
    "    metrics_calc = {\"accuracy\": [], \"f1_macro\": [], \"f1_weighted\": []} # to collect accuracy and f1 for every sentence\n",
    "    \n",
    "    #i=1 # in order to print out progress\n",
    "    for batch in test_data:\n",
    "        sentence = batch.sentence\n",
    "        predicate = batch.predicate\n",
    "        truth = batch.srlabel\n",
    "            \n",
    "        if detach_me == True: # to avoid some CUDA memory shortage issues\n",
    "            prediction = model(sentence, predicate).detach().to(\"cpu\")\n",
    "            truth = batch.srlabel.detach().to(\"cpu\")\n",
    "        \n",
    "        else:\n",
    "            prediction = model(sentence, predicate) \n",
    "        \n",
    "        batched_pred_labels = prediction.argmax(2) \n",
    "        batched_true_labels = truth\n",
    "        \n",
    "        bsz = batched_pred_labels.shape[0]\n",
    "        \n",
    "        for b in range(bsz):\n",
    "            lst_sent    = [vocab.itos[token] for token in sentence[b]]\n",
    "            str_sent    = \" \".join(lst_sent) \n",
    "            seq_len     = len([x for x in sentence[b] if vocab.itos[x] not in [\"<pad>\", \"<sos>\", \"<eos>\"]])\n",
    "            pred_labels = batched_pred_labels[b].tolist()\n",
    "            true_labels = batched_true_labels[b].tolist()\n",
    "            lst_lab     = [labels.itos[token] for token in pred_labels]\n",
    "            annot_sent  = \" \".join([f\"{w}/{sr}\" for w, sr in zip(lst_sent, lst_lab)])\n",
    "            \n",
    "            #accuracy, f1_macro, f1_micro, X = metrics(true_labels, pred_labels)\n",
    "            accuracy, f1_macro, f1_weighted = metrics(true_labels, pred_labels)\n",
    "            \n",
    "            prediction_pooled.extend(pred_labels)\n",
    "            truth_pooled.extend(true_labels)\n",
    "            seq_lengths.append(seq_len)\n",
    "            \n",
    "            for p, t in zip(pred_labels, true_labels):\n",
    "                confusion[srl_labels[p]][srl_labels[t]] += 1\n",
    "                \n",
    "            for m, v in zip([\"accuracy\", \"f1_macro\", \"f1_weighted\"], [accuracy, f1_macro, f1_weighted]):\n",
    "                metrics_calc[m].append( (f\"{str_sent}\\n{annot_sent}\", v) )\n",
    "    \n",
    "    #print(prediction_pooled)\n",
    "    \n",
    "    #pooled_accuracy, pooled_f1_macro, pooled_f1_micro, X = metrics(truth_pooled, prediction_pooled)\n",
    "    pooled_accuracy, pooled_f1_macro, pooled_f1_weighted = metrics(truth_pooled, prediction_pooled)\n",
    "\n",
    "    lst_accuracy    = list(zip(*metrics_calc[\"accuracy\"]))[1]\n",
    "    lst_f1_macro    = list(zip(*metrics_calc[\"f1_macro\"]))[1]\n",
    "    lst_f1_weighted = list(zip(*metrics_calc[\"f1_weighted\"]))[1]\n",
    "\n",
    "    evaluation.pooled_acc         = pooled_accuracy\n",
    "    evaluation.pooled_f1_macro    = pooled_f1_macro\n",
    "    evaluation.pooled_f1_weighted = pooled_f1_weighted \n",
    "\n",
    "    evaluation.mean_acc         = mean(lst_accuracy)\n",
    "    evaluation.mean_f1_macro    = mean(lst_f1_macro)\n",
    "    evaluation.mean_f1_weighted = mean(lst_f1_weighted)\n",
    "    \n",
    "    evaluation.corr_l_acc         = np.corrcoef(lst_accuracy, seq_lengths)[0][1] # double zero indices due to output of numpy.corrcoef\n",
    "    evaluation.corr_l_f1_macro    = np.corrcoef(lst_f1_macro, seq_lengths)[0][1]\n",
    "    evaluation.corr_l_f1_weighted = np.corrcoef(lst_f1_weighted, seq_lengths)[0][1]\n",
    "\n",
    "    evaluation.confusion    = confusion\n",
    "    evaluation.metrics_dict = metrics_calc\n",
    "    \n",
    "    t2 = time.perf_counter()\n",
    "    passed_time = t2 - t1\n",
    "    print(\"Done! ({} m., {} s.)\".format(int(passed_time/60), int(passed_time%60)))\n",
    "    \n",
    "    return evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done! (1 m., 9 s.)\n"
     ]
    }
   ],
   "source": [
    "simsrl_evaluation = evaluator(SimpleModel, model_name, detach_me = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model simple_b64ep10ly4em512do2lr01_csample performs as follows:\n",
      "Pooled Accuracy: 0.008698447480246363\n",
      "Pooled F1_macro: 0.00023306586396465879\n",
      "Pooled F1_weighted: 0.01724687393338475\n",
      "Mean Accuracy: 0.005419621675712997 (std = 0.016489233733082368)\n",
      "Mean F1_macro: 0.0012722763876300662 (std = 0.003942717454644402)\n",
      "Mean F1_weighted: 0.010294713987881536 (std = 0.02972268111821122)\n",
      "Correlation sentence length and accuracy: 0.22561381824654003\n",
      "Correlation sentence length and F1_macro: 0.22123422121514558\n",
      "Correlation sentence length and F1_weighted: 0.24148233958945806\n"
     ]
    }
   ],
   "source": [
    "simsrl_evaluation.print_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simsrl_evaluation.print_confusion_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> published 1989 . <end> <pad> <pad>\\n<start>/<pad> published/<pad> 1989/<pad> ./<pad> <end>/<pad> <pad>/<pad> <pad>/<pad>'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simsrl_evaluation.best_case(\"f1_macro\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simsrl_evaluation.best_cases(\"accuracy\", 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> get the wheel .. . <end>\\n<start>/<pad> get/<pad> the/<pad> wheel/<pad> ../<pad> ./<pad> <end>/<pad>'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simsrl_evaluation.worst_case(\"f1_macro\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simsrl_evaluation.worst_cases(\"accuracy\", 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simsrl_evaluation.save(\"accuracy\")\n",
    "simsrl_evaluation.save(\"f1_macro\")\n",
    "simsrl_evaluation.save(\"f1_weighted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 2: seq2seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SRL_Encoder(nn.Module):\n",
    "    def __init__(self, voc_size, embedding_size, hidden_size):  \n",
    "        super(SRL_Encoder, self).__init__()\n",
    "        \n",
    "        self.embeddings = nn.Embedding(voc_size, embedding_size)\n",
    "        self.sp_pair = embedding_size + 1 # emedded sentence + predicate vector\n",
    "        self.rnn = nn.LSTM(self.sp_pair, hidden_size, bidirectional=True, batch_first=True)\n",
    "        \n",
    "    def forward(self, sentences, pred_vec):\n",
    "        \n",
    "        embeddings = self.embeddings(sentences)\n",
    "        pred_vec = pred_vec.unsqueeze(2)        \n",
    "        sentence_pred_pair = torch.cat((embeddings, pred_vec), dim=2)\n",
    "        contextualized_embedding, (hidden_final, cell_final) = self.rnn(sentence_pred_pair)\n",
    "        \n",
    "        return contextualized_embedding, (hidden_final, cell_final)\n",
    "    \n",
    "    def initHidden(self): # ?\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# attention?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SRL_Decoder(nn.Module):\n",
    "    def __init__(self, xxx, xxx, hidden_size):  \n",
    "        super(SRL_Decoder, self).__init__()\n",
    "        \n",
    "        self.embeddings = nn.Embedding(voc_size, embedding_size)\n",
    "        self.rnn = nn.LSTM(xxx, hidden_size, bidirectional=True, batch_first=True)\n",
    "        \n",
    "    def forward(self, sentences, pred_vec):\n",
    "        \n",
    "        embeddings = self.embeddings(sentences)\n",
    "        pred_vec = pred_vec.unsqueeze(2)        \n",
    "        sentence_pred_pair = torch.cat((embeddings, pred_vec), dim=2)\n",
    "        contextualized_embedding, (hidden_final, cell_final) = self.rnn(sentence_pred_pair)\n",
    "        \n",
    "        \n",
    "        # from end_hidden_state dim --> n_labels, by linear layes\n",
    "        # decoder will be called n_len(target sequence) times\n",
    "        \n",
    "        return contextualized_embedding, (hidden_final, cell_final)\n",
    "    \n",
    "    def initHidden(self): # ?\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#training\n",
    "\n",
    "previous_labels = [\"start\"]\n",
    "\n",
    "c, (e_hidden, e_cell) = encoder(sentence, pred)\n",
    "\n",
    "\n",
    "\n",
    "for w in len(target_seq):\n",
    "    next_label, hidden, cell = decoder(hidden, cell, previous_labels)\n",
    "    previous_labels.append(nex_label)\n",
    "    \n",
    "    loss = cross_entophy(next_label, actual_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Know your enemies; keep until ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for x in train:\n",
    "    output = my_model(x.sentence, x.predicate)\n",
    "    print(\"op\", output.shape)\n",
    "    soft = F.softmax(output, dim=2)\n",
    "    print(torch.argmax(soft, dim=2))\n",
    "    #print(\"sm\", soft.shape)\n",
    "    #print(torch.sum(soft, dim=2).shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
