{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchtext\n",
    "import torch.nn.functional as F\n",
    "#from torchtext.data import Field, BucketIterator, Iterator, TabularDataset\n",
    "from torchtext.legacy.data import Field, BucketIterator, Iterator, TabularDataset # Needed for running this on my laptop\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Meta variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#device = torch.device('cuda:0')\n",
    "device = torch.device('cpu')\n",
    "\n",
    "my_data_directory = \"../../data/\" # where to store files\n",
    "my_models_directory = \"../../models/\"\n",
    "\n",
    "mini_testing = True\n",
    "my_train_file = \"mini_train.csv\" if mini_testing == True else \"train.csv\"\n",
    "my_test_file  = \"mini_test.csv\" if mini_testing == True else \"test.csv\"\n",
    "\n",
    "batch_size = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataloader(directory  = my_data_directory,\n",
    "               train_file = my_train_file,\n",
    "               test_file  = my_test_file,\n",
    "               batch      = batch_size):\n",
    "    \n",
    "    num_whitespacer = lambda x: [int(e) for e in x.split(\" \")]\n",
    "    \n",
    "    SENTENCE = Field(lower = True, \n",
    "                     batch_first = True, \n",
    "                     init_token = \"<start>\", \n",
    "                     eos_token = \"<end>\")\n",
    "    \n",
    "    PREDICATE = Field(tokenize = num_whitespacer, # Here might be some problems ...\n",
    "                      batch_first = True, \n",
    "                      pad_token = 0,\n",
    "                      use_vocab = False,\n",
    "                      init_token = 0, \n",
    "                      eos_token = 0) \n",
    "    \n",
    "    SRLABEL = Field(batch_first = True, \n",
    "                    init_token = \"<start>\", \n",
    "                    eos_token = \"<end>\")\n",
    "    \n",
    "    my_fields = [(\"sentence\", SENTENCE),\n",
    "                 (\"predicate\", PREDICATE),\n",
    "                 (\"srlabel\", SRLABEL)]\n",
    "    \n",
    "    train, test = TabularDataset.splits(path   = directory,\n",
    "                                        train  = train_file,\n",
    "                                        test   = test_file,\n",
    "                                        format = 'csv',\n",
    "                                        fields = my_fields,\n",
    "                                        csv_reader_params = {'delimiter':'\\t',\n",
    "                                                             'quotechar':'Â¤'}) # Seems not to be in data\n",
    "    SENTENCE.build_vocab(train)\n",
    "    SRLABEL.build_vocab(train)  \n",
    "\n",
    "    train_iter, test_iter = BucketIterator.splits((train, test),\n",
    "                                                  batch_size        = batch,\n",
    "                                                  sort_within_batch = True,\n",
    "                                                  sort_key          = lambda x: len(x.sentence),\n",
    "                                                  shuffle           = True,\n",
    "                                                  device            = device)\n",
    "\n",
    "    return train_iter, test_iter, SENTENCE.vocab, SRLABEL.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test, vocab, labels = dataloader()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SRLabeler1(nn.Module):\n",
    "    def __init__(self, voc_size, embedding_size, n_labels):  \n",
    "        super(SRLabeler1, self).__init__()\n",
    "        \n",
    "        self.embeddings = nn.Embedding(voc_size, embedding_size)\n",
    "        self.sp_pair = embedding_size + 1 # emedded sentence + predicate vector\n",
    "        self.rnn = nn.LSTM(self.sp_pair, n_labels, bidirectional=True, batch_first=True)\n",
    "        \n",
    "    def forward(self, sentences, pred_vec, softmax=False):\n",
    "        \n",
    "        embeddings = self.embeddings(sentences)\n",
    "        pred_vec = pred_vec.unsqueeze(2)        \n",
    "        sentence_pred_pair = torch.cat((embeddings, pred_vec), dim=2)\n",
    "        contextualized_embedding, *_ = self.rnn(sentence_pred_pair)\n",
    "        \n",
    "        if softmax == True:\n",
    "            return F.softmax(contextualized_embedding, dim=2)\n",
    "        else:\n",
    "            return contextualized_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainer1(model, # Must be an instance of a model!\n",
    "            name_of_model,\n",
    "            learning_rate,\n",
    "            epochs,\n",
    "            data,\n",
    "            val_data = None,\n",
    "            save_model = False,\n",
    "            directory = my_models_directory,\n",
    "            my_loss_function = nn.CrossEntropyLoss,\n",
    "            my_optimizer = optim.Adam\n",
    "           ):\n",
    "    \"\"\" Specifices a general training procedure for a model. \n",
    "        Note: trainer() requires an instantiated model as model argument. \n",
    "    \"\"\"\n",
    "    \n",
    "    optimizer = my_optimizer(model.parameters(), lr=learning_rate)    \n",
    "    \n",
    "    model.to(device)\n",
    "    model.train()\n",
    "    \n",
    "    loss_function = my_loss_function()\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        epoch_loss = 0\n",
    "        \n",
    "        for i, batch in enumerate(data):\n",
    "            optimizer.zero_grad # reset gradients\n",
    "            \n",
    "            sentence = batch.sentence\n",
    "            predicate = batch.predicate\n",
    "            targets = batch.srlabel\n",
    "            \n",
    "            b=sentence.shape[0] # !\n",
    "            sequence_length = sentence.shape[1] # !\n",
    "            l = targets.shape[1] # !\n",
    "                        \n",
    "            output = model(sentence, predicate)\n",
    "            d = output.shape[2] # !\n",
    "            \n",
    "            #print(\"Output:\", output.shape)\n",
    "            #print(\"Target:\", targets.shape)\n",
    "            \n",
    "            loss = loss_function(output.reshape(b*sequence_length, d), # !\n",
    "                                 targets.reshape(b*sequence_length))\n",
    "            \n",
    "            epoch_loss += loss.item()\n",
    "            loss.backward() # compute gradients\n",
    "            optimizer.step() # update parameters\n",
    "            #break\n",
    "            \n",
    "        print(f\"Epoch: {epoch+1} (out of {epochs}); total loss: {epoch_loss}.\")\n",
    "            \n",
    "        if val_data != None:\n",
    "            model.eval()\n",
    "            # Here we could do some evaluation of model progress, but I have ignored this for now. \n",
    "            model.train()\n",
    "            \n",
    "    if save_model == True:\n",
    "        torch.save(model, directory+name_of_model+\".pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of vocabulary: 1054\n",
      "Number of lables: 34\n",
      "Epoch: 1 (out of 100); total loss: 86.28764319419861.\n",
      "Epoch: 2 (out of 100); total loss: 72.04073572158813.\n",
      "Epoch: 3 (out of 100); total loss: 69.4432680606842.\n",
      "Epoch: 4 (out of 100); total loss: 68.5328722000122.\n",
      "Epoch: 5 (out of 100); total loss: 67.69099807739258.\n",
      "Epoch: 6 (out of 100); total loss: 67.14039778709412.\n",
      "Epoch: 7 (out of 100); total loss: 66.73629450798035.\n",
      "Epoch: 8 (out of 100); total loss: 66.35661482810974.\n",
      "Epoch: 9 (out of 100); total loss: 66.20807361602783.\n",
      "Epoch: 10 (out of 100); total loss: 66.17869806289673.\n",
      "Epoch: 11 (out of 100); total loss: 66.04222083091736.\n",
      "Epoch: 12 (out of 100); total loss: 65.95484614372253.\n",
      "Epoch: 13 (out of 100); total loss: 65.81365489959717.\n",
      "Epoch: 14 (out of 100); total loss: 65.71085548400879.\n",
      "Epoch: 15 (out of 100); total loss: 65.77319741249084.\n",
      "Epoch: 16 (out of 100); total loss: 65.59802865982056.\n",
      "Epoch: 17 (out of 100); total loss: 65.6946473121643.\n",
      "Epoch: 18 (out of 100); total loss: 65.617990732193.\n",
      "Epoch: 19 (out of 100); total loss: 65.65711283683777.\n",
      "Epoch: 20 (out of 100); total loss: 65.75102925300598.\n",
      "Epoch: 21 (out of 100); total loss: 65.97612071037292.\n",
      "Epoch: 22 (out of 100); total loss: 65.83701419830322.\n",
      "Epoch: 23 (out of 100); total loss: 65.87639832496643.\n",
      "Epoch: 24 (out of 100); total loss: 65.82824182510376.\n",
      "Epoch: 25 (out of 100); total loss: 65.81012988090515.\n",
      "Epoch: 26 (out of 100); total loss: 65.85714221000671.\n",
      "Epoch: 27 (out of 100); total loss: 65.90498399734497.\n",
      "Epoch: 28 (out of 100); total loss: 65.8778088092804.\n",
      "Epoch: 29 (out of 100); total loss: 65.83777213096619.\n",
      "Epoch: 30 (out of 100); total loss: 65.94122290611267.\n",
      "Epoch: 31 (out of 100); total loss: 66.05459976196289.\n",
      "Epoch: 32 (out of 100); total loss: 66.12091183662415.\n",
      "Epoch: 33 (out of 100); total loss: 66.0612952709198.\n",
      "Epoch: 34 (out of 100); total loss: 66.13674211502075.\n",
      "Epoch: 35 (out of 100); total loss: 66.12811613082886.\n",
      "Epoch: 36 (out of 100); total loss: 66.13290023803711.\n",
      "Epoch: 37 (out of 100); total loss: 66.16236662864685.\n",
      "Epoch: 38 (out of 100); total loss: 66.19967126846313.\n",
      "Epoch: 39 (out of 100); total loss: 66.3118326663971.\n",
      "Epoch: 40 (out of 100); total loss: 66.37134194374084.\n",
      "Epoch: 41 (out of 100); total loss: 66.42023968696594.\n",
      "Epoch: 42 (out of 100); total loss: 66.4880850315094.\n",
      "Epoch: 43 (out of 100); total loss: 66.4823784828186.\n",
      "Epoch: 44 (out of 100); total loss: 66.50971555709839.\n",
      "Epoch: 45 (out of 100); total loss: 66.51015520095825.\n",
      "Epoch: 46 (out of 100); total loss: 66.46897411346436.\n",
      "Epoch: 47 (out of 100); total loss: 66.5210771560669.\n",
      "Epoch: 48 (out of 100); total loss: 66.63564467430115.\n",
      "Epoch: 49 (out of 100); total loss: 66.64361429214478.\n",
      "Epoch: 50 (out of 100); total loss: 66.69082593917847.\n",
      "Epoch: 51 (out of 100); total loss: 66.7129225730896.\n",
      "Epoch: 52 (out of 100); total loss: 66.73509192466736.\n",
      "Epoch: 53 (out of 100); total loss: 66.88251686096191.\n",
      "Epoch: 54 (out of 100); total loss: 66.86027336120605.\n",
      "Epoch: 55 (out of 100); total loss: 66.76658415794373.\n",
      "Epoch: 56 (out of 100); total loss: 66.74891209602356.\n",
      "Epoch: 57 (out of 100); total loss: 66.79709982872009.\n",
      "Epoch: 58 (out of 100); total loss: 66.79723596572876.\n",
      "Epoch: 59 (out of 100); total loss: 66.83423614501953.\n",
      "Epoch: 60 (out of 100); total loss: 66.91172242164612.\n",
      "Epoch: 61 (out of 100); total loss: 66.95707869529724.\n",
      "Epoch: 62 (out of 100); total loss: 66.92068362236023.\n",
      "Epoch: 63 (out of 100); total loss: 66.94134140014648.\n",
      "Epoch: 64 (out of 100); total loss: 66.94429397583008.\n",
      "Epoch: 65 (out of 100); total loss: 66.88160967826843.\n",
      "Epoch: 66 (out of 100); total loss: 66.88181281089783.\n",
      "Epoch: 67 (out of 100); total loss: 66.9114682674408.\n",
      "Epoch: 68 (out of 100); total loss: 66.95346403121948.\n",
      "Epoch: 69 (out of 100); total loss: 67.01945281028748.\n",
      "Epoch: 70 (out of 100); total loss: 67.04354333877563.\n",
      "Epoch: 71 (out of 100); total loss: 67.06281614303589.\n",
      "Epoch: 72 (out of 100); total loss: 67.083829164505.\n",
      "Epoch: 73 (out of 100); total loss: 67.0851411819458.\n",
      "Epoch: 74 (out of 100); total loss: 67.0981822013855.\n",
      "Epoch: 75 (out of 100); total loss: 67.09892749786377.\n",
      "Epoch: 76 (out of 100); total loss: 67.03886699676514.\n",
      "Epoch: 77 (out of 100); total loss: 67.0288655757904.\n",
      "Epoch: 78 (out of 100); total loss: 66.9623761177063.\n",
      "Epoch: 79 (out of 100); total loss: 66.97492456436157.\n",
      "Epoch: 80 (out of 100); total loss: 66.93994569778442.\n",
      "Epoch: 81 (out of 100); total loss: 66.96259832382202.\n",
      "Epoch: 82 (out of 100); total loss: 67.01159191131592.\n",
      "Epoch: 83 (out of 100); total loss: 67.07106161117554.\n",
      "Epoch: 84 (out of 100); total loss: 67.0905020236969.\n",
      "Epoch: 85 (out of 100); total loss: 67.16503047943115.\n",
      "Epoch: 86 (out of 100); total loss: 67.22604274749756.\n",
      "Epoch: 87 (out of 100); total loss: 67.23290133476257.\n",
      "Epoch: 88 (out of 100); total loss: 67.1910228729248.\n",
      "Epoch: 89 (out of 100); total loss: 67.19056797027588.\n",
      "Epoch: 90 (out of 100); total loss: 67.20852065086365.\n",
      "Epoch: 91 (out of 100); total loss: 67.1989483833313.\n",
      "Epoch: 92 (out of 100); total loss: 67.15728235244751.\n",
      "Epoch: 93 (out of 100); total loss: 67.23541307449341.\n",
      "Epoch: 94 (out of 100); total loss: 67.22410321235657.\n",
      "Epoch: 95 (out of 100); total loss: 67.22224831581116.\n",
      "Epoch: 96 (out of 100); total loss: 67.31728529930115.\n",
      "Epoch: 97 (out of 100); total loss: 67.31005454063416.\n",
      "Epoch: 98 (out of 100); total loss: 67.27764844894409.\n",
      "Epoch: 99 (out of 100); total loss: 67.30236315727234.\n",
      "Epoch: 100 (out of 100); total loss: 67.27032208442688.\n"
     ]
    }
   ],
   "source": [
    "vocab_size = len(vocab)\n",
    "number_of_labels = len(labels)\n",
    "print(\"Size of vocabulary:\", vocab_size)\n",
    "print(\"Number of labels:\", number_of_labels)\n",
    "epochs = 100\n",
    "my_learning_rate = 0.01\n",
    "my_emedding_size = 50\n",
    "simple_srl_model = SRLabeler1(vocab_size, my_emedding_size, number_of_labels)\n",
    "trainer(simple_srl_model, \"simple_srl\", my_learning_rate, epochs, train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 2: seq2seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SRL_Encoder(nn.Module):\n",
    "    def __init__(self, voc_size, embedding_size, hidden_size):  \n",
    "        super(SRL_Encoder, self).__init__()\n",
    "        \n",
    "        self.embeddings = nn.Embedding(voc_size, embedding_size)\n",
    "        self.sp_pair = embedding_size + 1 # emedded sentence + predicate vector\n",
    "        self.rnn = nn.LSTM(self.sp_pair, hidden_size, bidirectional=True, batch_first=True)\n",
    "        \n",
    "    def forward(self, sentences, pred_vec):\n",
    "        \n",
    "        embeddings = self.embeddings(sentences)\n",
    "        pred_vec = pred_vec.unsqueeze(2)        \n",
    "        sentence_pred_pair = torch.cat((embeddings, pred_vec), dim=2)\n",
    "        contextualized_embedding, (hidden_final, cell_final) = self.rnn(sentence_pred_pair)\n",
    "        \n",
    "        return contextualized_embedding, (hidden_final, cell_final)\n",
    "    \n",
    "    def initHidden(self): # ?\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# attention?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SRL_Decoder(nn.Module):\n",
    "    def __init__(self, xxx, xxx, hidden_size):  \n",
    "        super(SRL_Decoder, self).__init__()\n",
    "        \n",
    "        self.embeddings = nn.Embedding(voc_size, embedding_size)\n",
    "        self.rnn = nn.LSTM(xxx, hidden_size, bidirectional=True, batch_first=True)\n",
    "        \n",
    "    def forward(self, sentences, pred_vec):\n",
    "        \n",
    "        embeddings = self.embeddings(sentences)\n",
    "        pred_vec = pred_vec.unsqueeze(2)        \n",
    "        sentence_pred_pair = torch.cat((embeddings, pred_vec), dim=2)\n",
    "        contextualized_embedding, (hidden_final, cell_final) = self.rnn(sentence_pred_pair)\n",
    "        \n",
    "        \n",
    "        # from end_hidden_state dim --> n_labels, by linear layes\n",
    "        # decoder will be called n_len(target sequence) times\n",
    "        \n",
    "        return contextualized_embedding, (hidden_final, cell_final)\n",
    "    \n",
    "    def initHidden(self): # ?\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#training\n",
    "\n",
    "previous_labels = [\"start\"]\n",
    "\n",
    "c, (e_hidden, e_cell) = encoder(sentence, pred)\n",
    "\n",
    "\n",
    "\n",
    "for w in len(target_seq):\n",
    "    next_label, hidden, cell = decoder(hidden, cell, previous_labels)\n",
    "    previous_labels.append(nex_label)\n",
    "    \n",
    "    loss = cross_entophy(next_label, actual_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Know your enemies; keep until ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for x in train:\n",
    "    output = my_model(x.sentence, x.predicate)\n",
    "    print(\"op\", output.shape)\n",
    "    soft = F.softmax(output, dim=2)\n",
    "    print(torch.argmax(soft, dim=2))\n",
    "    #print(\"sm\", soft.shape)\n",
    "    #print(torch.sum(soft, dim=2).shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
