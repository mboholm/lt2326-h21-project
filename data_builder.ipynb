{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "xml_file = \"/home/max/Documents/mlt/adv_machine/project/data/ukwac_nowiki3466.xml\" # the original xml file\n",
    "path = \"../../data/\" # where to store files\n",
    "corpus_file = \"ukwac_slim.txt\"\n",
    "train_file = \"train.csv\"\n",
    "test_file = \"test.csv\"\n",
    "\n",
    "subsample = 100\n",
    "mini_train_file = \"mini_train.csv\"\n",
    "mini_test_file = \"mini_test.csv\"\n",
    "\n",
    "train_proportion = 0.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree = ET.parse(xml_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "root = tree.getroot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. of sentences: 21187\n",
      "No. of predicates: 76965\n"
     ]
    }
   ],
   "source": [
    "# Know your data\n",
    "print(\"No. of sentences:\", len(root))\n",
    "frames = [f for f in root.iter(\"Frame\")]\n",
    "print(\"No. of predicates:\", len(frames))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing: 208\n"
     ]
    }
   ],
   "source": [
    "# Getting what we need\n",
    "data = []\n",
    "nones=0\n",
    "for s in root:\n",
    "    sentence = {}\n",
    "    \n",
    "    sentence[\"tokens\"] = s.find(\"tokenized\").text.split() # 'quotechar' and 'delimiter'\n",
    "\n",
    "    predicates = []\n",
    "    \n",
    "    ignore = False\n",
    "    \n",
    "    for frame in s.iter(\"Frame\"):\n",
    "        a_pred = {} \n",
    "        \n",
    "        if frame.attrib[\"prd_idx\"] == \"None\":\n",
    "            nones+=1\n",
    "            ignore = True\n",
    "            break # Stop looking for data of that sentence\n",
    "            \n",
    "        else:\n",
    "            a_pred[\"idx\"] = int(frame.attrib[\"prd_idx\"]) #frame prd_idx (later one 0s-and-1(s) vector) HOW TO REPRESENT?\n",
    "        \n",
    "            args = []\n",
    "            for arg in frame.iter(\"Arg\"):\n",
    "                element = {}\n",
    "                \n",
    "                attributes = arg.attrib\n",
    "                element[\"role\"] = attributes[\"role\"]\n",
    "                element[\"token\"] = attributes[\"phrase\"]\n",
    "                element[\"start\"] = int(attributes[\"span_begin\"])\n",
    "                element[\"end\"] = int(attributes[\"span_end\"])\n",
    "\n",
    "                args.append(element)\n",
    "            \n",
    "        a_pred[\"arguments\"] = args\n",
    "        predicates.append(a_pred)\n",
    "    \n",
    "    sentence[\"srl\"] = predicates\n",
    "    if ignore == False:\n",
    "        data.append(sentence)\n",
    "    \n",
    "print(\"Missing:\", nones)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size: 76757\n"
     ]
    }
   ],
   "source": [
    "# Prepare our data\n",
    "mega_list = []\n",
    "for sentence in data:\n",
    "    for predicate in sentence[\"srl\"]:\n",
    "        if predicate[\"idx\"] != \"None\":\n",
    "            pred_vec = [\"0\" for i in range(len(sentence[\"tokens\"]))]\n",
    "            pred_vec[predicate[\"idx\"]] = \"1\"\n",
    "\n",
    "            io_sequence = [\"O\" for i in range(len(sentence[\"tokens\"]))]\n",
    "            for arg in predicate[\"arguments\"]:\n",
    "                if arg[\"start\"] == arg[\"end\"]:\n",
    "                    io_sequence[arg[\"start\"]] = \"B-\"+arg[\"role\"]\n",
    "                else:\n",
    "                    io_sequence[arg[\"start\"]] = \"B-\"+arg[\"role\"]\n",
    "                    for i in range(arg[\"start\"]+1, arg[\"end\"]+1): # +1?\n",
    "                        io_sequence[i] = \"I-\"+arg[\"role\"]\n",
    "                        \n",
    "        mega_list.append([sentence[\"tokens\"], pred_vec, io_sequence])\n",
    "print(\"Size:\", len(mega_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking that it adds up\n",
    "for row in mega_list:\n",
    "    a = len(row[0])\n",
    "    b = len(row[1])\n",
    "    c = len(row[2])\n",
    "    \n",
    "    if a != b:\n",
    "        print(a, b)\n",
    "    if a != c:\n",
    "        print(a, c)\n",
    "    if b != c:\n",
    "        print(b, c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write our data to a text file \n",
    "with open(path+corpus_file, \"w\") as f:\n",
    "    for x in mega_list:\n",
    "        f.write(\"\\t\".join([\" \".join(lst) for lst in x]))\n",
    "        f.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create train and test data form our text file\n",
    "with open(path+corpus_file, \"r\") as corpus, open(path+train_file, \"w\") as train, open(path+test_file, \"w\") as test:\n",
    "    c = corpus.readlines()\n",
    "    random.shuffle(c)\n",
    "    cut = int(len(c)*train_proportion)\n",
    "    train.write(\"\\n\".join([line.replace(\"\\n\", \"\") for line in c[:cut]]))\n",
    "    test.write(\"\\n\".join([line.replace(\"\\n\", \"\") for line in c[cut:]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating mini versions of train and test data for development purposes\n",
    "with open(path+corpus_file, \"r\") as corpus, open(path+mini_train_file, \"w\") as train, open(path+mini_test_file, \"w\") as test:\n",
    "    c = corpus.readlines()\n",
    "    random.shuffle(c)\n",
    "    c = c[:subsample]\n",
    "    cut = int(len(c)*train_proportion)\n",
    "    train.write(\"\\n\".join([line.replace(\"\\n\", \"\") for line in c[:cut]]))\n",
    "    test.write(\"\\n\".join([line.replace(\"\\n\", \"\") for line in c[cut:]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
